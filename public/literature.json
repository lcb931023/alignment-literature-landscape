[{"x":"7.5278726","y":"6.9663434","title":"How truthful is GPT-3? A benchmark for language models","cluster":"2","author":"['Owain_Evans']","source":"alignment forum","tags":"AI/Truth, Semantics, & Meaning/AI Risk/Academic Papers/Language Models","date":"2021-09-16T10:09","url":"https://www.lesswrong.com/posts/PF58wEdztZFX2dSue/how-truthful-is-gpt-3-a-benchmark-for-language-models"},{"x":"12.747436","y":"9.277503","title":"Oracle predictions don’t apply to non-existent worlds","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Decision Theory/World Modeling","date":"2021-09-15T09:44","url":"https://www.lesswrong.com/posts/psyhmuDhazzFJKjXf/oracle-predictions-don-t-apply-to-non-existent-worlds"},{"x":"10.370766","y":"7.2945976","title":"Measurement, Optimization, and Take-off Speed","cluster":"1","author":"['jsteinhardt']","source":"alignment forum","tags":"Optimization/Practice & Philosophy of Science/AI","date":"2021-09-10T19:30","url":"https://www.lesswrong.com/posts/u4mFjCPviXHAPjZK7/measurement-optimization-and-take-off-speed"},{"x":"8.708715","y":"7.9202685","title":"Paths To High-Level Machine Intelligence","cluster":"4","author":"['Daniel_Eth']","source":"alignment forum","tags":"AI/World Modeling/AI Timelines/Whole Brain Emulation","date":"2021-09-10T13:21","url":"https://www.lesswrong.com/posts/amK9EqxALJXyd9Rb2/paths-to-high-level-machine-intelligence"},{"x":"11.182188","y":"10.096376","title":"The Blackwell order as a formalization of knowledge","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Information Theory","date":"2021-09-10T02:51","url":"https://www.lesswrong.com/posts/wEjozSY9rhkpAaABt/the-blackwell-order-as-a-formalization-of-knowledge"},{"x":"9.427073","y":"8.311235","title":"The alignment problem in different capability regimes","cluster":"4","author":"['Buck']","source":"alignment forum","tags":"AI Capabilities/AI/AI Risk","date":"2021-09-09T19:46","url":"https://www.lesswrong.com/posts/HHunb8FPnhWaDAQci/the-alignment-problem-in-different-capability-regimes"},{"x":"11.769329","y":"10.781846","title":"Countably Factored Spaces","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"AI/Finite Factored Sets","date":"2021-09-09T04:24","url":"https://www.lesswrong.com/posts/QEfbg6vbjGgfFzJM4/countably-factored-spaces"},{"x":"8.919784","y":"6.562589","title":"Gradient descent is not just more efficient genetic algorithms","cluster":"2","author":"['leogao']","source":"alignment forum","tags":"Gradient Descent/AI","date":"2021-09-08T16:23","url":"https://www.lesswrong.com/posts/c9NSeCapaKtP6kvQD/gradient-descent-is-not-just-more-efficient-genetic"},{"x":"8.61599","y":"9.761071","title":"Distinguishing AI takeover scenarios","cluster":"3","author":"['Sam Clarke', 'Sammy Martin']","source":"alignment forum","tags":"Threat Models/AI/AI Risk/World Modeling/Outer Alignment","date":"2021-09-08T16:19","url":"https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios"},{"x":"11.41014","y":"7.309688","title":"Multi-Agent Inverse Reinforcement Learning: Suboptimal Demonstrations and ","cluster":"0","author":"['sage_bergerson']","source":"alignment forum","tags":"Reinforcement Learning/AI","date":"2021-09-07T16:11","url":"https://www.lesswrong.com/posts/pRD5u2omuDoMTuH39/multi-agent-inverse-reinforcement-learning-suboptimal"},{"x":"9.034199","y":"6.5752115","title":"Obstacles to gradient hacking","cluster":"2","author":"['leogao']","source":"alignment forum","tags":"AI/Inner Alignment/Mesa-Optimization/Gradient Hacking","date":"2021-09-05T22:42","url":"https://www.lesswrong.com/posts/KfX7Ld7BeCMQn5gbz/obstacles-to-gradient-hacking"},{"x":"9.091854","y":"6.5526237","title":"Thoughts on gradient hacking","cluster":"2","author":"['Richard_Ngo']","source":"alignment forum","tags":"Mesa-Optimization/AI/Gradient Hacking","date":"2021-09-03T13:02","url":"https://www.lesswrong.com/posts/egzqHKkzhuZuivHZ4/thoughts-on-gradient-hacking"},{"x":"9.91885","y":"9.485761","title":"Formalizing Objections against Surrogate Goals","cluster":"1","author":"['VojtaKovarik']","source":"alignment forum","tags":"AI/Center on Long-Term Risk (CLR)/Coordination / Cooperation/Game Theory","date":"2021-09-02T16:24","url":"https://www.lesswrong.com/posts/K4FrKRTrmyxrw5Dip/formalizing-objections-against-surrogate-goals"},{"x":"11.19876","y":"9.085402","title":"Competent Preferences","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Goodhart's Law/Modeling People/AI","date":"2021-09-02T14:26","url":"https://www.lesswrong.com/posts/7kuhXtwFdXvD2Ngie/competent-preferences"},{"x":"8.319479","y":"9.728425","title":"NIST AI Risk Management Framework request for information (RFI)","cluster":"3","author":"['Aryeh Englander']","source":"alignment forum","tags":"AI/Community","date":"2021-09-01T00:15","url":"https://www.lesswrong.com/posts/uFLCwj6jcvnvMtBk3/nist-ai-risk-management-framework-request-for-information"},{"x":"7.6168966","y":"7.308506","title":"Call for research on evaluating alignment (funding + advice available)","cluster":"2","author":"['Beth Barnes']","source":"alignment forum","tags":"Grants & Fundraising Opportunities/AI/Community/Inner Alignment/Outer Alignment","date":"2021-08-31T23:28","url":"https://www.lesswrong.com/posts/7Rvctxk73BrKqEaqh/call-for-research-on-evaluating-alignment-funding-advice"},{"x":"11.749643","y":"10.755983","title":"Finite Factored Sets: Applications","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Finite Factored Sets/World Modeling/AI","date":"2021-08-31T21:19","url":"https://www.lesswrong.com/posts/yGFiw23pJ32obgLbw/finite-factored-sets-applications"},{"x":"11.752942","y":"10.710594","title":"Finite Factored Sets: Inferring Time","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Finite Factored Sets/AI","date":"2021-08-31T21:18","url":"https://www.lesswrong.com/posts/hePucCfKyiRHECz3e/finite-factored-sets-inferring-time"},{"x":"11.232627","y":"9.239013","title":"What are biases, anyway? Multiple type signatures","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-08-31T21:16","url":"https://www.lesswrong.com/posts/Jute9YcbYvm4ZWdXk/what-are-biases-anyway-multiple-type-signatures"},{"x":"11.652229","y":"9.971575","title":"The Telephone Theorem: Information At A Distance Is Mediated By Deterministic Constraints","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Information Theory/World Modeling","date":"2021-08-31T16:50","url":"https://www.lesswrong.com/posts/jJf4FrfiQdDGg7uco/the-telephone-theorem-information-at-a-distance-is-mediated"},{"x":"10.441521","y":"9.542036","title":"Grokking the Intentional Stance","cluster":"1","author":"['jbkjr']","source":"alignment forum","tags":"Goal-Directedness/AI/Agency/Dissolving the Question/Intentionality","date":"2021-08-31T15:49","url":"https://www.lesswrong.com/posts/jHSi6BwDKTLt5dmsG/grokking-the-intentional-stance"},{"x":"7.9157376","y":"8.150939","title":"Alignment Research = Conceptual Alignment Research + Applied Alignment Research","cluster":"3","author":"['adamShimi']","source":"alignment forum","tags":"AI","date":"2021-08-30T21:13","url":"https://www.lesswrong.com/posts/2Xfv3GQgo2kGER8vA/alignment-research-conceptual-alignment-research-applied"},{"x":"8.366182","y":"6.0471992","title":"A short introduction to machine learning","cluster":"2","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2021-08-30T14:31","url":"https://www.lesswrong.com/posts/qE73pqxAZmeACsAdF/a-short-introduction-to-machine-learning"},{"x":"7.6334047","y":"7.815783","title":"[Question] What are good alignment conference papers?","cluster":"2","author":"['adamShimi']","source":"alignment forum","tags":"AI/AI Risk/Literature Reviews/Community","date":"2021-08-28T13:35","url":"https://www.lesswrong.com/posts/cSNaxb8wu564x9n6r/what-are-good-alignment-conference-papers"},{"x":"11.893742","y":"9.01102","title":"Can you control the past?","cluster":"1","author":"['Joe Carlsmith']","source":"alignment forum","tags":"Decision Theory/Rationality","date":"2021-08-27T19:39","url":"https://www.lesswrong.com/posts/PcfHSSAMNFMgdqFyB/can-you-control-the-past"},{"x":"10.567235","y":"8.5844","title":"Introduction to Reducing Goodhart","cluster":"4","author":"['Charlie Steiner']","source":"alignment forum","tags":"Goodhart's Law/Value Learning/AI","date":"2021-08-26T18:38","url":"https://www.lesswrong.com/posts/RozggPiqQxzzDaNYF/introduction-to-reducing-goodhart"},{"x":"11.746765","y":"9.459311","title":"MIRI/​OP exchange about decision theory","cluster":"1","author":"['Rob Bensinger']","source":"alignment forum","tags":"Decision Theory/Functional Decision Theory/AI/Causal Decision Theory/Evidential Decision Theory/Embedded Agency/Rationality","date":"2021-08-25T22:44","url":"https://www.lesswrong.com/posts/FBbHEjkZzdupcjkna/miri-op-exchange-about-decision-theory-1"},{"x":"7.866072","y":"8.643524","title":"Welcome & FAQ!","cluster":"3","author":"['Ruby', 'habryka']","source":"alignment forum","tags":"Site Meta","date":"2021-08-24T20:14","url":"https://www.lesswrong.com/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq"},{"x":"11.458612","y":"6.1030035","title":"Extraction of human preferences ","cluster":"0","author":"['arunraja-hub']","source":"alignment forum","tags":"AI/AI Safety Camp/Reinforcement Learning","date":"2021-08-24T16:34","url":"https://www.lesswrong.com/posts/PZYD5kBpeHWgE5jX4/extraction-of-human-preferences"},{"x":"7.652602","y":"7.0535636","title":"The Codex Skeptic FAQ","cluster":"2","author":"['Michaël Trazzi']","source":"alignment forum","tags":"Software Tools/Practical/AI/Language Models","date":"2021-08-24T16:01","url":"https://www.lesswrong.com/posts/Rhg27MqkxJsnZwoYg/the-codex-skeptic-faq"},{"x":"12.671086","y":"8.715476","title":"Yet More Modal Combat","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"Game Theory/World Modeling","date":"2021-08-24T10:32","url":"https://www.lesswrong.com/posts/EPMnjRdhHNd65XpDt/yet-more-modal-combat"},{"x":"10.398258","y":"9.617205","title":"AI Risk for Epistemic Minimalists","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2021-08-22T15:39","url":"https://www.lesswrong.com/posts/8fpzBHt7e6n7Qjoo9/ai-risk-for-epistemic-minimalists"},{"x":"8.152571","y":"8.780185","title":"AI Safety Papers: An App for the TAI Safety Database","cluster":"3","author":"['ozziegooen']","source":"alignment forum","tags":"AI/QURI/Software Tools","date":"2021-08-21T02:02","url":"https://www.lesswrong.com/posts/GgusnG2tiPEa4aYFS/ai-safety-papers-an-app-for-the-tai-safety-database"},{"x":"9.04496","y":"8.841494","title":"Analogies and General Priors on Intelligence","cluster":"4","author":"['riceissa', 'Sammy Martin']","source":"alignment forum","tags":"AI/World Modeling/Evolution/AI Takeoff","date":"2021-08-20T21:03","url":"https://www.lesswrong.com/posts/yFQkFNCszoJPZTnK6/analogies-and-general-priors-on-intelligence"},{"x":"8.007669","y":"7.4578156","title":"Provide feedback on Open Philanthropy’s AI alignment RFP","cluster":"2","author":"['abergal', 'Nick_Beckstead']","source":"alignment forum","tags":"AI","date":"2021-08-20T19:52","url":"https://www.lesswrong.com/posts/TmhrC93mj2Pgsox9t/provide-feedback-on-open-philanthropy-s-ai-alignment-rfp"},{"x":"10.236929","y":"5.764081","title":"How DeepMind’s Generally Capable Agents Were Trained","cluster":"0","author":"['1a3orn']","source":"alignment forum","tags":"AI/DeepMind","date":"2021-08-20T18:52","url":"https://www.lesswrong.com/posts/DreKBuMvK7fdESmSJ/how-deepmind-s-generally-capable-agents-were-trained"},{"x":"11.826586","y":"10.727919","title":"Finite Factored Sets: Polynomials and Probability","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Rationality/Finite Factored Sets","date":"2021-08-17T21:53","url":"https://www.lesswrong.com/posts/jr5kyRhNriCX2Ayyg/finite-factored-sets-polynomials-and-probability"},{"x":"8.560076","y":"9.81197","title":"Modelling Transformative AI Risks (MTAIR) Project: Introduction","cluster":"3","author":"['Davidmanheim', 'Aryeh Englander']","source":"alignment forum","tags":"World Modeling/AI/World Modeling Techniques/Deconfusion/Distinctions","date":"2021-08-16T07:12","url":"https://www.lesswrong.com/posts/qnA6paRwMky3Q6ktk/modelling-transformative-ai-risks-mtair-project-introduction"},{"x":"8.940368","y":"6.46435","title":"Approaches to gradient hacking","cluster":"2","author":"['adamShimi']","source":"alignment forum","tags":"Inner Alignment/Mesa-Optimization/Deconfusion/AI Risk/AI/Gradient Hacking","date":"2021-08-14T15:16","url":"https://www.lesswrong.com/posts/S2jsBsZvqjBZa3pKT/approaches-to-gradient-hacking"},{"x":"10.793084","y":"9.324794","title":"A review of \"Agents and Devices\"","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"Agency/Goal-Directedness/Deconfusion/AI/Rationality","date":"2021-08-13T08:42","url":"https://www.lesswrong.com/posts/WrsQfBRqAPKiyGygT/a-review-of-agents-and-devices"},{"x":"11.810405","y":"7.8097353","title":"Power-seeking for successive choices","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Deconfusion/Instrumental Convergence/AI","date":"2021-08-12T20:37","url":"https://www.lesswrong.com/posts/5Hc4R6rj5yJ3xBhiX/power-seeking-for-successive-choices"},{"x":"8.160679","y":"7.4690676","title":"Some criteria for sandwiching projects","cluster":"4","author":"['DMZ']","source":"alignment forum","tags":"AI","date":"2021-08-12T03:40","url":"https://www.lesswrong.com/posts/Gfbf7RsE2fvxGXKC5/some-criteria-for-sandwiching-projects"},{"x":"8.533498","y":"7.9267306","title":"Automating Auditing: An ambitious concrete technical research proposal","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2021-08-11T20:32","url":"https://www.lesswrong.com/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research"},{"x":"11.432142","y":"9.487075","title":"When Most VNM-Coherent Preference Orderings Have Convergent Instrumental Incentives","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence/Coherence Arguments/Rationality/Goal-Directedness","date":"2021-08-09T17:22","url":"https://www.lesswrong.com/posts/LYxWrxram2JFBaeaq/when-most-vnm-coherent-preference-orderings-have-convergent"},{"x":"10.610378","y":"9.139546","title":"Goal-Directedness and Behavior, Redux","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/Deconfusion/AI","date":"2021-08-09T14:26","url":"https://www.lesswrong.com/posts/YApiu7x3oTTzDgFFN/goal-directedness-and-behavior-redux"},{"x":"10.516754","y":"9.106419","title":"Applications for Deconfusing Goal-Directedness","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/Instrumental Convergence/Optimization/Inner Alignment/Deconfusion/AI","date":"2021-08-08T13:05","url":"https://www.lesswrong.com/posts/ECPmgwwWBikTtdqXo/applications-for-deconfusing-goal-directedness"},{"x":"11.884457","y":"8.491418","title":"Seeking Power is Convergently Instrumental in a Broad Class of Environments","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence","date":"2021-08-08T02:02","url":"https://www.lesswrong.com/posts/hzeLSQ9nwDkPc4KNt/seeking-power-is-convergently-instrumental-in-a-broad-class"},{"x":"9.361619","y":"8.35351","title":"Research agenda update","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuroscience/AI/Research Agendas","date":"2021-08-06T19:24","url":"https://www.lesswrong.com/posts/DkfGaZTgwsE7XZq9k/research-agenda-update"},{"x":"7.989383","y":"7.164243","title":"What 2026 looks like","cluster":"2","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/Forecasting & Prediction/Forecasts (Specific Predictions)/AI Timelines/AI Takeoff","date":"2021-08-06T16:14","url":"https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like"},{"x":"11.8322","y":"10.345927","title":"Traps of Formalization in Deconfusion","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Deconfusion/World Modeling/Rationality","date":"2021-08-05T22:40","url":"https://www.lesswrong.com/posts/pEB3LrNxvMKFLGBSG/traps-of-formalization-in-deconfusion"},{"x":"9.756135","y":"8.0687065","title":"Value loading in the human brain: a worked example","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Goal-Directedness/Neuroscience/AI/Planning & Decision-Making","date":"2021-08-04T17:20","url":"https://www.lesswrong.com/posts/iMM6dvHzco6jBMFMX/value-loading-in-the-human-brain-a-worked-example"},{"x":"8.901978","y":"8.609028","title":"Garrabrant and Shah on human modeling in AGI","cluster":"4","author":"['Rob Bensinger']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)/Mesa-Optimization/Humans Consulting HCH/Iterated Amplification /Factored Cognition","date":"2021-08-04T04:35","url":"https://www.lesswrong.com/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi"},{"x":"9.726072","y":"8.960354","title":"LCDT, A Myopic Decision Theory","cluster":"4","author":"['adamShimi', 'evhub']","source":"alignment forum","tags":"Myopia/Decision Theory/Causal Decision Theory/AI/Deception","date":"2021-08-03T22:41","url":"https://www.lesswrong.com/posts/Y76durQHrfqwgwM5o/lcdt-a-myopic-decision-theory"},{"x":"10.455807","y":"10.226378","title":"What does GPT-3 understand? Symbol grounding and Chinese rooms","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Practical/AI/Community/Symbol Grounding/GPT","date":"2021-08-03T13:14","url":"https://www.lesswrong.com/posts/ns95FHkkzpjXh4x5Q/what-does-gpt-3-understand-symbol-grounding-and-chinese"},{"x":"9.66546","y":"7.198057","title":"[Question] Did they or didn’t they learn tool use?","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI","date":"2021-07-29T13:26","url":"https://www.lesswrong.com/posts/8GoynCn4jaXKsiDky/did-they-or-didn-t-they-learn-tool-use"},{"x":"9.431901","y":"6.104027","title":"[Question] How much compute was used to train DeepMind’s generally capable agents?","cluster":"2","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/DeepMind","date":"2021-07-29T11:34","url":"https://www.lesswrong.com/posts/KaPaTdpLggdMqzdyo/how-much-compute-was-used-to-train-deepmind-s-generally"},{"x":"9.952231","y":"5.8861847","title":"DeepMind: Generally capable agents emerge from open-ended play","cluster":"0","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/DeepMind/Machine Learning/General Intelligence/Reinforcement Learning/AI Capabilities","date":"2021-07-27T14:19","url":"https://www.lesswrong.com/posts/mTGrrX8SZJ2tQDuqz/deepmind-generally-capable-agents-emerge-from-open-ended"},{"x":"10.1046915","y":"9.334283","title":"Refactoring Alignment (attempt #2)","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI","date":"2021-07-26T20:12","url":"https://www.lesswrong.com/posts/vayxfTSQEDtwhPGpW/refactoring-alignment-attempt-2"},{"x":"8.237096","y":"8.66503","title":"AXRP Episode 10 - AI’s Future and Impacts with Katja Grace","cluster":"3","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Audio/Interviews/Technological Forecasting/AXRP","date":"2021-07-23T22:10","url":"https://www.lesswrong.com/posts/xbABZRxoSTAnsf8os/axrp-episode-10-ai-s-future-and-impacts-with-katja-grace"},{"x":"10.550915","y":"8.257187","title":"Re-Define Intent Alignment?","cluster":"4","author":"['abramdemski']","source":"alignment forum","tags":"Inner Alignment/AI","date":"2021-07-22T19:00","url":"https://www.lesswrong.com/posts/7fkaJLzRiEr2hmSDi/re-define-intent-alignment"},{"x":"10.158006","y":"7.5557485","title":"Reward splintering for AI design","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-07-21T16:13","url":"https://www.lesswrong.com/posts/xoQhHxgwdHvWhj4P4/reward-splintering-for-ai-design"},{"x":"10.246707","y":"8.078408","title":"A model of decision-making in the brain (the short version)","cluster":"1","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuroscience/Reinforcement Learning/World Modeling/AI","date":"2021-07-18T14:39","url":"https://www.lesswrong.com/posts/e5duEqhAhurT8tCyr/a-model-of-decision-making-in-the-brain-the-short-version"},{"x":"11.442161","y":"9.309432","title":"Bayesianism versus conservatism versus Goodhart","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-07-16T23:39","url":"https://www.lesswrong.com/posts/EFZ64igiNNwiLHaYk/bayesianism-versus-conservatism-versus-goodhart"},{"x":"11.093916","y":"10.23867","title":"Underlying model of an imperfect morphism","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling","date":"2021-07-16T13:13","url":"https://www.lesswrong.com/posts/RnxkAiGcQpfErjHYT/underlying-model-of-an-imperfect-morphism"},{"x":"11.768471","y":"10.299472","title":"Generalizing Koopman-Pitman-Darmois","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"Rationality/World Modeling","date":"2021-07-15T22:33","url":"https://www.lesswrong.com/posts/tGCyRQigGoqA4oSRo/generalizing-koopman-pitman-darmois"},{"x":"7.6731043","y":"8.720481","title":"Fractional progress estimates for AI timelines and implied resource requirements","cluster":"3","author":"['Mark Xu', 'CarlShulman']","source":"alignment forum","tags":"AI/AI Timelines/Surveys","date":"2021-07-15T18:43","url":"https://www.lesswrong.com/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied"},{"x":"10.132546","y":"7.7305408","title":"Model-based RL, Desires, Brains, Wireheading","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Wireheading/Corrigibility/Inner Alignment/AI","date":"2021-07-14T15:11","url":"https://www.lesswrong.com/posts/K5ikTdaNymfWXQHFb/model-based-rl-desires-brains-wireheading"},{"x":"8.695471","y":"7.189114","title":"Answering questions honestly instead of predicting human answers: lots of problems and some solutions","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2021-07-13T18:49","url":"https://www.lesswrong.com/posts/gEw8ig38mCGjia7dj/answering-questions-honestly-instead-of-predicting-human"},{"x":"11.817289","y":"10.411601","title":"The Additive Summary Equation","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"World Modeling","date":"2021-07-13T18:23","url":"https://www.lesswrong.com/posts/E4GvMdELt6s6CaXrb/the-additive-summary-equation"},{"x":"11.29048","y":"9.543644","title":"Anthropic decision theory for self-locating beliefs","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Sleeping Beauty Paradox/Anthropics","date":"2021-07-12T14:11","url":"https://www.lesswrong.com/posts/MZJxtzjSeezEkedWn/anthropic-decision-theory-for-self-locating-beliefs"},{"x":"11.200268","y":"10.141172","title":"The inescapability of knowledge","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Rationality/AI","date":"2021-07-11T22:59","url":"https://www.lesswrong.com/posts/DLjCSHjwbxzEEa6Hu/the-inescapability-of-knowledge"},{"x":"11.661586","y":"8.018875","title":"The More Power At Stake, The Stronger Instrumental Convergence Gets For Optimal Policies","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence","date":"2021-07-11T17:36","url":"https://www.lesswrong.com/posts/Yc5QSSZCQ9qdyxZF6/the-more-power-at-stake-the-stronger-instrumental"},{"x":"11.1886425","y":"10.137506","title":"The accumulation of knowledge: literature review","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Rationality/AI/Truth, Semantics, & Meaning","date":"2021-07-10T18:36","url":"https://www.lesswrong.com/posts/dkruhqAEhXnbAk7iJ/the-accumulation-of-knowledge-literature-review"},{"x":"11.218914","y":"10.279883","title":"Generalised models: imperfect morphisms and informational entropy","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Logic & Mathematics /Probability & Statistics/World Modeling","date":"2021-07-09T17:35","url":"https://www.lesswrong.com/posts/mMCvmLMHXid5tHKju/generalised-models-imperfect-morphisms-and-informational"},{"x":"11.872007","y":"10.755642","title":"Finite Factored Sets: Conditional Orthogonality","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Finite Factored Sets/Rationality/AI/Causality/Abstraction","date":"2021-07-09T06:01","url":"https://www.lesswrong.com/posts/hA6z9s72KZDYpuFhq/finite-factored-sets-conditional-orthogonality"},{"x":"9.621666","y":"8.422005","title":"Intermittent Distillations #4: Semiconductors, Economics, Intelligence, and Technological Progress.","cluster":"1","author":"['Mark Xu']","source":"alignment forum","tags":"AI/World Modeling/Automation/Superintelligence/Economic Consequences of AGI/Practice & Philosophy of Science","date":"2021-07-08T22:14","url":"https://www.lesswrong.com/posts/rQGW2GqHAFprupYkf/intermittent-distillations-4-semiconductors-economics"},{"x":"9.857246","y":"6.6158586","title":"BASALT: A Benchmark for Learning from Human Feedback","cluster":"0","author":"['Rohin Shah']","source":"alignment forum","tags":"AI","date":"2021-07-08T17:40","url":"https://www.lesswrong.com/posts/RyH8LtgMbRAJ9Dv6R/basalt-a-benchmark-for-learning-from-human-feedback"},{"x":"10.970716","y":"9.764597","title":"Practical anthropics summary","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling/Anthropics/Rationality","date":"2021-07-08T15:10","url":"https://www.lesswrong.com/posts/jfMExCKWipKeCdSuG/practical-anthropics-summary"},{"x":"10.732365","y":"9.770472","title":"Anthropics and Fermi: grabby, visible, zoo-keeping, and early aliens","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling/Anthropics/Rationality/Extraterrestrial Life/Grabby Aliens","date":"2021-07-08T15:07","url":"https://www.lesswrong.com/posts/wgHbNZHsqfiXiqofd/anthropics-and-fermi-grabby-visible-zoo-keeping-and-early"},{"x":"10.619444","y":"9.661138","title":"The SIA population update can be surprisingly small","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling/Rationality/Anthropics","date":"2021-07-08T10:45","url":"https://www.lesswrong.com/posts/xfEsxAtBTLgFe7fSZ/the-sia-population-update-can-be-surprisingly-small"},{"x":"11.366132","y":"10.113018","title":"Anthropics in infinite universes","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling/Anthropics/Infinity","date":"2021-07-08T06:56","url":"https://www.lesswrong.com/posts/rbJLrcmHtusGBudTY/anthropics-in-infinite-universes"},{"x":"10.049777","y":"8.265924","title":"A world in which the alignment problem seems lower-stakes","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence","date":"2021-07-08T02:31","url":"https://www.lesswrong.com/posts/sunXMY5WyDcrHsNRr/a-world-in-which-the-alignment-problem-seems-lower-stakes"},{"x":"9.484513","y":"6.1100597","title":"How much chess engine progress is about adapting to bigger computers?","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Bounties (active)/Scaling Laws","date":"2021-07-07T22:35","url":"https://www.lesswrong.com/posts/H6L7fuEN9qXDanQ6W/how-much-chess-engine-progress-is-about-adapting-to-bigger"},{"x":"10.65067","y":"8.805068","title":"Agency and the unreliable autonomous car","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"AI/World Modeling/Rationality/Counterfactuals","date":"2021-07-07T14:58","url":"https://www.lesswrong.com/posts/8AjDwHp9pvZdm6ZEp/agency-and-the-unreliable-autonomous-car"},{"x":"11.802631","y":"10.74771","title":"A second example of conditional orthogonality in finite factored sets","cluster":"1","author":"['DanielFilan']","source":"alignment forum","tags":"Finite Factored Sets/AI","date":"2021-07-07T01:40","url":"https://www.lesswrong.com/posts/GFGNwCwkffBevyXR2/a-second-example-of-conditional-orthogonality-in-finite"},{"x":"11.768309","y":"10.735847","title":"A simple example of conditional orthogonality in finite factored sets","cluster":"1","author":"['DanielFilan']","source":"alignment forum","tags":"Rationality/AI/Finite Factored Sets","date":"2021-07-06T00:36","url":"https://www.lesswrong.com/posts/qGjCt4Xq83MBaygPx/a-simple-example-of-conditional-orthogonality-in-finite"},{"x":"10.436834","y":"9.347452","title":"Anthropic Effects in Estimating Evolution Difficulty","cluster":"1","author":"['Mark Xu']","source":"alignment forum","tags":"AI/Anthropics/Evolution","date":"2021-07-05T04:02","url":"https://www.lesswrong.com/posts/pg6Z5tiuXotGTWaG8/anthropic-effects-in-estimating-evolution-difficulty"},{"x":"12.224043","y":"8.507923","title":"Confusions re: Higher-Level Game Theory","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Game Theory/World Modeling/Rationality","date":"2021-07-02T03:15","url":"https://www.lesswrong.com/posts/FPML8k4QtjJxk3Y4M/confusions-re-higher-level-game-theory"},{"x":"8.696126","y":"7.360293","title":"Experimentally evaluating whether honesty generalizes","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-07-01T17:47","url":"https://www.lesswrong.com/posts/BxersHYN2qcFoonwg/experimentally-evaluating-whether-honesty-generalizes"},{"x":"9.656567","y":"7.5578246","title":"Thoughts on safety in predictive learning","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Mesa-Optimization/Self Fulfilling/Refuting Prophecies","date":"2021-06-30T19:17","url":"https://www.lesswrong.com/posts/ey7jACdF4j6GrQLrG/thoughts-on-safety-in-predictive-learning"},{"x":"10.315255","y":"9.52333","title":"Musings on general systems alignment","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"AI/Deconfusion","date":"2021-06-30T18:16","url":"https://www.lesswrong.com/posts/hKMgCaAYS4hnanxBL/musings-on-general-systems-alignment"},{"x":"11.626907","y":"8.77377","title":"Progress on Causal Influence Diagrams","cluster":"1","author":"['tom4everitt']","source":"alignment forum","tags":"AI/Causality/Incentives/Game Theory","date":"2021-06-30T15:34","url":"https://www.lesswrong.com/posts/Cd7Hw492RqooYgQAS/progress-on-causal-influence-diagrams"},{"x":"9.974591","y":"7.89897","title":"Brute force searching for alignment","cluster":"4","author":"['Donald Hobson']","source":"alignment forum","tags":"AI","date":"2021-06-27T21:54","url":"https://www.lesswrong.com/posts/MRFXpedeKJRa324dL/brute-force-searching-for-alignment"},{"x":"11.620123","y":"10.657767","title":"Finite Factored Sets: LW transcript with running commentary","cluster":"1","author":"['Rob Bensinger', 'Scott Garrabrant']","source":"alignment forum","tags":"AI/Finite Factored Sets","date":"2021-06-27T16:02","url":"https://www.lesswrong.com/posts/6t9F5cS3JjtSspbAZ/finite-factored-sets-lw-transcript-with-running-commentary"},{"x":"11.460403","y":"10.573398","title":"AXRP Episode 9 - Finite Factored Sets with Scott Garrabrant","cluster":"1","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Interviews/Embedded Agency/Causality/Abstraction/Finite Factored Sets/Audio/AXRP","date":"2021-06-24T22:10","url":"https://www.lesswrong.com/posts/s4FNjvrJG6zmYdBuG/axrp-episode-9-finite-factored-sets-with-scott-garrabrant"},{"x":"10.036465","y":"7.2007494","title":"Discussion: Objective Robustness and Inner Alignment Terminology","cluster":"4","author":"['jbkjr', 'Lauro Langosco']","source":"alignment forum","tags":"AI/Inner Alignment/Agency/Goal-Directedness","date":"2021-06-23T23:25","url":"https://www.lesswrong.com/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment"},{"x":"10.074428","y":"6.7563396","title":"Empirical Observations of Objective Robustness Failures","cluster":"0","author":"['jbkjr', 'Lauro Langosco']","source":"alignment forum","tags":"Inner Alignment/AI/Agency/Goal-Directedness","date":"2021-06-23T23:23","url":"https://www.lesswrong.com/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures"},{"x":"11.509974","y":"8.211968","title":"Alex Turner’s Research, Comprehensive Information Gathering","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"AI/Impact Measures/Instrumental Convergence/AI Risk/Deconfusion","date":"2021-06-23T09:44","url":"https://www.lesswrong.com/posts/rxsg2sTyHGnMTYbeH/alex-turner-s-research-comprehensive-information-gathering"},{"x":"8.237385","y":"7.2720494","title":"Frequent arguments about alignment","cluster":"4","author":"['John Schulman']","source":"alignment forum","tags":"AI/World Modeling","date":"2021-06-23T00:46","url":"https://www.lesswrong.com/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment"},{"x":"11.461135","y":"7.8137197","title":"Environmental Structure Can Cause Instrumental Convergence","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/AI Risk/Instrumental Convergence","date":"2021-06-22T22:26","url":"https://www.lesswrong.com/posts/b6jJddSvWMdZHJHh3/environmental-structure-can-cause-instrumental-convergence"},{"x":"8.411816","y":"6.148119","title":"Parameter counts in Machine Learning","cluster":"2","author":"['Jsevillamol', 'Pablo Villalobos']","source":"alignment forum","tags":"AI/Machine Learning/Scaling Laws","date":"2021-06-19T16:04","url":"https://www.lesswrong.com/posts/GzoWcYibWYwJva8aL/parameter-counts-in-machine-learning"},{"x":"11.17364","y":"10.132711","title":"Knowledge is not just precipitation of action","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"AI/Rationality/Truth, Semantics, & Meaning","date":"2021-06-18T23:26","url":"https://www.lesswrong.com/posts/JMpERTz9TcnMfEapF/knowledge-is-not-just-precipitation-of-action"},{"x":"11.959633","y":"9.018493","title":"Non-poisonous cake: anthropic updates are normal","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Rationality","date":"2021-06-18T14:51","url":"https://www.lesswrong.com/posts/3kwwDieE9SmFoXz9F/non-poisonous-cake-anthropic-updates-are-normal"},{"x":"8.665656","y":"9.039512","title":"[Question] Pros and cons of working on near-term technical AI safety and assurance","cluster":"3","author":"['Aryeh Englander']","source":"alignment forum","tags":"AI/World Optimization","date":"2021-06-17T20:17","url":"https://www.lesswrong.com/posts/gBLs3GefMdtWe6iSk/pros-and-cons-of-working-on-near-term-technical-ai-safety"},{"x":"10.071491","y":"8.8334875","title":"MIRIx Part I: Insufficient Values","cluster":"4","author":"['Jozdien', 'Jacob Abraham', 'Abraham Francis']","source":"alignment forum","tags":"AI/Inner Alignment/Outer Alignment/Coherent Extrapolated Volition","date":"2021-06-16T14:33","url":"https://www.lesswrong.com/posts/Pd53Mip7Aa3TsdA7E/mirix-part-i-insufficient-values-1"},{"x":"10.482556","y":"7.397548","title":"Reward Is Not Enough","cluster":"0","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Reinforcement Learning/Neuroscience/Corrigibility/Subagents","date":"2021-06-16T13:52","url":"https://www.lesswrong.com/posts/frApEhpyKQAcFvbXJ/reward-is-not-enough"},{"x":"12.404618","y":"8.3860655","title":"[Question] Open problem: how can we quantify player alignment in 2x2 normal-form games?","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Open Problems/Deconfusion/Game Theory","date":"2021-06-16T02:09","url":"https://www.lesswrong.com/posts/ghyw76DfRyiiMxo3t/open-problem-how-can-we-quantify-player-alignment-in-2x2"},{"x":"8.307469","y":"9.087163","title":"Vignettes Workshop (AI Impacts)","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"Threat Models/AI","date":"2021-06-15T12:05","url":"https://www.lesswrong.com/posts/jusSrXEAsiqehBsmh/vignettes-workshop-ai-impacts"},{"x":"11.095422","y":"10.133358","title":"Knowledge is not just digital abstraction layers","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"World Modeling/Truth, Semantics, & Meaning","date":"2021-06-15T03:49","url":"https://www.lesswrong.com/posts/fcnFddKjKZdDXt5cp/knowledge-is-not-just-digital-abstraction-layers"},{"x":"11.907597","y":"9.760484","title":"Looking Deeper at Deconfusion","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Deconfusion/AI/Intellectual Progress (Society-Level)/Epistemology/Rationality","date":"2021-06-13T21:29","url":"https://www.lesswrong.com/posts/5Nz4PJgvLCpJd6YTA/looking-deeper-at-deconfusion"},{"x":"8.996571","y":"7.118568","title":"Avoiding the instrumental policy by hiding information about humans","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-06-13T20:00","url":"https://www.lesswrong.com/posts/roZvoF6tRH6xYtHMF/avoiding-the-instrumental-policy-by-hiding-information-about"},{"x":"9.645301","y":"7.5493712","title":"Answering questions honestly given world-model mismatches","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-06-13T18:00","url":"https://www.lesswrong.com/posts/SRJ5J9Tnyq7bySxbt/answering-questions-honestly-given-world-model-mismatches"},{"x":"10.961692","y":"6.290037","title":"Nearest unblocked strategy versus learning patches","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-02-23T12:33","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037536d/nearest-unblocked-strategy-versus-learning-patches"},{"x":"9.623313","y":"8.415067","title":"Maximally efficient agents will probably have an anti-daemon immune system","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-02-23T00:40","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375367/maximally-efficient-agents-will-probably-have-an-anti-daemon-immune-system"},{"x":"12.669545","y":"8.608588","title":"Prediction Based Robust Cooperation","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2017-02-22T01:52","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037536c/prediction-based-robust-cooperation"},{"x":"10.337125","y":"8.741841","title":"Are daemons a problem for ideal agents?","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-02-11T08:29","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037535e/are-daemons-a-problem-for-ideal-agents"},{"x":"8.922097","y":"7.4011974","title":"How likely is a random AGI to be honest?","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-02-11T03:32","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037535a/how-likely-is-a-random-agi-to-be-honest"},{"x":"8.8278055","y":"7.5539107","title":"True understanding comes from passing exams","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-02-06T11:45","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037534a/true-understanding-comes-from-passing-exams"},{"x":"10.505533","y":"6.44357","title":"Learning Impact in RL","cluster":"0","author":"['IAFF-User-111']","source":"alignment forum","tags":"","date":"2017-02-04T21:42","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375343/learning-impact-in-rl"},{"x":"10.000969","y":"7.8916855","title":"Censoring out-of-domain representations","cluster":"4","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2017-02-01T04:09","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037533f/censoring-out-of-domain-representations"},{"x":"8.396845","y":"8.188564","title":"My current take on the Paul-MIRI disagreement on alignability of messy AI","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"Cognitive Reduction/Machine Intelligence Research Institute (MIRI)/AI/Dissolving the Question","date":"2017-01-29T20:52","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703752c6/my-current-take-on-the-paul-miri-disagreement-on-alignability-of-messy-ai"},{"x":"9.608042","y":"8.647288","title":"On motivations for MIRI’s highly reliable agent design research","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"Machine Intelligence Research Institute (MIRI)/AI","date":"2017-01-29T19:34","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375321/on-motivations-for-miri-s-highly-reliable-agent-design-research"},{"x":"12.183935","y":"8.330909","title":"Strategies for coalitions in unit-sum games","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-01-23T04:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375325/strategies-for-coalitions-in-unit-sum-games"},{"x":"12.125157","y":"9.299192","title":"An impossibility result for doing without good priors","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-01-20T05:44","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375323/an-impossibility-result-for-doing-without-good-priors"},{"x":"12.409573","y":"9.96416","title":"A measure-theoretic generalization of logical induction","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Logical Induction/Logical Uncertainty","date":"2017-01-18T13:56","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037531b/a-measure-theoretic-generalization-of-logical-induction"},{"x":"12.201155","y":"9.659811","title":"Open problem: thin logical priors","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2017-01-11T20:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375313/open-problem-thin-logical-priors"},{"x":"12.30972","y":"8.860012","title":"Towards learning incomplete models using inner prediction markets","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2017-01-08T13:37","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037530a/towards-learning-incomplete-models-using-inner-prediction"},{"x":"9.337736","y":"9.076391","title":"Pursuing convergent instrumental subgoals on the user’s behalf doesn’t always require good priors","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-12-30T02:36","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703752da/pursuing-convergent-instrumental-subgoals-on-the-user-s-behalf-doesn-t-always-require-good-priors"},{"x":"9.232357","y":"8.9430895","title":"My recent posts","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"","date":"2016-11-29T18:51","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703752a1/my-recent-posts"},{"x":"11.599133","y":"8.24135","title":"Predicting HCH using expert advice","cluster":"2","author":"['jessicata']","source":"alignment forum","tags":"Humans Consulting HCH","date":"2016-11-28T03:38","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037529f/predicting-hch-using-expert-advice"},{"x":"9.380392","y":"7.624846","title":"ALBA requires incremental design of good long-term memory systems","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-11-28T02:10","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037529e/alba-requires-incremental-design-of-good-long-term-memory-systems"},{"x":"11.675814","y":"7.1691036","title":"Online Learning 3: Adversarial bandit learning with catastrophes","cluster":"0","author":"['RyanCarey']","source":"alignment forum","tags":"","date":"2016-11-14T22:58","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037529b/online-learning-3-adversarial-bandit-learning-with-catastrophes"},{"x":"11.688817","y":"9.814427","title":"postCDT: Decision Theory using post-selected Bayes nets","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2016-11-06T22:22","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375292/postcdt-decision-theory-using-post-selected-bayes-nets"},{"x":"11.999497","y":"9.582696","title":"Updatelessness and Son of X","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Updateless Decision Theory","date":"2016-11-04T22:58","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037528e/updatelessness-and-son-of-x"},{"x":"12.632894","y":"9.775169","title":"A failed attempt at Updatelessness using Universal Inductors","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2016-11-03T20:25","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037528c/a-failed-attempt-at-updatelessness-using-universal-inductors"},{"x":"11.500668","y":"6.6967416","title":"Vector-Valued Reinforcement Learning","cluster":"0","author":"['orthonormal']","source":"alignment forum","tags":"Reinforcement Learning","date":"2016-11-01T00:21","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375286/vector-valued-reinforcement-learning"},{"x":"11.615784","y":"7.89302","title":"Online Learning 1: Bias-detecting online learners","cluster":"1","author":"['RyanCarey']","source":"alignment forum","tags":"","date":"2016-10-29T16:45","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037525e/online-learning-1-bias-detecting-online-learners"},{"x":"12.581944","y":"9.444777","title":"Training Garrabrant inductors to predict counterfactuals","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2016-10-27T02:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037527b/training-garrabrant-inductors-to-predict-counterfactuals"},{"x":"11.6132965","y":"9.248443","title":"Desiderata for decision theory","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2016-10-27T02:10","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037527a/desiderata-for-decision-theory"},{"x":"12.39048","y":"8.959631","title":"Transitive negotiations with counterfactual agents","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Counterfactuals","date":"2016-10-20T23:27","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375274/transitive-negotiations-with-counterfactual-agents"},{"x":"12.016574","y":"9.090116","title":"Attacking the grain of truth problem using Bayes-Savage agents","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2016-10-20T14:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375273/attacking-the-grain-of-truth-problem-using-bayes-savage"},{"x":"9.02428","y":"8.915927","title":"Control and security","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"","date":"2016-10-15T21:11","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375265/control-and-security"},{"x":"12.672993","y":"10.378811","title":"Logical inductor limits are dense under pointwise convergence","cluster":"1","author":"['SamEisenstat']","source":"alignment forum","tags":"Formal Proof","date":"2016-10-06T08:07","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037525d/logical-inductor-limits-are-dense-under-pointwise-convergence"},{"x":"12.679533","y":"9.871978","title":"The set of Logical Inductors is not Convex","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Logical Induction/Logical Uncertainty","date":"2016-09-27T09:05","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375250/the-set-of-logical-inductors-is-not-convex"},{"x":"12.777024","y":"9.687116","title":"Logical Inductors contain Logical Inductors over other complexity classes","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2016-09-26T22:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037524f/logical-inductors-contain-logical-inductors-over-other-complexity-classes"},{"x":"12.55191","y":"10.030395","title":"Logical Inductors that trust their limits","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Logical Induction/Logical Uncertainty","date":"2016-09-20T23:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037523a/logical-inductors-that-trust-their-limits"},{"x":"10.934517","y":"6.8731003","title":"(C)IRL is not solely a learning process","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Inverse Reinforcement Learning","date":"2016-09-15T08:35","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037520e/c-irl-is-not-solely-a-learning-process"},{"x":"12.692698","y":"10.088071","title":"Universal Inductors","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2016-09-14T00:09","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037520a/universal-inductors"},{"x":"10.771995","y":"7.2538915","title":"IRL is hard","cluster":"4","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2016-09-13T14:55","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375209/irl-is-hard"},{"x":"9.50789","y":"7.783837","title":"Modeling the capabilities of advanced AI systems as episodic reinforcement learning","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"Reinforcement Learning","date":"2016-08-19T02:52","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751eb/modeling-the-capabilities-of-advanced-ai-systems-as-episodic-reinforcement-learning"},{"x":"11.769199","y":"9.086919","title":"Can we hybridize Absent-Minded Driver with Death in Damascus?","cluster":"1","author":"['Eliezer Yudkowsky']","source":"alignment forum","tags":"","date":"2016-08-01T21:43","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751e6/can-we-hybridize-absent-minded-driver-with-death-in-damascus"},{"x":"11.265103","y":"7.6820784","title":"Learning (meta-)preferences","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-07-27T14:43","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751db/learning-meta-preferences"},{"x":"11.178498","y":"7.6330395","title":"What does an imperfect agent want?","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-07-27T14:03","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751dd/what-does-an-imperfect-agent-want"},{"x":"12.975947","y":"9.179101","title":"Three Oracle designs","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI","date":"2016-07-20T15:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751d1/three-oracle-designs"},{"x":"10.184413","y":"8.4846325","title":"Simpler, cruder, virtual world AIs","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-06-26T15:44","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751c3/simpler-cruder-virtual-world-ais"},{"x":"11.818272","y":"10.7286215","title":"Finite Factored Sets: Orthogonality and Time","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Finite Factored Sets","date":"2021-06-10T01:22","url":"https://www.lesswrong.com/posts/yT7QdN2wEubR8exAH/finite-factored-sets-orthogonality-and-time"},{"x":"11.168937","y":"10.114503","title":"Knowledge is not just mutual information","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Rationality/AI/World Modeling/Truth, Semantics, & Meaning","date":"2021-06-10T01:01","url":"https://www.lesswrong.com/posts/QLosiQsPJepZWtXG4/knowledge-is-not-just-mutual-information"},{"x":"8.495434","y":"7.2732534","title":"A naive alignment strategy and optimism about generalization","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-06-10T00:10","url":"https://www.lesswrong.com/posts/QvtHSsZLFCAHmzes7/a-naive-alignment-strategy-and-optimism-about-generalization"},{"x":"10.4693","y":"6.9326544","title":"AXRP Episode 8 - Assistance Games with Dylan Hadfield-Menell","cluster":"0","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Inverse Reinforcement Learning/Corrigibility/Interviews/Center for Human-Compatible AI (CHAI)/Audio/AXRP","date":"2021-06-08T23:20","url":"https://www.lesswrong.com/posts/fzFyCJ6gB9kBL9RqW/axrp-episode-8-assistance-games-with-dylan-hadfield-menell"},{"x":"9.206224","y":"7.5325794","title":"The Inside View #3: Evan Hubinger— homogeneity in takeoff speeds, learned optimization and interpretability","cluster":"4","author":"['Michaël Trazzi']","source":"alignment forum","tags":"AI/Myopia","date":"2021-06-08T19:20","url":"https://www.lesswrong.com/posts/NFfZsWrzALPdw54NL/the-inside-view-3-evan-hubinger-homogeneity-in-takeoff"},{"x":"8.466272","y":"9.809651","title":"Survey on AI existential risk scenarios","cluster":"3","author":"['Sam Clarke', 'Alexis Carlier', 'Jonas Schuett']","source":"alignment forum","tags":"Threat Models/Surveys/AI/AI Risk/AI Safety Camp","date":"2021-06-08T17:12","url":"https://www.lesswrong.com/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios"},{"x":"11.58798","y":"9.017353","title":"The reverse Goodhart problem","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Goodhart's Law","date":"2021-06-08T15:48","url":"https://www.lesswrong.com/posts/4RH5cMSBLZcv8DEw2/the-reverse-goodhart-problem"},{"x":"10.392861","y":"7.9548106","title":"Supplement to \"Big picture of phasic dopamine\"","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/Reinforcement Learning/World Modeling","date":"2021-06-08T13:08","url":"https://www.lesswrong.com/posts/BwaxYiJ3ZmXHLoZJ6/supplement-to-big-picture-of-phasic-dopamine"},{"x":"10.189423","y":"7.6979585","title":"Big picture of phasic dopamine","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuroscience/Reinforcement Learning/AI/World Modeling","date":"2021-06-08T13:07","url":"https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine"},{"x":"12.19382","y":"8.307342","title":"Game-theoretic Alignment in terms of Attainable Utility","cluster":"1","author":"['midco', 'TurnTrout']","source":"alignment forum","tags":"AI/Game Theory","date":"2021-06-08T12:36","url":"https://www.lesswrong.com/posts/buaGz3aiqCotzjKie/game-theoretic-alignment-in-terms-of-attainable-utility"},{"x":"10.515382","y":"7.9608607","title":"Dangerous optimisation includes variance minimisation","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-06-08T11:34","url":"https://www.lesswrong.com/posts/nbvd4o9uDPe5whFxa/dangerous-optimisation-includes-variance-minimisation"},{"x":"11.616415","y":"8.361866","title":"Conservative Agency with Multiple Stakeholders","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Principal-Agent Problems/Conservatism (AI)","date":"2021-06-08T00:30","url":"https://www.lesswrong.com/posts/gLfHp8XaWpfsmXyWZ/conservative-agency-with-multiple-stakeholders"},{"x":"7.9433975","y":"7.356513","title":"Speculations against GPT-n writing alignment papers","cluster":"2","author":"['Donald Hobson']","source":"alignment forum","tags":"AI/GPT/Transparency / Interpretability (ML & AI)","date":"2021-06-07T21:13","url":"https://www.lesswrong.com/posts/wkhfytDQvfx3Jeie9/speculations-against-gpt-n-writing-alignment-papers"},{"x":"8.022694","y":"9.74975","title":"Some AI Governance Research Ideas","cluster":"3","author":"['Alexis Carlier', 'markusanderljung']","source":"alignment forum","tags":"AI","date":"2021-06-07T14:40","url":"https://www.lesswrong.com/posts/RBsTG5F2LqsMaqdzP/some-ai-governance-research-ideas"},{"x":"8.058649","y":"7.375677","title":"Review of \"Learning Normativity: A Research Agenda\"","cluster":"4","author":"['Gyrodiot', 'adamShimi', 'Joe_Collman']","source":"alignment forum","tags":"AI/Intellectual Progress via LessWrong","date":"2021-06-06T13:33","url":"https://www.lesswrong.com/posts/ykvw6sMQD7JXK5cdJ/review-of-learning-normativity-a-research-agenda"},{"x":"10.483099","y":"7.58278","title":"Search-in-Territory vs Search-in-Map","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"Rationality","date":"2021-06-05T23:22","url":"https://www.lesswrong.com/posts/s2KJWLAPyjtmQ9ze3/search-in-territory-vs-search-in-map"},{"x":"12.111488","y":"9.301172","title":"The Nature of Counterfactuals","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Decision Theory/Counterfactuals","date":"2021-06-05T09:18","url":"https://www.lesswrong.com/posts/T4Mef9ZkL4WftQBqw/the-nature-of-counterfactuals"},{"x":"11.075306","y":"10.118537","title":"The underlying model of a morphism","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling/AI","date":"2021-06-04T22:29","url":"https://www.lesswrong.com/posts/GRAWAqfgZEgtuCvje/the-underlying-model-of-a-morphism"},{"x":"11.734314","y":"10.670168","title":"Finite Factored Sets: Introduction and Factorizations","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Causality/Abstraction/Finite Factored Sets","date":"2021-06-04T17:41","url":"https://www.lesswrong.com/posts/sZa5LQg6rrWgMR4Jx/finite-factored-sets-introduction-and-factorizations"},{"x":"11.0645275","y":"9.883387","title":"SIA is basically just Bayesian updating on existence","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Sleeping Beauty Paradox/World Modeling","date":"2021-06-04T13:17","url":"https://www.lesswrong.com/posts/BYy62ib5tAkn9rsKn/sia-is-basically-just-bayesian-updating-on-existence"},{"x":"12.341309","y":"10.302522","title":"An Intuitive Guide to Garrabrant Induction","cluster":"1","author":"['Mark Xu']","source":"alignment forum","tags":"Logical Induction/AI","date":"2021-06-03T22:21","url":"https://www.lesswrong.com/posts/y5GftLezdozEHdXkL/an-intuitive-guide-to-garrabrant-induction"},{"x":"8.030731","y":"9.153053","title":"Rogue AGI Embodies Valuable Intellectual Property","cluster":"3","author":"['Mark Xu', 'CarlShulman']","source":"alignment forum","tags":"AI/Threat Models/Economic Consequences of AGI/AI Risk","date":"2021-06-03T20:37","url":"https://www.lesswrong.com/posts/FM49gHBrs5GTx7wFf/rogue-agi-embodies-valuable-intellectual-property"},{"x":"7.4880943","y":"7.063736","title":"Thoughts on the Alignment Implications of Scaling Language Models","cluster":"2","author":"['leogao']","source":"alignment forum","tags":"Scaling Laws/GPT/AI/Machine Learning/World Modeling/Outer Alignment/Language Models","date":"2021-06-02T21:32","url":"https://www.lesswrong.com/posts/EmxfgPGvaKqhttPM8/thoughts-on-the-alignment-implications-of-scaling-language"},{"x":"8.756091","y":"9.147895","title":"\"Existential risk from AI\" survey results","cluster":"3","author":"['Rob Bensinger']","source":"alignment forum","tags":"AI/Surveys","date":"2021-06-01T20:02","url":"https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results"},{"x":"7.8345423","y":"7.8408613","title":"[Event] Weekly Alignment Research Coffee Time","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"AI","date":"2021-05-29T13:26","url":"https://www.lesswrong.com/posts/cysgh8zpmvt56f6Qw/event-weekly-alignment-research-coffee-time"},{"x":"8.672764","y":"7.142906","title":"Teaching ML to answer questions honestly instead of predicting human answers","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-05-28T17:30","url":"https://www.lesswrong.com/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of"},{"x":"11.660996","y":"6.245757","title":"The blue-minimising robot and model splintering","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-05-28T15:09","url":"https://www.lesswrong.com/posts/BeeirdrMXCPYZwgfj/the-blue-minimising-robot-and-model-splintering"},{"x":"8.197334","y":"8.823114","title":"AXRP Episode 7.5 - Forecasting Transformative AI from Biological Anchors with Ajeya Cotra","cluster":"3","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Forecasting & Prediction/Inside/Outside View/Interviews/AXRP","date":"2021-05-28T00:20","url":"https://www.lesswrong.com/posts/CuDYhLLXq6FuHvGZc/axrp-episode-7-5-forecasting-transformative-ai-from"},{"x":"8.427225","y":"9.088245","title":"[Question] List of good AI safety project ideas?","cluster":"3","author":"['Aryeh Englander']","source":"alignment forum","tags":"AI","date":"2021-05-26T22:36","url":"https://www.lesswrong.com/posts/stdfRDMF3sFpSsGeG/list-of-good-ai-safety-project-ideas"},{"x":"11.804356","y":"8.337503","title":"MDP models are determined by the agent architecture and the environmental dynamics","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Instrumental Convergence/AI","date":"2021-05-26T00:14","url":"https://www.lesswrong.com/posts/XkXL96H6GknCbT5QH/mdp-models-are-determined-by-the-agent-architecture-and-the"},{"x":"8.973542","y":"9.587066","title":"Decoupling deliberation from competition","cluster":"3","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-05-25T18:50","url":"https://www.lesswrong.com/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition"},{"x":"11.028118","y":"10.000744","title":"Knowledge is not just map/​territory resemblance","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Rationality/World Modeling/AI/Truth, Semantics, & Meaning","date":"2021-05-25T17:58","url":"https://www.lesswrong.com/posts/YLoXcquNkNdsteZYd/knowledge-is-not-just-map-territory-resemblance"},{"x":"11.090439","y":"9.919713","title":"Problems facing a correspondence theory of knowledge","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Rationality/Truth, Semantics, & Meaning","date":"2021-05-24T16:02","url":"https://www.lesswrong.com/posts/YdxG2D3bvG5YsuHpG/problems-facing-a-correspondence-theory-of-knowledge"},{"x":"11.7970495","y":"10.628009","title":"Finite Factored Sets","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Causality/Abstraction/Finite Factored Sets","date":"2021-05-23T20:52","url":"https://www.lesswrong.com/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets"},{"x":"7.7634544","y":"7.823213","title":" [Event] Weekly Alignment Research Coffee Time (05/​24)","cluster":"2","author":"['adamShimi']","source":"alignment forum","tags":"Community","date":"2021-05-21T17:45","url":"https://www.lesswrong.com/posts/KGJC6HLG5hcFR7pM4/event-weekly-alignment-research-coffee-time-05-24"},{"x":"9.775283","y":"9.035852","title":"AI Safety Research Project Ideas","cluster":"4","author":"['Owain_Evans']","source":"alignment forum","tags":"AI/AI Risk/Practical","date":"2021-05-21T13:39","url":"https://www.lesswrong.com/posts/f69LK7CndhSNA7oPn/ai-safety-research-project-ideas"},{"x":"12.291984","y":"10.328761","title":"Response to \"What does the universal prior actually look like?\"","cluster":"1","author":"['michaelcohen']","source":"alignment forum","tags":"World Modeling/Solomonoff Induction/Inner Alignment","date":"2021-05-20T16:12","url":"https://www.lesswrong.com/posts/n2Gseb3XFpMyc2FEb/response-to-what-does-the-universal-prior-actually-look-like"},{"x":"8.729183","y":"6.6748223","title":"SGD’s Bias","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Machine Learning","date":"2021-05-18T23:19","url":"https://www.lesswrong.com/posts/ej2r2JADoWiEtxkCd/sgd-s-bias"},{"x":"10.734207","y":"9.465494","title":"Saving Time","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Causality/Abstraction/AI/Decision Theory/Agency","date":"2021-05-18T20:11","url":"https://www.lesswrong.com/posts/gEKHX8WKrXGM4roRC/saving-time"},{"x":"8.358388","y":"6.5642323","title":"Knowledge Neurons in Pretrained Transformers","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)","date":"2021-05-17T22:54","url":"https://www.lesswrong.com/posts/LdoKzGom7gPLqEZyQ/knowledge-neurons-in-pretrained-transformers"},{"x":"7.766579","y":"7.8071613","title":" [Event] Weekly Alignment Research Coffee Time (05/​17)","cluster":"2","author":"['adamShimi']","source":"alignment forum","tags":"AI","date":"2021-05-15T22:07","url":"https://www.lesswrong.com/posts/gLRphsnSHefpcqZoF/event-weekly-alignment-research-coffee-time-05-17"},{"x":"9.326491","y":"7.7984576","title":"Intermittent Distillations #3","cluster":"4","author":"['Mark Xu']","source":"alignment forum","tags":"AI","date":"2021-05-15T07:13","url":"https://www.lesswrong.com/posts/jnHxfXgyQj3ALsD5a/intermittent-distillations-3"},{"x":"8.810411","y":"8.690594","title":"AXRP Episode 7 - Side Effects with Victoria Krakovna","cluster":"4","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Impact Measures/Interviews/Audio/AXRP","date":"2021-05-14T03:50","url":"https://www.lesswrong.com/posts/C9vj5ZX3KsgFfwXAN/axrp-episode-7-side-effects-with-victoria-krakovna"},{"x":"8.929959","y":"6.050908","title":"Understanding the Lottery Ticket Hypothesis","cluster":"2","author":"['Alex Flint']","source":"alignment forum","tags":"AI/Lottery Ticket Hypothesis","date":"2021-05-14T00:25","url":"https://www.lesswrong.com/posts/dpzLqQQSs7XRacEfK/understanding-the-lottery-ticket-hypothesis"},{"x":"9.655987","y":"8.783535","title":"Agency in Conway’s Game of Life","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"AI/Agency","date":"2021-05-13T01:07","url":"https://www.lesswrong.com/posts/3SG4WbNPoP8fsuZgs/agency-in-conway-s-game-of-life"},{"x":"10.199268","y":"8.39274","title":"Formal Inner Alignment, Prospectus","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Inner Alignment","date":"2021-05-12T19:57","url":"https://www.lesswrong.com/posts/a7jnbtoKFyvu5qfkd/formal-inner-alignment-prospectus"},{"x":"8.710478","y":"9.643294","title":"Yampolskiy on AI Risk Skepticism","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"AI","date":"2021-05-11T14:50","url":"https://www.lesswrong.com/posts/D3PnBxkj5jkKPm6jr/yampolskiy-on-ai-risk-skepticism"},{"x":"8.979566","y":"6.5461407","title":"Challenge: know everything that the best go bot knows about go","cluster":"2","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)","date":"2021-05-11T05:10","url":"https://www.lesswrong.com/posts/m5frrcYTSH6ENjsc9/challenge-know-everything-that-the-best-go-bot-knows-about"},{"x":"11.534028","y":"9.914594","title":"Human priors, features and models, languages, and Solmonoff induction","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-05-10T10:55","url":"https://www.lesswrong.com/posts/9aFpMtpivqPCBfx2w/human-priors-features-and-models-languages-and-solmonoff"},{"x":"7.800497","y":"7.829698","title":" [Event] Weekly Alignment Research Coffee Time (05/​10)","cluster":"2","author":"['adamShimi']","source":"alignment forum","tags":"AI/Community","date":"2021-05-09T11:05","url":"https://www.lesswrong.com/posts/ErXseAhtiymqRdCq9/event-weekly-alignment-research-coffee-time-05-10"},{"x":"7.8292046","y":"6.9923882","title":"Pre-Training + Fine-Tuning Favors Deception","cluster":"2","author":"['Mark Xu']","source":"alignment forum","tags":"AI/Inner Alignment","date":"2021-05-08T18:36","url":"https://www.lesswrong.com/posts/rZTjsKy4Jvu6krWJt/pre-training-fine-tuning-favors-deception"},{"x":"10.693237","y":"9.395735","title":"Life and expanding steerable consequences","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"World Optimization/AI/AI Takeoff","date":"2021-05-07T18:33","url":"https://www.lesswrong.com/posts/tmZRyXvH9dgopcnuE/life-and-expanding-steerable-consequences"},{"x":"8.405746","y":"7.9466114","title":"Less Realistic Tales of Doom","cluster":"4","author":"['Mark Xu']","source":"alignment forum","tags":"Threat Models/AI/AI Risk","date":"2021-05-06T23:01","url":"https://www.lesswrong.com/posts/ZRTr6rEcpYtfMTDBs/less-realistic-tales-of-doom"},{"x":"8.161424","y":"5.852886","title":"Parsing Chris Mingard on Neural Networks","cluster":"2","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2021-05-06T22:16","url":"https://www.lesswrong.com/posts/5p4ynEJQ8nXxp2sxC/parsing-chris-mingard-on-neural-networks"},{"x":"11.481453","y":"9.7188225","title":"Anthropics: different probabilities, different questions","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Rationality/Anthropics/Sleeping Beauty Paradox","date":"2021-05-06T13:14","url":"https://www.lesswrong.com/posts/LARmKTbpAkEYeG43u/anthropics-different-probabilities-different-questions"},{"x":"9.108157","y":"7.083189","title":"Mundane solutions to exotic problems","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-05-04T18:20","url":"https://www.lesswrong.com/posts/d5m3G3ov5phZu7FX3/mundane-solutions-to-exotic-problems"},{"x":"10.399201","y":"7.5722466","title":"Parsing Abram on Gradations of Inner Alignment Obstacles","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2021-05-04T17:44","url":"https://www.lesswrong.com/posts/pTm6aEvmepJEA5cuK/parsing-abram-on-gradations-of-inner-alignment-obstacles"},{"x":"11.43651","y":"9.426507","title":"Consistencies as (meta-)preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Rationality/World Optimization","date":"2021-05-03T15:10","url":"https://www.lesswrong.com/posts/KptP3J2ThDTnriric/consistencies-as-meta-preferences"},{"x":"7.707647","y":"8.289213","title":"[Weekly Event] Alignment Researcher Coffee Time (in Walled Garden)","cluster":"3","author":"['adamShimi']","source":"alignment forum","tags":"AI","date":"2021-05-02T12:59","url":"https://www.lesswrong.com/posts/2Ps9easGbqdMP6win/weekly-event-alignment-researcher-coffee-time-in-walled"},{"x":"9.120966","y":"6.7599716","title":"Low-stakes alignment","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-04-30T00:10","url":"https://www.lesswrong.com/posts/TPan9sQFuPP6jgEJo/low-stakes-alignment"},{"x":"10.506372","y":"9.583691","title":"Pitfalls of the agent model","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"AI/Agency","date":"2021-04-27T22:19","url":"https://www.lesswrong.com/posts/8HWGXhnCfAPgJYa9D/pitfalls-of-the-agent-model"},{"x":"11.518631","y":"9.603837","title":"Agents Over Cartesian World Models","cluster":"1","author":"['Mark Xu', 'evhub']","source":"alignment forum","tags":"AI/Agency","date":"2021-04-27T02:06","url":"https://www.lesswrong.com/posts/LBNjeGaJZw7QdybMw/agents-over-cartesian-world-models"},{"x":"7.7255583","y":"7.962197","title":"Announcing the Alignment Research Center","cluster":"3","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Organization Updates","date":"2021-04-26T23:30","url":"https://www.lesswrong.com/posts/3ejHFgQihLG4L6WQf/announcing-the-alignment-research-center"},{"x":"10.889879","y":"7.7936134","title":"[Linkpost] Treacherous turns in the wild","cluster":"4","author":"['Mark Xu']","source":"alignment forum","tags":"AI/Treacherous Turn","date":"2021-04-26T22:51","url":"https://www.lesswrong.com/posts/NEa3puQB23FyiifnW/linkpost-treacherous-turns-in-the-wild"},{"x":"7.9845033","y":"8.402207","title":"FAQ: Advice for AI Alignment Researchers","cluster":"3","author":"['Rohin Shah']","source":"alignment forum","tags":"AI/Careers","date":"2021-04-26T18:59","url":"https://www.lesswrong.com/posts/kdwk5aHNjM53PZFKL/faq-advice-for-ai-alignment-researchers"},{"x":"10.651779","y":"9.455082","title":"Beware over-use of the agent model","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"AI/Agency/World Modeling/Map and Territory","date":"2021-04-25T22:19","url":"https://www.lesswrong.com/posts/2QuAcx8XQw7rrXzGC/beware-over-use-of-the-agent-model"},{"x":"9.481815","y":"9.368755","title":"Naturalism and AI alignment","cluster":"4","author":"['Michele Campolo']","source":"alignment forum","tags":"AI","date":"2021-04-24T16:16","url":"https://www.lesswrong.com/posts/Jo2LWuuGEGHHfGZCM/naturalism-and-ai-alignment"},{"x":"12.131669","y":"10.288912","title":"Probability theory and logical induction as lenses","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Rationality/AI","date":"2021-04-23T02:41","url":"https://www.lesswrong.com/posts/Zd5Bsra7ar2pa3bwS/probability-theory-and-logical-induction-as-lenses"},{"x":"7.698392","y":"8.976284","title":"Three reasons to expect long AI timelines","cluster":"3","author":"['Matthew Barnett']","source":"alignment forum","tags":"AI/AI Timelines","date":"2021-04-22T18:44","url":"https://www.lesswrong.com/posts/Z5gPrKTR2oDmm6fqJ/three-reasons-to-expect-long-ai-timelines"},{"x":"7.9800563","y":"5.800408","title":"NTK/​GP Models of Neural Nets Can’t Learn Features","cluster":"2","author":"['interstice']","source":"alignment forum","tags":"AI","date":"2021-04-22T03:01","url":"https://www.lesswrong.com/posts/76cReK4Mix3zKCWNT/ntk-gp-models-of-neural-nets-can-t-learn-features"},{"x":"9.651796","y":"8.261801","title":"Where are intentions to be found?","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"World Optimization/AI","date":"2021-04-21T00:51","url":"https://www.lesswrong.com/posts/EA4Txiuo5Ce2b7iBd/where-are-intentions-to-be-found"},{"x":"9.731909","y":"6.9790063","title":"Gradations of Inner Alignment Obstacles","cluster":"4","author":"['abramdemski']","source":"alignment forum","tags":"Inner Alignment/Lottery Ticket Hypothesis/AI/Mesa-Optimization","date":"2021-04-20T22:18","url":"https://www.lesswrong.com/posts/wpbpvjZCK3JhzpR2D/gradations-of-inner-alignment-obstacles"},{"x":"8.78172","y":"6.014175","title":"Updating the Lottery Ticket Hypothesis","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Lottery Ticket Hypothesis/Machine Learning","date":"2021-04-18T21:45","url":"https://www.lesswrong.com/posts/i9p5KWNWcthccsxqm/updating-the-lottery-ticket-hypothesis"},{"x":"11.900913","y":"8.319694","title":"Superrational Agents Kelly Bet Influence!","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"World Optimization/AI/Values handshakes/Superrationality","date":"2021-04-16T22:08","url":"https://www.lesswrong.com/posts/7EupfLrZ63pbdyb9J/superrational-agents-kelly-bet-influence"},{"x":"10.915223","y":"10.426058","title":"Computing Natural Abstractions: Linear Approximation","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Rationality","date":"2021-04-15T17:47","url":"https://www.lesswrong.com/posts/f6oWbqxEwktfPrKJw/computing-natural-abstractions-linear-approximation"},{"x":"12.033636","y":"9.080843","title":"Intermittent Distillations #2","cluster":"1","author":"['Mark Xu']","source":"alignment forum","tags":"Rationality/AI","date":"2021-04-14T06:47","url":"https://www.lesswrong.com/posts/Rjrq6xPoavgC4JznB/intermittent-distillations-2"},{"x":"12.4472065","y":"9.004401","title":"Identifiability Problem for Superrational Decision Theories","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"Decision Theory/Embedded Agency/Rationality/Game Theory","date":"2021-04-09T20:33","url":"https://www.lesswrong.com/posts/iNGXKB8iExpcLvu55/identifiability-problem-for-superrational-decision-theories"},{"x":"8.593743","y":"6.8035417","title":"Opinions on Interpretable Machine Learning and 70 Summaries of Recent Papers","cluster":"2","author":"['lifelonglearner', 'Peter Hase']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI/Machine Learning","date":"2021-04-09T19:19","url":"https://www.lesswrong.com/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries"},{"x":"12.285972","y":"9.439841","title":"My Current Take on Counterfactuals","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory/Counterfactuals/AI/Rationality","date":"2021-04-09T17:51","url":"https://www.lesswrong.com/posts/yXfka98pZXAmXiyDp/my-current-take-on-counterfactuals"},{"x":"11.513254","y":"7.3091803","title":"Why unriggable *almost* implies uninfluenceable","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-04-09T17:07","url":"https://www.lesswrong.com/posts/k3J3sYgmjMmpkzbbc/why-unriggable-almost-implies-uninfluenceable"},{"x":"9.500585","y":"9.8250885","title":"AXRP Episode 6 - Debate and Imitative Generalization with Beth Barnes","cluster":"4","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Debate (AI safety technique)/Interviews/Audio/AXRP","date":"2021-04-08T21:20","url":"https://www.lesswrong.com/posts/behyPgMWFhXpKi73P/axrp-episode-6-debate-and-imitative-generalization-with-beth"},{"x":"11.100472","y":"8.726214","title":"A possible preference algorithm","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-04-08T18:25","url":"https://www.lesswrong.com/posts/2SrzejmaxnwJBNkFE/a-possible-preference-algorithm"},{"x":"9.595937","y":"8.059066","title":"If you don’t design for extrapolation, you’ll extrapolate poorly—possibly fatally","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-04-08T18:10","url":"https://www.lesswrong.com/posts/4wa9XGnJHB3apPqoq/if-you-don-t-design-for-extrapolation-you-ll-extrapolate"},{"x":"9.350042","y":"8.62954","title":"Solving the whole AGI control problem, version 0.0001","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI Success Models/AI/Corrigibility/Conservatism (AI)/Transparency / Interpretability (ML & AI)/Tool AI","date":"2021-04-08T15:14","url":"https://www.lesswrong.com/posts/Gfw7JMdKirxeSPiAk/solving-the-whole-agi-control-problem-version-0-0001"},{"x":"8.494283","y":"8.32112","title":"Another (outer) alignment failure story","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Threat Models/Outer Alignment/AI Risk/AI","date":"2021-04-07T20:12","url":"https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story"},{"x":"12.178807","y":"9.081807","title":"Which counterfactuals should an AI follow?","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Rationality","date":"2021-04-07T16:47","url":"https://www.lesswrong.com/posts/j7kyt6sHEjukRND8B/which-counterfactuals-should-an-ai-follow"},{"x":"7.652362","y":"7.8636627","title":"Alignment Newsletter Three Year Retrospective","cluster":"3","author":"['Rohin Shah']","source":"alignment forum","tags":"AI","date":"2021-04-07T14:39","url":"https://www.lesswrong.com/posts/L7yHdqRiHKd3FhQ7B/alignment-newsletter-three-year-retrospective"},{"x":"9.276825","y":"8.212526","title":"Testing The Natural Abstraction Hypothesis: Project Intro","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"World Modeling","date":"2021-04-06T21:24","url":"https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro"},{"x":"11.999364","y":"9.8778305","title":"Reflective Bayesianism","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Radical Probabilism/Epistemology/Rationality/AI/Distinctions","date":"2021-04-06T19:48","url":"https://www.lesswrong.com/posts/vpvLqinp4FoigqvKy/reflective-bayesianism"},{"x":"12.05237","y":"9.525815","title":"The Many Faces of Infra-Beliefs","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Infra-Bayesianism/Decision Theory/Counterfactuals","date":"2021-04-06T10:43","url":"https://www.lesswrong.com/posts/GS5P7LLLbSSExb3Sk/the-many-faces-of-infra-beliefs"},{"x":"8.010412","y":"5.9282417","title":"[Question] How do scaling laws work for fine-tuning?","cluster":"2","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI","date":"2021-04-04T12:18","url":"https://www.lesswrong.com/posts/2j7mtf58Zr9XehjxP/how-do-scaling-laws-work-for-fine-tuning"},{"x":"11.8747225","y":"9.706005","title":"Phylactery Decision Theory","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"AI/Decision Theory/Embedded Agency","date":"2021-04-02T20:55","url":"https://www.lesswrong.com/posts/pba68kdmmHrp9oHGG/phylactery-decision-theory"},{"x":"10.03107","y":"7.56864","title":"My take on Michael Littman on \"The HCI of HAI\"","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"AI/Inverse Reinforcement Learning/Reinforcement Learning","date":"2021-04-02T19:51","url":"https://www.lesswrong.com/posts/wydAtj6FkPDHkdtzS/my-take-on-michael-littman-on-the-hci-of-hai"},{"x":"11.477378","y":"8.571789","title":"Learning Russian Roulette","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"Anthropics/Rationality/Decision Theory","date":"2021-04-02T18:56","url":"https://www.lesswrong.com/posts/L5Tf34FXA6weiGwEz/learning-russian-roulette"},{"x":"8.10474","y":"6.7197304","title":"\"AI and Compute\" trend isn’t predictive of what is happening","cluster":"2","author":"['alexlyzhov']","source":"alignment forum","tags":"AI","date":"2021-04-02T00:44","url":"https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening"},{"x":"9.020081","y":"9.431352","title":"What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)","cluster":"3","author":"['Andrew_Critch']","source":"alignment forum","tags":"Threat Models/Moloch/Coordination / Cooperation/Existential Risk/AI","date":"2021-03-31T23:50","url":"https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"},{"x":"11.473228","y":"9.073444","title":"Optimization, speculations on the X and only X problem.","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"World Optimization","date":"2021-03-30T21:38","url":"https://www.lesswrong.com/posts/PhKZgz5Gxw9soHtng/optimization-speculations-on-the-x-and-only-x-problem"},{"x":"8.229733","y":"8.593592","title":"[Question] How do we prepare for final crunch time?","cluster":"3","author":"['Eli Tyre']","source":"alignment forum","tags":"AI/World Optimization","date":"2021-03-30T05:47","url":"https://www.lesswrong.com/posts/wyYubb3eC5FS365nk/how-do-we-prepare-for-final-crunch-time"},{"x":"8.766018","y":"6.592764","title":"Transparency Trichotomy","cluster":"2","author":"['Mark Xu']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)","date":"2021-03-28T20:26","url":"https://www.lesswrong.com/posts/cgJ447adbMAeoKTSt/transparency-trichotomy"},{"x":"8.283704","y":"8.246832","title":"Review of \"Fun with +12 OOMs of Compute\"","cluster":"4","author":"['adamShimi', 'Joe_Collman', 'Gyrodiot']","source":"alignment forum","tags":"AI/Intellectual Progress via LessWrong/AI Timelines/Intellectual Progress (Society-Level)/AI Risk","date":"2021-03-28T14:55","url":"https://www.lesswrong.com/posts/bAAtiG8og7CxH3cXG/review-of-fun-with-12-ooms-of-compute"},{"x":"11.856625","y":"10.418133","title":"Inframeasures and Domain Theory","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Infra-Bayesianism/AI/World Modeling/Domain Theory","date":"2021-03-28T09:19","url":"https://www.lesswrong.com/posts/vrbidMiczaoHBhZGp/inframeasures-and-domain-theory"},{"x":"11.91827","y":"10.440388","title":"Infra-Domain proofs 1","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"World Modeling/AI/Domain Theory/Formal Proof","date":"2021-03-28T09:16","url":"https://www.lesswrong.com/posts/H5zo4L7yv4bnBgexQ/infra-domain-proofs-1"},{"x":"11.940497","y":"10.422724","title":"Infra-Domain Proofs 2","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"AI/World Modeling/Domain Theory/Formal Proof","date":"2021-03-28T09:15","url":"https://www.lesswrong.com/posts/fLRgddjMTBnpbMeiM/infra-domain-proofs-2"},{"x":"10.857011","y":"9.469049","title":"Coherence arguments imply a force for goal-directed behavior","cluster":"1","author":"['KatjaGrace']","source":"alignment forum","tags":"AI/Rationality/Utility Functions/Orthogonality Thesis/Instrumental Convergence/Coherence Arguments","date":"2021-03-26T16:10","url":"https://www.lesswrong.com/posts/DkcdXsP56g9kXyBdq/coherence-arguments-imply-a-force-for-goal-directed-behavior"},{"x":"9.2452345","y":"8.696032","title":"My AGI Threat Model: Misaligned Model-Based RL Agent","cluster":"3","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Threat Models/Reinforcement Learning/Inner Alignment/Outer Alignment","date":"2021-03-25T13:45","url":"https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent"},{"x":"11.828515","y":"9.710302","title":"Why 1-boxing doesn’t imply backwards causation","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Newcomb's Problem/Causality/Decision Theory/Rationality","date":"2021-03-25T02:32","url":"https://www.lesswrong.com/posts/gAAFzqJkfeSHvcwTw/why-1-boxing-doesn-t-imply-backwards-causation"},{"x":"11.498326","y":"9.270542","title":"Toy model of preference, bias, and extra information","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-03-24T10:14","url":"https://www.lesswrong.com/posts/DQhwrir3nCcMtqA2j/toy-model-of-preference-bias-and-extra-information"},{"x":"11.128605","y":"9.366587","title":"Preferences and biases, the information argument","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Rationality","date":"2021-03-23T12:44","url":"https://www.lesswrong.com/posts/dh8WsHfzmQJ5L7bd8/preferences-and-biases-the-information-argument"},{"x":"9.339918","y":"7.1512604","title":"Against evolution as an analogy for how humans will create AGI","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/Evolution/Inner Alignment","date":"2021-03-23T12:29","url":"https://www.lesswrong.com/posts/pz7Mxyr7Ac43tWMaC/against-evolution-as-an-analogy-for-how-humans-will-create"},{"x":"8.262781","y":"7.0959244","title":"My research methodology","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/World Modeling","date":"2021-03-22T21:20","url":"https://www.lesswrong.com/posts/EF5M6CmKRd6qZk27Z/my-research-methodology"},{"x":"12.035631","y":"8.197892","title":"Generalizing POWER to multi-agent games","cluster":"1","author":"['midco', 'TurnTrout']","source":"alignment forum","tags":"World Modeling/AI/Instrumental Convergence","date":"2021-03-22T02:41","url":"https://www.lesswrong.com/posts/MJc9AqyMWpG3BqfyK/generalizing-power-to-multi-agent-games"},{"x":"11.211457","y":"8.127563","title":"Fisherian Runaway as a decision-theoretic problem","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"World Modeling/Evolution/Self Fulfilling/Refuting Prophecies","date":"2021-03-20T16:34","url":"https://www.lesswrong.com/posts/PvbzCuj293D5Hxvu3/fisherian-runaway-as-a-decision-theoretic-problem"},{"x":"11.522416","y":"10.215005","title":"HCH Speculation Post #2A","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Humans Consulting HCH/AI","date":"2021-03-17T13:26","url":"https://www.lesswrong.com/posts/MnCMkh7hirX8YwT2t/hch-speculation-post-2a"},{"x":"9.069533","y":"6.5767484","title":"Intermittent Distillations #1","cluster":"2","author":"['Mark Xu']","source":"alignment forum","tags":"Rationality/AI","date":"2021-03-17T05:15","url":"https://www.lesswrong.com/posts/pqkdsqd6s6w2HtT9g/intermittent-distillations-1"},{"x":"8.02305","y":"6.111908","title":"Comments on \"The Singularity is Nowhere Near\"","cluster":"2","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/AI Timelines","date":"2021-03-16T23:59","url":"https://www.lesswrong.com/posts/P7P2iG4zvBNANvQFK/comments-on-the-singularity-is-nowhere-near"},{"x":"8.15732","y":"8.187183","title":"AI x-risk reduction: why I chose academia over industry","cluster":"3","author":"['capybaralet']","source":"alignment forum","tags":"World Optimization/AI","date":"2021-03-14T17:25","url":"https://www.lesswrong.com/posts/4jFnquoHuoaTqdphu/ai-x-risk-reduction-why-i-chose-academia-over-industry"},{"x":"10.742825","y":"8.165863","title":"Four Motivations for Learning Normativity","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI","date":"2021-03-11T20:13","url":"https://www.lesswrong.com/posts/oqghwKKifztYWLsea/four-motivations-for-learning-normativity"},{"x":"10.579875","y":"9.149848","title":"Behavioral Sufficient Statistics for Goal-Directedness","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/AI/AI Risk","date":"2021-03-11T15:01","url":"https://www.lesswrong.com/posts/jkRFZNAZmWskTdCSt/behavioral-sufficient-statistics-for-goal-directedness"},{"x":"11.521818","y":"7.1378736","title":"TASP Ep 3 - Optimal Policies Tend to Seek Power","cluster":"0","author":"['Quinn']","source":"alignment forum","tags":"AI/Instrumental Convergence","date":"2021-03-11T01:44","url":"https://www.lesswrong.com/posts/eM6SgkXDbFXav4kD4/tasp-ep-3-optimal-policies-tend-to-seek-power"},{"x":"10.12136","y":"7.8402543","title":"Open Problems with Myopia","cluster":"4","author":"['Mark Xu', 'evhub']","source":"alignment forum","tags":"Myopia/AI/Open Problems","date":"2021-03-10T18:38","url":"https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia"},{"x":"11.319331","y":"10.100066","title":"Extended Picture Theory or Models inside Models inside Models","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Philosophy of Language/Epistemology","date":"2021-03-10T13:24","url":"https://www.lesswrong.com/posts/nvLNjY7aoh2i7JxbB/extended-picture-theory-or-models-inside-models-inside"},{"x":"10.424621","y":"9.491173","title":"AXRP Episode 5 - Infra-Bayesianism with Vanessa Kosoy","cluster":"1","author":"['DanielFilan']","source":"alignment forum","tags":"Infra-Bayesianism/AI/Functional Decision Theory/Counterfactual Mugging/Newcomb's Problem/Epistemology/Interviews/Audio/AXRP","date":"2021-03-10T04:30","url":"https://www.lesswrong.com/posts/FkMPXiomjGBjMfosg/axrp-episode-5-infra-bayesianism-with-vanessa-kosoy"},{"x":"10.565766","y":"9.099097","title":"Towards a Mechanistic Understanding of Goal-Directedness","cluster":"4","author":"['Mark Xu']","source":"alignment forum","tags":"AI/Goal-Directedness","date":"2021-03-09T20:17","url":"https://www.lesswrong.com/posts/nTiAyxFybZ7jgtWvn/towards-a-mechanistic-understanding-of-goal-directedness"},{"x":"9.132803","y":"9.746145","title":"CLR’s recent work on multi-agent systems","cluster":"3","author":"['JesseClifton']","source":"alignment forum","tags":"AI/Risks of Astronomical Suffering (S-risks)/Center on Long-Term Risk (CLR)","date":"2021-03-09T02:28","url":"https://www.lesswrong.com/posts/EzoCZjTdWTMgacKGS/clr-s-recent-work-on-multi-agent-systems"},{"x":"9.999554","y":"9.66417","title":"Epistemological Framing for AI Alignment Research","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"AI/Intellectual Progress (Society-Level)/Practice & Philosophy of Science/AI Risk/Epistemology","date":"2021-03-08T22:05","url":"https://www.lesswrong.com/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research"},{"x":"8.936089","y":"7.9295774","title":"MIRI comments on Cotra’s \"Case for Aligning Narrowly Superhuman Models\"","cluster":"4","author":"['Rob Bensinger']","source":"alignment forum","tags":"Outer Alignment/Transparency / Interpretability (ML & AI)/GPT/AI","date":"2021-03-05T23:43","url":"https://www.lesswrong.com/posts/AyfDnnAdjG7HHeD3d/miri-comments-on-cotra-s-case-for-aligning-narrowly"},{"x":"9.097049","y":"7.8670263","title":"The case for aligning narrowly superhuman models","cluster":"1","author":"['Ajeya Cotra']","source":"alignment forum","tags":"AI/Outer Alignment/Machine Learning/GPT/Language Models","date":"2021-03-05T22:29","url":"https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models"},{"x":"7.801424","y":"5.8215632","title":"Multimodal Neurons in Artificial Neural Networks","cluster":"2","author":"['Kaj_Sotala']","source":"alignment forum","tags":"AI/OpenAI/Machine Learning","date":"2021-03-05T09:01","url":"https://www.lesswrong.com/posts/bgQysKL6Luqacw3SN/multimodal-neurons-in-artificial-neural-networks"},{"x":"12.884625","y":"10.273615","title":"A Semitechnical Introductory Dialogue on Solomonoff Induction","cluster":"1","author":"['Eliezer Yudkowsky']","source":"alignment forum","tags":"Solomonoff Induction/Occam's Razor/World Modeling/AI/Rationality/Dialogue (format)","date":"2021-03-04T17:27","url":"https://www.lesswrong.com/posts/EL4HNa92Z95FKL9R2/a-semitechnical-introductory-dialogue-on-solomonoff-1"},{"x":"12.117702","y":"8.61604","title":"A non-logarithmic argument for Kelly","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"Rationality/Kelly Criterion/Decision Theory/Betting","date":"2021-03-04T16:21","url":"https://www.lesswrong.com/posts/HLCcTypehEJtstNnD/a-non-logarithmic-argument-for-kelly"},{"x":"10.849624","y":"10.1898","title":"Connecting the good regulator theorem with semantics and symbol grounding","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Rationality/Symbol Grounding","date":"2021-03-04T14:35","url":"https://www.lesswrong.com/posts/uky9nAtnw9WrAjziD/connecting-the-good-regulator-theorem-with-semantics-and"},{"x":"8.3479805","y":"6.3787165","title":"Book review: \"A Thousand Brains\" by Jeff Hawkins","cluster":"2","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuroscience/Neuromorphic AI/AI/Inner Alignment/Outer Alignment","date":"2021-03-04T05:10","url":"https://www.lesswrong.com/posts/ixZLTmFfnKRbaStA5/book-review-a-thousand-brains-by-jeff-hawkins"},{"x":"8.718694","y":"6.644039","title":"How does bee learning compare with machine learning?","cluster":"2","author":"['guicosta']","source":"alignment forum","tags":"AI","date":"2021-03-04T01:59","url":"https://www.lesswrong.com/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning"},{"x":"8.181883","y":"7.026987","title":"Fun with +12 OOMs of Compute","cluster":"2","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI Timelines/AI/World Optimization","date":"2021-03-01T13:30","url":"https://www.lesswrong.com/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute"},{"x":"9.08345","y":"8.381986","title":"Full-time AGI Safety!","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Community","date":"2021-03-01T12:42","url":"https://www.lesswrong.com/posts/tnEQMnpyBFK5QBRz3/full-time-agi-safety"},{"x":"8.787098","y":"7.9973984","title":"Bootstrapped Alignment","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"AI/Goodhart's Law","date":"2021-02-27T15:46","url":"https://www.lesswrong.com/posts/teCsd4Aqg9KDxkaC9/bootstrapped-alignment"},{"x":"9.962466","y":"8.786841","title":"Creating a satisficer","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-03-12T11:31","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374eeb/creating-a-satisficer"},{"x":"11.870039","y":"9.009163","title":"Acausal trade barriers","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-03-11T14:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374eea/acausal-trade-barriers"},{"x":"7.7315836","y":"8.273159","title":"Meta- the goals of this forum","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-03-10T20:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ee8/meta-the-goals-of-this-forum"},{"x":"8.5711975","y":"5.6024203","title":"Statistical learning theory and robust concept learning","cluster":"2","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-03-07T04:13","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ee3/statistical-learning-theory-and-robust-concept-learning"},{"x":"10.53692","y":"7.715584","title":"Proposal: Modeling goal stability in machine learning","cluster":"0","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-03-03T01:31","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374edf/proposal-modeling-goal-stability-in-machine-learning"},{"x":"13.048306","y":"9.263328","title":"Oracle machines for automated philosophy","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Oracle AI","date":"2015-02-17T15:10","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ed9/oracle-machines-for-automated-philosophy"},{"x":"12.189327","y":"9.103336","title":"Un-manipulable counterfactuals","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Counterfactuals","date":"2015-02-12T19:51","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ecc/un-manipulable-counterfactuals"},{"x":"11.592474","y":"8.460618","title":"Resource gathering agent","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-02-12T19:31","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ed5/resource-gathering-agent"},{"x":"12.564474","y":"9.452454","title":"An implementation of modal UDT","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2015-02-11T06:02","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ed6/an-implementation-of-modal-udt"},{"x":"12.785782","y":"9.232978","title":"UDT in the Land of Probabilistic Oracles","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Oracle AI","date":"2015-02-08T09:13","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ed2/udt-in-the-land-of-probabilistic-oracles"},{"x":"12.727986","y":"9.063189","title":"Non-manipulative oracles","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI","date":"2015-02-06T17:05","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374eca/non-manipulative-oracles"},{"x":"11.61071","y":"8.469052","title":"Generalizing the Corrigibility paper’s impossibility result?","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2015-02-04T03:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ec7/generalizing-the-corrigibility-paper-s-impossibility-result"},{"x":"12.432487","y":"9.450293","title":"On notation for modal UDT","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2015-02-03T19:26","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ec3/on-notation-for-modal-udt"},{"x":"12.689517","y":"9.552993","title":"From halting oracles to modal logic","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"Oracle AI","date":"2015-02-03T19:26","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e6a/from-halting-oracles-to-modal-logic"},{"x":"12.210808","y":"9.401641","title":"Third-person counterfactuals","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"Counterfactuals","date":"2015-02-03T01:13","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ebf/third-person-counterfactuals"},{"x":"12.49421","y":"9.277204","title":"The odd counterfactuals of playing chicken","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"Counterfactuals","date":"2015-02-02T07:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ec0/the-odd-counterfactuals-of-playing-chicken"},{"x":"12.322868","y":"9.381365","title":"A different angle on UDT","cluster":"1","author":"['So8res']","source":"alignment forum","tags":"","date":"2015-02-02T02:36","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374eb3/a-different-angle-on-udt"},{"x":"12.062182","y":"9.655266","title":"Why conditioning on \"the agent takes action a\" isn’t enough","cluster":"1","author":"['So8res']","source":"alignment forum","tags":"","date":"2015-02-02T02:35","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374eb9/why-conditioning-on-the-agent-takes-action-a-isn-t-enough"},{"x":"13.028422","y":"9.264343","title":"Multibit reflective oracles","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"Oracle AI","date":"2015-01-25T02:23","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ebe/multibit-reflective-oracles"},{"x":"12.567469","y":"9.419672","title":"\"Evil\" decision problems in provability logic","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2015-01-10T01:04","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e8c/evil-decision-problems-in-provability-logic"},{"x":"9.149264","y":"8.718123","title":"The steering problem","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"","date":"2014-12-29T04:50","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374eb8/the-steering-problem"},{"x":"12.158585","y":"9.256183","title":"Utility indifference and infinite improbability drives","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-29T06:26","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374eab/utility-indifference-and-infinite-improbability-drives"},{"x":"12.532933","y":"9.509916","title":"Uniqueness of UDT for transparent universes","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2014-11-24T05:57","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ea8/uniqueness-of-udt-for-transparent-universes"},{"x":"12.53051","y":"9.522093","title":"Improving the modal UDT optimality result","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-23T22:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ea9/improving-the-modal-udt-optimality-result"},{"x":"10.598407","y":"9.066966","title":"Trustworthy automated philosophy?","cluster":"4","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-21T02:57","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ea3/trustworthy-automated-philosophy"},{"x":"10.656962","y":"9.146098","title":"Stable self-improvement as a research problem","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"Recursive Self-Improvement","date":"2014-11-17T17:51","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e92/stable-self-improvement-as-a-research-problem"},{"x":"12.917858","y":"10.17425","title":"Approximability","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2014-11-17T00:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e97/approximability"},{"x":"12.260942","y":"9.366385","title":"Topological truth predicates: Towards a model of perfect Bayesian agents","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-15T06:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e5f/topological-truth-predicates-towards-a-model-of-perfect-bayesian-agents"},{"x":"13.02958","y":"9.606423","title":"Oracle machines instead of topological truth predicates","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-15T06:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e6f/oracle-machines-instead-of-topological-truth-predicates"},{"x":"13.020017","y":"9.264853","title":"Simplicity priors with reflective oracles","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"Oracle AI/Priors","date":"2014-11-15T06:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e76/simplicity-priors-with-reflective-oracles"},{"x":"12.5885725","y":"9.105424","title":"A primer on provability logic","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-15T06:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e86/a-primer-on-provability-logic"},{"x":"12.576484","y":"9.557114","title":"An optimality result for modal UDT","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-15T06:38","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e8f/an-optimality-result-for-modal-udt"},{"x":"11.5311165","y":"8.134574","title":"Predictors that don’t try to manipulate you(?)","cluster":"4","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-15T05:53","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e90/predictors-that-don-t-try-to-manipulate-you"},{"x":"10.2902975","y":"9.186683","title":"Exploiting EDT","cluster":"4","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-10T19:59","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e7d/exploiting-edt"},{"x":"10.5429","y":"9.428091","title":"Welcome!","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2014-11-04T03:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e5e/welcome"},{"x":"11.765036","y":"10.721033","title":"Exploring Finite Factored Sets with some toy examples","cluster":"1","author":"['Thomas Kehrenberg']","source":"alignment forum","tags":"AI/Finite Factored Sets","date":"2022-03-19T22:08","url":"https://www.lesswrong.com/posts/hxuKtHH4jTdtmEAbK/exploring-finite-factored-sets-with-some-toy-examples"},{"x":"9.411747","y":"7.916494","title":"[Intro to brain-like-AGI safety] 8. Takeaways from neuro 1/​2: On AGI development","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuromorphic AI/AI","date":"2022-03-16T13:59","url":"https://www.lesswrong.com/posts/fDPsYdDtkzhBp9A8D/intro-to-brain-like-agi-safety-8-takeaways-from-neuro-1-2-on"},{"x":"9.795541","y":"6.587902","title":"ELK contest submission: route understanding through the human ontology","cluster":"0","author":"['Vika', 'Ramana Kumar', 'Vikrant Varma']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-03-14T21:42","url":"https://www.lesswrong.com/posts/QrhCsuaEmSLzc8NQ4/elk-contest-submission-route-understanding-through-the-human"},{"x":"10.502209","y":"9.20209","title":"A Longlist of Theories of Impact for Interpretability","cluster":"1","author":"['Neel Nanda']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI","date":"2022-03-11T14:55","url":"https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"},{"x":"7.9779897","y":"7.8471413","title":"ELK Sub—Note-taking in internal rollouts","cluster":"2","author":"['Hoagy']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-03-09T17:23","url":"https://www.lesswrong.com/posts/fftQP7zrnYkDqgwfj/elk-sub-note-taking-in-internal-rollouts"},{"x":"8.347606","y":"6.7123327","title":"It Looks Like You’re Trying To Take Over The World","cluster":"2","author":"['gwern']","source":"alignment forum","tags":"Fiction/AI/Paperclip Maximizer","date":"2022-03-09T16:35","url":"https://www.lesswrong.com/posts/a5e9arCnbDac9Doig/it-looks-like-you-re-trying-to-take-over-the-world"},{"x":"9.844188","y":"8.003546","title":"[Intro to brain-like-AGI safety] 7. From hardcoded drives to foresighted plans: A worked example","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuromorphic AI","date":"2022-03-09T14:28","url":"https://www.lesswrong.com/posts/zXibERtEWpKuG5XAC/intro-to-brain-like-agi-safety-7-from-hardcoded-drives-to"},{"x":"10.942604","y":"9.045928","title":"ELK prize results","cluster":"1","author":"['paulfchristiano', 'Mark Xu']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-03-09T00:01","url":"https://www.lesswrong.com/posts/zjMKpSB2Xccn9qi5t/elk-prize-results"},{"x":"10.421758","y":"8.555465","title":"Value extrapolation, concept extrapolation, model splintering","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning/AI","date":"2022-03-08T22:50","url":"https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering"},{"x":"8.112595","y":"5.041175","title":"[MLSN #3]: NeurIPS Safety Paper Roundup","cluster":"2","author":"['Dan Hendrycks']","source":"alignment forum","tags":"AI","date":"2022-03-08T15:17","url":"https://www.lesswrong.com/posts/dhbLE8BqRvhBtsXhS/mlsn-3-neurips-safety-paper-roundup"},{"x":"8.108827","y":"6.657781","title":"Projecting compute trends in Machine Learning","cluster":"2","author":"['Tamay', 'lennart', 'Jsevillamol']","source":"alignment forum","tags":"AI/Forecasting & Prediction/AI Timelines/Moore's Law","date":"2022-03-07T15:32","url":"https://www.lesswrong.com/posts/3dBtgKCkJh5yCHbag/projecting-compute-trends-in-machine-learning-2"},{"x":"9.828372","y":"7.974001","title":"[Intro to brain-like-AGI safety] 6. Big picture of motivation, decision-making, and RL","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Reinforcement Learning/Neuromorphic AI","date":"2022-03-02T15:26","url":"https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"},{"x":"8.458876","y":"6.708778","title":"Musings on the Speed Prior","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-03-02T04:04","url":"https://www.lesswrong.com/posts/GC69Hmc6ZQDM9xC3w/musings-on-the-speed-prior"},{"x":"8.662945","y":"8.397928","title":"Shah and Yudkowsky on alignment failures","cluster":"4","author":"['Rohin Shah', 'Eliezer Yudkowsky']","source":"alignment forum","tags":"AI","date":"2022-02-28T19:18","url":"https://www.lesswrong.com/posts/tcCxPLBrEXdxN5HCQ/shah-and-yudkowsky-on-alignment-failures"},{"x":"11.23161","y":"10.132724","title":"ELK Thought Dump","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Truth, Semantics, & Meaning/Eliciting Latent Knowledge (ELK)/AI","date":"2022-02-28T18:46","url":"https://www.lesswrong.com/posts/eqzbXmqGqXiyjX3TP/elk-thought-dump-1"},{"x":"10.124527","y":"9.554328","title":"How I Formed My Own Views About AI Safety","cluster":"3","author":"['Neel Nanda']","source":"alignment forum","tags":"AI/AI Risk/Rationality/Inside/Outside View/Humility","date":"2022-02-27T18:50","url":"https://www.lesswrong.com/posts/JZrN4ckaCfd6J37cG/how-i-formed-my-own-views-about-ai-safety"},{"x":"7.602726","y":"7.1080465","title":"How do new models from OpenAI, DeepMind and Anthropic perform on TruthfulQA?","cluster":"2","author":"['Owain_Evans']","source":"alignment forum","tags":"AI/OpenAI/DeepMind/Truth, Semantics, & Meaning/Honesty/Anthropic/Outer Alignment","date":"2022-02-26T12:46","url":"https://www.lesswrong.com/posts/yYkrbS5iAwdEQyynW/how-do-new-models-from-openai-deepmind-and-anthropic-perform"},{"x":"10.972057","y":"10.36319","title":"The Big Picture Of Alignment (Talk Part 2)","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"AI","date":"2022-02-25T02:53","url":"https://www.lesswrong.com/posts/aEtc5GgqJGFtTH2kQ/the-big-picture-of-alignment-talk-part-2-1"},{"x":"8.134077","y":"6.3861766","title":"Transformer inductive biases & RASP","cluster":"2","author":"['Vivek Hebbar']","source":"alignment forum","tags":"AI","date":"2022-02-24T00:42","url":"https://www.lesswrong.com/posts/kwpvEpDXsivbmdYhr/transformer-inductive-biases-and-rasp"},{"x":"7.919851","y":"8.493105","title":"A comment on Ajeya Cotra’s draft report on AI timelines","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"AI Timelines/AI","date":"2022-02-24T00:41","url":"https://www.lesswrong.com/posts/qnjDGitKxYaesbsem/a-comment-on-ajeya-cotra-s-draft-report-on-ai-timelines"},{"x":"8.650617","y":"7.0141697","title":"Christiano and Yudkowsky on AI predictions and human intelligence","cluster":"2","author":"['Eliezer Yudkowsky']","source":"alignment forum","tags":"AI","date":"2022-02-23T21:34","url":"https://www.lesswrong.com/posts/NbGmfxbaABPsspib7/christiano-and-yudkowsky-on-ai-predictions-and-human"},{"x":"10.539828","y":"10.301447","title":"More GPT-3 and symbol grounding","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"GPT/AI","date":"2022-02-23T18:30","url":"https://www.lesswrong.com/posts/QppXf4yfcG8JAKhnw/more-gpt-3-and-symbol-grounding"},{"x":"9.819016","y":"8.038608","title":"[Intro to brain-like-AGI safety] 5. The \"long-term predictor\", and TD learning","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuromorphic AI/Reinforcement Learning","date":"2022-02-23T14:44","url":"https://www.lesswrong.com/posts/F759WQ8iKjqBncDki/intro-to-brain-like-agi-safety-5-the-long-term-predictor-and"},{"x":"9.752635","y":"7.1658983","title":"ELK Proposal: Thinking Via A Human Imitator","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-02-22T01:52","url":"https://www.lesswrong.com/posts/z3xTDPDsndJBmHLFH/elk-proposal-thinking-via-a-human-imitator"},{"x":"10.22235","y":"10.033134","title":"Ngo and Yudkowsky on scientific reasoning and pivotal acts","cluster":"1","author":"['Eliezer Yudkowsky', 'Richard_Ngo']","source":"alignment forum","tags":"AI/General Intelligence","date":"2022-02-21T20:54","url":"https://www.lesswrong.com/posts/cCrpbZ4qTCEYXbzje/ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts"},{"x":"8.25389","y":"7.4805694","title":"Alignment research exercises","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/Exercises / Problem-Sets","date":"2022-02-21T20:24","url":"https://www.lesswrong.com/posts/kj37Hzb2MsALwLqWt/alignment-research-exercises"},{"x":"9.927192","y":"8.805277","title":"The Big Picture Of Alignment (Talk Part 1)","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"AI","date":"2022-02-21T05:49","url":"https://www.lesswrong.com/posts/xdSDFQs4aC5GrdHNZ/the-big-picture-of-alignment-talk-part-1"},{"x":"8.983312","y":"7.246342","title":"Two Challenges for ELK","cluster":"4","author":"['derek shiller']","source":"alignment forum","tags":"AI/Eliciting Latent Knowledge (ELK)","date":"2022-02-21T05:49","url":"https://www.lesswrong.com/posts/rxQbX2JpigjnbnL3A/two-challenges-for-elk"},{"x":"8.037934","y":"8.888241","title":"Alignment researchers, how useful is extra compute for you?","cluster":"3","author":"['Lauro Langosco']","source":"alignment forum","tags":"AI","date":"2022-02-19T15:35","url":"https://www.lesswrong.com/posts/A4djH6sc9vZq2AYBD/alignment-researchers-how-useful-is-extra-compute-for-you-1"},{"x":"11.544212","y":"9.834143","title":"Implications of automated ontology identification","cluster":"1","author":"['Alex Flint', 'adamShimi', 'Robert Miles']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI/Ontology","date":"2022-02-18T03:30","url":"https://www.lesswrong.com/posts/LRgM9cuLNPbsjwEdN/implications-of-automated-ontology-identification"},{"x":"9.489462","y":"8.644443","title":"Why I’m co-founding Aligned AI","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Community","date":"2022-02-17T19:55","url":"https://www.lesswrong.com/posts/vBoq5yd7qbYoGKCZK/why-i-m-co-founding-aligned-ai"},{"x":"11.339716","y":"9.700186","title":"REPL’s and ELK","cluster":"1","author":"['scottviteri']","source":"alignment forum","tags":"AI/Eliciting Latent Knowledge (ELK)","date":"2022-02-17T01:14","url":"https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk"},{"x":"8.109897","y":"6.359947","title":"Compute Trends Across Three eras of Machine Learning","cluster":"2","author":"['Jsevillamol', 'Pablo Villalobos', 'lennart', 'Marius Hobbhahn', 'Tamay Besiroglu', 'anson.ho']","source":"alignment forum","tags":"AI/Machine Learning/Scaling Laws/Moore's Law/Technological Forecasting","date":"2022-02-16T14:18","url":"https://www.lesswrong.com/posts/XKtybmbjhC6mXDm5z/compute-trends-across-three-eras-of-machine-learning"},{"x":"9.350588","y":"8.055797","title":"[Intro to brain-like-AGI safety] 4. The \"short-term predictor\"","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuromorphic AI","date":"2022-02-16T13:12","url":"https://www.lesswrong.com/posts/Y3bkJ59j4dciiLYyw/intro-to-brain-like-agi-safety-4-the-short-term-predictor"},{"x":"9.900672","y":"8.277873","title":"Is ELK enough? Diamond, Matrix and Child AI","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-02-15T02:29","url":"https://www.lesswrong.com/posts/XjDcwtgkHGWYA7stn/is-elk-enough-diamond-matrix-and-child-ai"},{"x":"10.801538","y":"8.903088","title":"Some Hacky ELK Ideas","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-02-15T02:27","url":"https://www.lesswrong.com/posts/3gAKoaziTXmvHusRv/some-hacky-elk-ideas"},{"x":"10.812893","y":"10.4291525","title":"What Does The Natural Abstraction Framework Say About ELK?","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/Abstraction/AI","date":"2022-02-15T02:27","url":"https://www.lesswrong.com/posts/HuqwRug3v6z3gEgKK/what-does-the-natural-abstraction-framework-say-about-elk"},{"x":"10.894039","y":"10.465472","title":"Abstractions as Redundant Information","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"World Modeling","date":"2022-02-13T04:17","url":"https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information"},{"x":"8.684184","y":"7.863735","title":"A summary of aligning narrowly superhuman models","cluster":"4","author":"['gugu']","source":"alignment forum","tags":"Narrow AI/AI","date":"2022-02-10T18:26","url":"https://www.lesswrong.com/posts/TSxAXeHHhgSxR5wGZ/a-summary-of-aligning-narrowly-superhuman-models"},{"x":"11.141445","y":"9.263084","title":"Inferring utility functions from locally non-transitive preferences","cluster":"1","author":"['Jan']","source":"alignment forum","tags":"Utility Functions/AI","date":"2022-02-10T10:33","url":"https://www.lesswrong.com/posts/QZiGEDiobFz8ropA5/inferring-utility-functions-from-locally-non-transitive"},{"x":"9.196385","y":"7.997223","title":"[Intro to brain-like-AGI safety] 3. Two subsystems: Learning & Steering","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/AI Timelines","date":"2022-02-09T13:09","url":"https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"},{"x":"8.199532","y":"6.0468097","title":"Hypothesis: gradient descent prefers general circuits","cluster":"2","author":"['Quintin Pope']","source":"alignment forum","tags":"Gradient Descent/AI/Optimization","date":"2022-02-08T21:12","url":"https://www.lesswrong.com/posts/JFibrXBewkSDmixuo/hypothesis-gradient-descent-prefers-general-circuits"},{"x":"9.421885","y":"7.3199773","title":"How complex are myopic imitators?","cluster":"4","author":"['Vivek Hebbar']","source":"alignment forum","tags":"AI/Myopia/Inner Alignment","date":"2022-02-08T12:00","url":"https://www.lesswrong.com/posts/2eRgFFeeS7pR4R8nD/how-complex-are-myopic-imitators-1"},{"x":"10.578398","y":"8.999409","title":"A broad basin of attraction around human values?","cluster":"1","author":"['Wei_Dai']","source":"alignment forum","tags":"Corrigibility/AI","date":"2022-02-08T05:00","url":"https://www.lesswrong.com/posts/TrvkWBwYvvJjSqSCj/a-broad-basin-of-attraction-around-human-values"},{"x":"9.050642","y":"9.0546055","title":"Paradigm-building: Introduction","cluster":"3","author":"['Cameron Berg']","source":"alignment forum","tags":"AI/Research Agendas/AI Risk/Community","date":"2022-02-08T00:06","url":"https://www.lesswrong.com/posts/4TuzWEKysvYdhRXLd/paradigm-building-introduction"},{"x":"8.527325","y":"8.916838","title":"Alignment versus AI Alignment","cluster":"3","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2022-02-04T22:59","url":"https://www.lesswrong.com/posts/hTfyX4823wKqnoFnS/alignment-versus-ai-alignment"},{"x":"7.9458337","y":"7.0666924","title":"QNR prospects are important for AI alignment research","cluster":"2","author":"['Eric Drexler']","source":"alignment forum","tags":"AI","date":"2022-02-03T15:20","url":"https://www.lesswrong.com/posts/FKE6cAzQxEK4QH9fC/qnr-prospects-are-important-for-ai-alignment-research"},{"x":"8.850842","y":"8.709282","title":"Thoughts on AGI safety from the top","cluster":"4","author":"['jylin04']","source":"alignment forum","tags":"AI/AI Risk/AI Timelines/Future of Humanity Institute (FHI)","date":"2022-02-02T20:06","url":"https://www.lesswrong.com/posts/ApLnWjgMwBTJt6buC/thoughts-on-agi-safety-from-the-top"},{"x":"9.231577","y":"8.134788","title":"[Intro to brain-like-AGI safety] 2. \"Learning from scratch\" in the brain","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuroscience/AI","date":"2022-02-02T13:22","url":"https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"},{"x":"10.6792","y":"10.2958555","title":"Causality, Transformative AI and alignment—part I","cluster":"1","author":"['Marius Hobbhahn']","source":"alignment forum","tags":"AI/Transformative AI","date":"2022-01-27T16:18","url":"https://www.lesswrong.com/posts/oqzasmQ9Lye45QDMZ/causality-transformative-ai-and-alignment-part-i"},{"x":"8.812052","y":"9.032716","title":"Arguments about Highly Reliable Agent Designs as a Useful Path to Artificial Intelligence Safety","cluster":"4","author":"['riceissa', 'Davidmanheim']","source":"alignment forum","tags":"AI/Agent Foundations","date":"2022-01-27T13:13","url":"https://www.lesswrong.com/posts/hWtpqjYXAvFExmAsD/arguments-about-highly-reliable-agent-designs-as-a-useful"},{"x":"9.066273","y":"8.239729","title":"[Intro to brain-like-AGI safety] 1. What’s the problem & Why work on it now?","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuromorphic AI","date":"2022-01-26T15:23","url":"https://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why"},{"x":"11.150642","y":"8.798577","title":"ELK First Round Contest Winners","cluster":"1","author":"['Mark Xu', 'paulfchristiano']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)","date":"2022-01-26T02:56","url":"https://www.lesswrong.com/posts/qXFbGzS3Sg2NhrNAu/elk-first-round-contest-winners"},{"x":"11.639397","y":"7.970056","title":"Instrumental Convergence For Realistic Agent Objectives","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence","date":"2022-01-22T00:41","url":"https://www.lesswrong.com/posts/W22Btd7NmGuucFejc/instrumental-convergence-for-realistic-agent-objectives"},{"x":"10.566966","y":"8.476647","title":"What’s Up With Confusingly Pervasive Consequentialism?","cluster":"1","author":"['Raemon']","source":"alignment forum","tags":"Consequentialism/AI/Rationality","date":"2022-01-20T19:22","url":"https://www.lesswrong.com/posts/DJnvFsZ2maKxPi7v7/what-s-up-with-confusingly-pervasive-consequentialism"},{"x":"7.9980826","y":"6.161987","title":"Estimating training compute of Deep Learning models","cluster":"2","author":"['lennart', 'Jsevillamol', 'Marius Hobbhahn', 'Tamay Besiroglu', 'anson.ho']","source":"alignment forum","tags":"AI/AI Capabilities/Scaling Laws","date":"2022-01-20T16:12","url":"https://www.lesswrong.com/posts/HvqQm6o8KnwxbdmhZ/estimating-training-compute-of-deep-learning-models"},{"x":"10.242735","y":"7.3538046","title":"Scalar reward is not enough for aligned AGI","cluster":"4","author":"['Peter Vamplew']","source":"alignment forum","tags":"Reinforcement Learning/AI","date":"2022-01-17T21:02","url":"https://www.lesswrong.com/posts/eeEEgNeTepZb6F6NF/scalar-reward-is-not-enough-for-aligned-agi"},{"x":"7.537633","y":"7.1547008","title":"Truthful LMs as a warm-up for aligned AGI","cluster":"2","author":"['Jacob_Hilton']","source":"alignment forum","tags":"Language Models/AI/Honesty/AI Risk/Outer Alignment/Truth, Semantics, & Meaning/GPT/Machine Learning","date":"2022-01-17T16:49","url":"https://www.lesswrong.com/posts/jWkqACmDes6SoAiyE/truthful-lms-as-a-warm-up-for-aligned-agi"},{"x":"8.219789","y":"5.7505736","title":"Different way classifiers can be diverse","cluster":"2","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2022-01-17T16:30","url":"https://www.lesswrong.com/posts/rv65vAPqpZGFLcnnD/different-way-classifiers-can-be-diverse"},{"x":"8.425797","y":"9.299002","title":"Challenges with Breaking into MIRI-Style Research","cluster":"3","author":"['Chris_Leong']","source":"alignment forum","tags":"Agent Foundations/AI Risk/AI/Machine Intelligence Research Institute (MIRI)","date":"2022-01-17T09:23","url":"https://www.lesswrong.com/posts/Kcbo4rXu3jYPnauoK/challenges-with-breaking-into-miri-style-research"},{"x":"10.973872","y":"8.150413","title":"The Greedy Doctor Problem… turns out to be relevant to the ELK problem?","cluster":"1","author":"['Jan']","source":"alignment forum","tags":"AI/Eliciting Latent Knowledge (ELK)","date":"2022-01-14T11:58","url":"https://www.lesswrong.com/posts/dDzHJGmyeQa2tGmqH/the-greedy-doctor-problem-turns-out-to-be-relevant-to-the"},{"x":"9.769526","y":"8.103364","title":"New year, new research agenda post ","cluster":"4","author":"['Charlie Steiner']","source":"alignment forum","tags":"Research Agendas/AI","date":"2022-01-12T17:58","url":"https://www.lesswrong.com/posts/zuHezdoGr2KtM2n43/new-year-new-research-agenda-post"},{"x":"10.79986","y":"7.4280095","title":"Value extrapolation partially resolves symbol grounding","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Value Learning","date":"2022-01-12T16:30","url":"https://www.lesswrong.com/posts/thZdioHTZALRPKmiH/value-extrapolation-partially-resolves-symbol-grounding"},{"x":"8.242875","y":"7.225064","title":"Understanding the two-head strategy for teaching ML to answer questions honestly","cluster":"2","author":"['Adam Scherlis']","source":"alignment forum","tags":"AI/Eliciting Latent Knowledge (ELK)","date":"2022-01-11T23:24","url":"https://www.lesswrong.com/posts/Ntmbm79zQakr29XLw/understanding-the-two-head-strategy-for-teaching-ml-to"},{"x":"8.778279","y":"7.5597224","title":"Future ML Systems Will Be Qualitatively Different","cluster":"4","author":"['jsteinhardt']","source":"alignment forum","tags":"AI/Machine Learning","date":"2022-01-11T19:50","url":"https://www.lesswrong.com/posts/pZaPhGg2hmmPwByHc/future-ml-systems-will-be-qualitatively-different"},{"x":"11.16528","y":"8.572785","title":"Importance of foresight evaluations within ELK","cluster":"1","author":"['Jonathan Uesato']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2022-01-06T15:34","url":"https://www.lesswrong.com/posts/mvGNKQ6iSDf3d4gCi/importance-of-foresight-evaluations-within-elk"},{"x":"8.727647","y":"7.715912","title":"More Is Different for AI","cluster":"4","author":"['jsteinhardt']","source":"alignment forum","tags":"AI/AI Risk","date":"2022-01-04T19:30","url":"https://www.lesswrong.com/posts/Lp4Q9kSGsJHLfoHX3/more-is-different-for-ai"},{"x":"11.694741","y":"9.901588","title":"Promising posts on AF that have fallen through the cracks","cluster":"1","author":"['Evan R. Murphy']","source":"alignment forum","tags":"AI/Community/Site Meta","date":"2022-01-04T15:39","url":"https://www.lesswrong.com/posts/WerwgmeYZYGC2hKXN/promising-posts-on-af-that-have-fallen-through-the-cracks"},{"x":"8.099672","y":"7.0151143","title":"Apply for research internships at ARC!","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"Community/AI","date":"2022-01-03T20:26","url":"https://www.lesswrong.com/posts/BRsxztzkTzScFQfDW/apply-for-research-internships-at-arc"},{"x":"8.320447","y":"6.8828273","title":"Prizes for ELK proposals","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI/Bounties (closed)/Community","date":"2022-01-03T20:23","url":"https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals"},{"x":"10.649928","y":"8.734397","title":"How an alien theory of mind might be unlearnable","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning/AI","date":"2022-01-03T11:16","url":"https://www.lesswrong.com/posts/kMJxwCZ4mc9w4ezbs/how-an-alien-theory-of-mind-might-be-unlearnable"},{"x":"12.167696","y":"9.399504","title":"$1000 USD prize—Circular Dependency of Counterfactuals","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Decision Theory/Bounties (active)/World Modeling","date":"2022-01-01T09:43","url":"https://www.lesswrong.com/posts/Gzw6FwPD9FeL4GTWC/usd1000-usd-prize-circular-dependency-of-counterfactuals"},{"x":"11.136141","y":"9.244097","title":"Counterexamples to some ELK proposals","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2021-12-31T17:05","url":"https://www.lesswrong.com/posts/FnZws8NuKw6BJzmvZ/counterexamples-to-some-elk-proposals"},{"x":"11.482416","y":"9.530324","title":"Eliciting Latent Knowledge Via Hypothetical Sensors","cluster":"1","author":"['John_Maxwell']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2021-12-30T15:53","url":"https://www.lesswrong.com/posts/H7v5yyXAmmgu9DJmi/eliciting-latent-knowledge-via-hypothetical-sensors"},{"x":"8.621886","y":"6.435249","title":"Reverse-engineering using interpretability","cluster":"2","author":"['Beth Barnes']","source":"alignment forum","tags":"AI","date":"2021-12-29T23:21","url":"https://www.lesswrong.com/posts/qrn2dRSwNratuM3tq/reverse-engineering-using-interpretability"},{"x":"9.011265","y":"6.6054735","title":"Gradient Hacking via Schelling Goals","cluster":"2","author":"['Adam Scherlis']","source":"alignment forum","tags":"AI/Gradient Hacking/Inner Alignment","date":"2021-12-28T20:38","url":"https://www.lesswrong.com/posts/A9eAPjpFjPwNW2rku/gradient-hacking-via-schelling-goals"},{"x":"8.422734","y":"9.362075","title":"My Overview of the AI Alignment Landscape: Threat Models","cluster":"3","author":"['Neel Nanda']","source":"alignment forum","tags":"AI/Inner Alignment/Outer Alignment/Coordination / Cooperation/Goodhart's Law/World Modeling/AI Risk/Existential Risk/Threat Models","date":"2021-12-25T23:07","url":"https://www.lesswrong.com/posts/3DFBbPFZyscrAiTKS/my-overview-of-the-ai-alignment-landscape-threat-models"},{"x":"8.431414","y":"8.635432","title":"Risks from AI persuasion","cluster":"3","author":"['Beth Barnes']","source":"alignment forum","tags":"AI","date":"2021-12-24T01:48","url":"https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion"},{"x":"8.42295","y":"8.001911","title":"Reply to Eliezer on Biological Anchors","cluster":"4","author":"['HoldenKarnofsky']","source":"alignment forum","tags":"AI","date":"2021-12-23T16:15","url":"https://www.lesswrong.com/posts/nNqXfnjiezYukiMJi/reply-to-eliezer-on-biological-anchors"},{"x":"8.102659","y":"9.769708","title":"2021 AI Alignment Literature Review and Charity Comparison","cluster":"3","author":"['Larks']","source":"alignment forum","tags":"AI/Academic Papers/Literature Reviews","date":"2021-12-23T14:06","url":"https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison"},{"x":"9.042334","y":"8.006258","title":"Worst-case thinking in AI alignment","cluster":"4","author":"['Buck']","source":"alignment forum","tags":"AI","date":"2021-12-23T01:29","url":"https://www.lesswrong.com/posts/yTvBSFrXhZfL8vr5a/worst-case-thinking-in-ai-alignment"},{"x":"7.79592","y":"6.7982616","title":"Transformer Circuits","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)/Language Models","date":"2021-12-22T21:09","url":"https://www.lesswrong.com/posts/2269iGRnWruLHsZ5r/transformer-circuits"},{"x":"10.302279","y":"7.149467","title":"Demanding and Designing Aligned Cognitive Architectures","cluster":"4","author":"['Koen.Holtman']","source":"alignment forum","tags":"AI/AI Governance/Reinforcement Learning/Academic Papers","date":"2021-12-21T17:32","url":"https://www.lesswrong.com/posts/cDR8GkzCaxXoovPwh/demanding-and-designing-aligned-cognitive-architectures"},{"x":"11.020251","y":"8.7921095","title":"Don’t Influence the Influencers!","cluster":"1","author":"['lhc']","source":"alignment forum","tags":"AI","date":"2021-12-19T09:02","url":"https://www.lesswrong.com/posts/Eg9FE2iYp3ngySsMD/don-t-influence-the-influencers"},{"x":"12.012814","y":"9.496787","title":"Exploring Decision Theories With Counterfactuals and Dynamic Agent Self-Pointers","cluster":"1","author":"['JoshuaOSHickman']","source":"alignment forum","tags":"AI/Embedded Agency/Decision Theory","date":"2021-12-18T21:50","url":"https://www.lesswrong.com/posts/zupqBxrNKpT5dhFQb/exploring-decision-theories-with-counterfactuals-and-dynamic"},{"x":"10.243986","y":"8.867528","title":" Equilibrium and prior selection problems in multipolar deployment","cluster":"3","author":"['JesseClifton']","source":"alignment forum","tags":"AI Governance/AI","date":"2020-04-02T20:06","url":"https://www.lesswrong.com/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1"},{"x":"11.705211","y":"9.410092","title":"[Question] What is the subjective experience of free will for agents?","cluster":"1","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Causality/Consciousness/Decision Theory/Free Will","date":"2020-04-02T15:53","url":"https://www.lesswrong.com/posts/BTM4SN53mWsHLkRJL/what-is-the-subjective-experience-of-free-will-for-agents"},{"x":"12.211236","y":"9.454135","title":"Two Alternatives to Logical Counterfactuals","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Decision Theory/Counterfactuals","date":"2020-04-01T09:48","url":"https://www.lesswrong.com/posts/yBdDXXmLYejrcPPv2/two-alternatives-to-logical-counterfactuals"},{"x":"9.208253","y":"7.3043194","title":"How special are human brains among animal brains? ","cluster":"4","author":"['zhukeepa']","source":"alignment forum","tags":"Biology/General Intelligence/AI Timelines/Consciousness/Neuroscience","date":"2020-04-01T01:35","url":"https://www.lesswrong.com/posts/d2jgBurQygbXzhPxc/how-special-are-human-brains-among-animal-brains"},{"x":"11.240497","y":"9.055214","title":"Meta-preferences two ways: generator vs. patch","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Philosophy/Metaethics","date":"2020-04-01T00:51","url":"https://www.lesswrong.com/posts/A5jN7vqAxsHCDC4dy/meta-preferences-two-ways-generator-vs-patch"},{"x":"9.862022","y":"5.798802","title":"Outperforming the human Atari benchmark","cluster":"2","author":"['Vaniver']","source":"alignment forum","tags":"AI","date":"2020-03-31T19:33","url":"https://www.lesswrong.com/posts/kwgM5nGe9QXcB4TTu/outperforming-the-human-atari-benchmark"},{"x":"8.543601","y":"8.535122","title":"Three Kinds of Competitiveness","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI","date":"2020-03-31T01:00","url":"https://www.lesswrong.com/posts/sD6KuprcS3PFym2eM/three-kinds-of-competitiveness"},{"x":"8.125507","y":"9.497702","title":"Openness Norms in AGI Development","cluster":"3","author":"['Sublation']","source":"alignment forum","tags":"","date":"2020-03-30T19:02","url":"https://www.lesswrong.com/posts/RvrTZ3qKWpg9aiFqZ/openness-norms-in-agi-development"},{"x":"7.681024","y":"8.5658045","title":"My current framework for thinking about AGI timelines","cluster":"3","author":"['zhukeepa']","source":"alignment forum","tags":"AI Timelines/AI/AI Takeoff/Crucial Considerations","date":"2020-03-30T01:23","url":"https://www.lesswrong.com/posts/w4jjwDPa853m9P4ag/my-current-framework-for-thinking-about-agi-timelines"},{"x":"8.512492","y":"9.624757","title":"[Question] What are the most plausible \"AI Safety warning shot\" scenarios?","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI","date":"2020-03-26T20:59","url":"https://www.lesswrong.com/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios"},{"x":"11.986845","y":"8.898766","title":"How important are MDPs for AGI (Safety)?","cluster":"1","author":"['michaelcohen']","source":"alignment forum","tags":"AI","date":"2020-03-26T20:32","url":"https://www.lesswrong.com/posts/6gL83HMF6tvPHKQxW/how-important-are-mdps-for-agi-safety"},{"x":"9.502929","y":"9.348108","title":"Deconfusing Human Values Research Agenda v1","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Research Agendas/AI/Value Learning/Perceptual Control Theory/Metaethics","date":"2020-03-23T16:25","url":"https://www.lesswrong.com/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1"},{"x":"8.097878","y":"8.826294","title":"[Question] [Meta] Do you want AIS Webinars?","cluster":"3","author":"['Linda Linsefors']","source":"alignment forum","tags":"AI/Community","date":"2020-03-21T16:01","url":"https://www.lesswrong.com/posts/BbrsgHPJmGxeg7nXG/meta-do-you-want-ais-webinars"},{"x":"12.133857","y":"9.541327","title":"Mediation From a Distance","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-03-20T22:02","url":"https://www.lesswrong.com/posts/DqfpcFwfeZHFe5J8h/mediation-from-a-distance"},{"x":"12.230093","y":"10.002105","title":"Thinking About Filtered Evidence Is (Very!) Hard","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Filtered Evidence/Rationality","date":"2020-03-19T23:20","url":"https://www.lesswrong.com/posts/fhJkQo34cYw6KqpH3/thinking-about-filtered-evidence-is-very-hard"},{"x":"7.7072005","y":"8.934741","title":"Alignment as Translation","cluster":"3","author":"['johnswentworth']","source":"alignment forum","tags":"AI","date":"2020-03-19T21:40","url":"https://www.lesswrong.com/posts/42YykiTqtGMyJAjDM/alignment-as-translation"},{"x":"10.97213","y":"10.4776945","title":"Abstraction = Information at a Distance","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-03-19T00:19","url":"https://www.lesswrong.com/posts/TTNS3tk5McHqrJCbR/abstraction-information-at-a-distance"},{"x":"11.014234","y":"9.032165","title":"What is Interpretability?","cluster":"1","author":"['RobertKirk', 'Tomáš Gavenčiak', 'spirali']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI","date":"2020-03-17T20:23","url":"https://www.lesswrong.com/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability"},{"x":"8.229514","y":"9.781303","title":"AI Alignment Podcast: On Lethal Autonomous Weapons with Paul Scharre","cluster":"3","author":"['Palus Astra']","source":"alignment forum","tags":"Transcripts/Interviews/Autonomous Weapons","date":"2020-03-16T23:00","url":"https://www.lesswrong.com/posts/xEzudcydk7APZbnai/ai-alignment-podcast-on-lethal-autonomous-weapons-with-paul"},{"x":"10.579487","y":"7.741043","title":"[Question] Positive Feedback → Optimization?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"","date":"2020-03-16T18:48","url":"https://www.lesswrong.com/posts/moxPbPfd3EW9e89dJ/positive-feedback-greater-than-optimization"},{"x":"9.425152","y":"9.522605","title":"[Question] What are some exercises for building/​generating intuitions about key disagreements in AI alignment?","cluster":"4","author":"['riceissa']","source":"alignment forum","tags":"Intuition","date":"2020-03-16T07:41","url":"https://www.lesswrong.com/posts/bDwQddhqaTiMhbpPF/what-are-some-exercises-for-building-generating-intuitions"},{"x":"11.566543","y":"10.4322405","title":"Trace README","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/Programming/AI","date":"2020-03-11T21:08","url":"https://www.lesswrong.com/posts/DbWoZNxgwr2NBFdoo/trace-readme"},{"x":"8.038353","y":"5.8243933","title":"Zoom In: An Introduction to Circuits","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI/Programming/OpenAI/Logic & Mathematics ","date":"2020-03-10T19:36","url":"https://www.lesswrong.com/posts/MG4ZjWQDrdpgeu8wG/zoom-in-an-introduction-to-circuits"},{"x":"12.337681","y":"8.841315","title":"Vulnerabilities in CDT and TI-unaware agents","cluster":"1","author":"['PabloAMC', 'Davide_Zagami', 'Chris_Leong']","source":"alignment forum","tags":"","date":"2020-03-10T14:14","url":"https://www.lesswrong.com/posts/vFXK8eQdLhicYNNqF/vulnerabilities-in-cdt-and-ti-unaware-agents"},{"x":"12.853244","y":"10.257569","title":"[Question] Name of Problem?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"","date":"2020-03-09T20:15","url":"https://www.lesswrong.com/posts/8Zq5f7pLqkPkneuSq/name-of-problem"},{"x":"11.995226","y":"9.449781","title":"Subjective implication decision theory in critical agentialism","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2020-03-05T23:30","url":"https://www.lesswrong.com/posts/t7jGT7uyf56TGjeba/subjective-implication-decision-theory-in-critical"},{"x":"11.829508","y":"9.703759","title":"A critical agential account of free will, causation, and physics","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Free Will/Causality/Agency","date":"2020-03-05T07:57","url":"https://www.lesswrong.com/posts/dvaCebTNc2tfMDcxS/a-critical-agential-account-of-free-will-causation-and"},{"x":"11.514535","y":"9.651876","title":"Embedded vs. External Decision Problems","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"","date":"2020-03-05T00:23","url":"https://www.lesswrong.com/posts/br7KRSeNymwSvZnf5/embedded-vs-external-decision-problems"},{"x":"9.0704365","y":"9.58227","title":"Robustness to fundamental uncertainty in AGI alignment","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2018-07-27T00:41","url":"https://www.lesswrong.com/posts/J8Dt7bvsT5B4sWS6k/robustness-to-fundamental-uncertainty-in-agi-alignment"},{"x":"11.135467","y":"10.035002","title":"Anthropics over-simplified: it’s about priors, not updates","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Anthropics","date":"2020-03-02T13:45","url":"https://www.lesswrong.com/posts/Hpam4RrJKfufXrmAi/anthropics-over-simplified-it-s-about-priors-not-updates"},{"x":"10.5250845","y":"8.15194","title":"If I were a well-intentioned AI…","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Mesa-Optimization/Inner Alignment","date":"2020-03-02T12:16","url":"https://www.lesswrong.com/posts/aqhMLqaoHb7uob7fr/if-i-were-a-well-intentioned-ai-iv-mesa-optimising"},{"x":"8.529214","y":"8.177975","title":"An Analytic Perspective on AI Alignment","cluster":"4","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)","date":"2020-03-01T04:10","url":"https://www.lesswrong.com/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment"},{"x":"8.083329","y":"9.65257","title":"Cortés, Pizarro, and Afonso as Precedents for Takeover","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"History/AI","date":"2020-03-01T03:49","url":"https://www.lesswrong.com/posts/ivpKSjM4D6FbqF4pZ/cortes-pizarro-and-afonso-as-precedents-for-takeover"},{"x":"11.663296","y":"10.357722","title":"Trace: Goals and Principles","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-02-28T23:50","url":"https://www.lesswrong.com/posts/rt5X74Az3mXwTubRA/trace-goals-and-principles"},{"x":"11.339422","y":"8.474001","title":"Conclusion to ‘Reframing Impact’","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI","date":"2020-02-28T16:05","url":"https://www.lesswrong.com/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact"},{"x":"10.289305","y":"7.85087","title":"If I were a well-intentioned AI…","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Outer Alignment/AI/Goodhart's Law","date":"2020-02-28T11:24","url":"https://www.lesswrong.com/posts/NdJtfujX4sE6xLCsb/if-i-were-a-well-intentioned-ai-iii-extremal-goodhart"},{"x":"9.402684","y":"8.42906","title":"Reasons for Excitement about Impact of Impact Measure Research","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI","date":"2020-02-27T21:42","url":"https://www.lesswrong.com/posts/wAAvP8RG6EwzCvHJy/reasons-for-excitement-about-impact-of-impact-measure"},{"x":"10.092812","y":"7.55196","title":"If I were a well-intentioned AI…","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Outer Alignment/AI","date":"2020-02-27T11:58","url":"https://www.lesswrong.com/posts/ZKzAjKSeNRtiaeJns/if-i-were-a-well-intentioned-ai-ii-acting-in-a-world"},{"x":"11.257321","y":"8.159058","title":"Attainable Utility Preservation: Scaling to Superhuman","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI","date":"2020-02-27T00:52","url":"https://www.lesswrong.com/posts/S8AGyJJsdBFXmxHcb/attainable-utility-preservation-scaling-to-superhuman"},{"x":"10.980531","y":"9.993629","title":"Where’s the Turing Machine? A step towards Ontology Identification","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"","date":"2020-02-26T17:10","url":"https://www.lesswrong.com/posts/tDXFRfkvijTzs2Mmr/where-s-the-turing-machine-a-step-towards-ontology"},{"x":"8.98741","y":"7.8181443","title":"If I were a well-intentioned AI…","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Outer Alignment/Machine Learning/Adversarial Examples","date":"2020-02-26T12:39","url":"https://www.lesswrong.com/posts/gzWb5kWwzhdaqmyTt/if-i-were-a-well-intentioned-ai-i-image-classifier"},{"x":"10.931233","y":"8.380837","title":"Other versions of \"No free lunch in value learning\"","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning","date":"2020-02-25T14:25","url":"https://www.lesswrong.com/posts/LRYwpq8i9ym7Wuyoc/other-versions-of-no-free-lunch-in-value-learning"},{"x":"11.274619","y":"7.894684","title":"How Low Should Fruit Hang Before We Pick It?","cluster":"0","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI","date":"2020-02-25T02:08","url":"https://www.lesswrong.com/posts/LfGzAduBWzY5gq6FE/how-low-should-fruit-hang-before-we-pick-it"},{"x":"11.276352","y":"8.169203","title":"Subagents and impact measures, full and fully illustrated","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Impact Measures","date":"2020-02-24T13:12","url":"https://www.lesswrong.com/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated"},{"x":"11.335858","y":"8.084716","title":"Attainable Utility Preservation: Empirical Results","cluster":"1","author":"['TurnTrout', 'nealeratzlaff']","source":"alignment forum","tags":"Impact Measures/AI","date":"2020-02-22T00:38","url":"https://www.lesswrong.com/posts/4J4TA2ZF3wmSxhxuc/attainable-utility-preservation-empirical-results"},{"x":"7.7670593","y":"8.977508","title":"Will AI undergo discontinuous progress?","cluster":"3","author":"['Sammy Martin']","source":"alignment forum","tags":"AI Takeoff/AI/World Modeling","date":"2020-02-21T22:16","url":"https://www.lesswrong.com/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress"},{"x":"10.54813","y":"8.151641","title":"Goal-directed = Model-based RL?","cluster":"0","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness","date":"2020-02-20T19:13","url":"https://www.lesswrong.com/posts/Tux9WH4daKcxjEetQ/goal-directed-model-based-rl"},{"x":"10.806757","y":"7.395483","title":"Curiosity Killed the Cat and the Asymptotically Optimal Agent","cluster":"4","author":"['michaelcohen']","source":"alignment forum","tags":"AI","date":"2020-02-20T17:28","url":"https://www.lesswrong.com/posts/fSC98Cy3zR9GsEPnT/curiosity-killed-the-cat-and-the-asymptotically-optimal"},{"x":"9.994694","y":"7.520934","title":"Tessellating Hills: a toy model for demons in imperfect search","cluster":"2","author":"['DaemonicSigil']","source":"alignment forum","tags":"Inner Alignment/Optimization/Programming","date":"2020-02-20T00:12","url":"https://www.lesswrong.com/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect"},{"x":"8.932251","y":"8.859034","title":"On unfixably unsafe AGI architectures","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI Risk/AI","date":"2020-02-19T21:16","url":"https://www.lesswrong.com/posts/qvyv72fCiC46sxfPt/on-unfixably-unsafe-agi-architectures"},{"x":"10.761224","y":"7.068051","title":"Stuck Exploration","cluster":"0","author":"['Chris_Leong']","source":"alignment forum","tags":"","date":"2020-02-19T12:31","url":"https://www.lesswrong.com/posts/ajvvtKuNzh7aHmooT/stuck-exploration"},{"x":"11.199674","y":"8.165787","title":"(In)action rollouts","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2020-02-18T14:48","url":"https://www.lesswrong.com/posts/z9MfmF8gA7SBxGSmb/in-action-rollouts"},{"x":"12.210456","y":"9.19151","title":"Counterfactuals versus the laws of physics","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Counterfactuals","date":"2020-02-18T13:21","url":"https://www.lesswrong.com/posts/bqyCd38tACvKgqmXG/counterfactuals-versus-the-laws-of-physics"},{"x":"11.319849","y":"7.575128","title":"Wireheading and discontinuity","cluster":"1","author":"['Michele Campolo']","source":"alignment forum","tags":"AI/Wireheading","date":"2020-02-18T10:49","url":"https://www.lesswrong.com/posts/KLNDgqQLfpFXbhQak/wireheading-and-discontinuity"},{"x":"11.320952","y":"8.276928","title":"Subagents and impact measures: summary tables","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2020-02-17T14:09","url":"https://www.lesswrong.com/posts/PmqQKBmt2phMT7YLG/subagents-and-impact-measures-summary-tables"},{"x":"11.34353","y":"8.185927","title":"Appendix: mathematics of indexical impact measures","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Impact Measures","date":"2020-02-17T13:22","url":"https://www.lesswrong.com/posts/M9aoMixFLf8JFLRaP/appendix-mathematics-of-indexical-impact-measures"},{"x":"11.100618","y":"8.08357","title":"Stepwise inaction and non-indexical impact measures","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2020-02-17T10:32","url":"https://www.lesswrong.com/posts/JB6edzY5cccrdbQxP/stepwise-inaction-and-non-indexical-impact-measures"},{"x":"11.323001","y":"8.06883","title":"Attainable Utility Preservation: Concepts","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI","date":"2020-02-17T05:20","url":"https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts"},{"x":"12.981341","y":"9.562244","title":"On the falsifiability of hypercomputation, part 2: finite input streams","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Falsifiability","date":"2020-02-17T03:51","url":"https://www.lesswrong.com/posts/PtaN3oMFPfAAuBNtw/on-the-falsifiability-of-hypercomputation-part-2-finite"},{"x":"11.86371","y":"9.485851","title":"Reference Post: Trivial Decision Problem","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Decision Theory","date":"2020-02-15T17:13","url":"https://www.lesswrong.com/posts/XAeWHqQTWjJmzB4k6/reference-post-trivial-decision-problem"},{"x":"9.3608055","y":"7.3945904","title":"[Question] What is the difference between robustness and inner alignment?","cluster":"4","author":"['JanBrauner']","source":"alignment forum","tags":"","date":"2020-02-15T13:28","url":"https://www.lesswrong.com/posts/SEmviT8tyPKYkz6mN/what-is-the-difference-between-robustness-and-inner"},{"x":"10.594195","y":"8.347896","title":"Bayesian Evolving-to-Extinction","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Myopia/Bayes' Theorem/AI","date":"2020-02-14T23:55","url":"https://www.lesswrong.com/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction"},{"x":"10.503769","y":"7.827274","title":"The Catastrophic Convergence Conjecture","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI/Instrumental Convergence","date":"2020-02-14T21:16","url":"https://www.lesswrong.com/posts/w6BtMqKRLxG9bNLMr/the-catastrophic-convergence-conjecture"},{"x":"10.970996","y":"9.741654","title":"The Reasonable Effectiveness of Mathematics or: AI vs sandwiches","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Logic & Mathematics ","date":"2020-02-14T18:46","url":"https://www.lesswrong.com/posts/qpbYwTqKQG8G7mdFK/the-reasonable-effectiveness-of-mathematics-or-ai-vs"},{"x":"8.723085","y":"8.330329","title":"Distinguishing definitions of takeoff","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"AI Takeoff/AI","date":"2020-02-14T00:16","url":"https://www.lesswrong.com/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff"},{"x":"10.975617","y":"7.688092","title":"In theory: does building the subagent have an \"impact\"?","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2020-02-13T14:17","url":"https://www.lesswrong.com/posts/jrrZids4LPiLuLzpu/in-theory-does-building-the-subagent-have-an-impact"},{"x":"10.920916","y":"8.043387","title":"Building and using the subagent","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2020-02-12T19:28","url":"https://www.lesswrong.com/posts/fqJmZBG5xmvvd8WRi/building-and-using-the-subagent"},{"x":"10.290165","y":"7.774419","title":"Demons in Imperfect Search","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Inner Alignment/Optimization","date":"2020-02-11T20:25","url":"https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search"},{"x":"10.064057","y":"8.943153","title":"Intelligence without causality","cluster":"4","author":"['Donald Hobson']","source":"alignment forum","tags":"","date":"2020-02-11T00:34","url":"https://www.lesswrong.com/posts/y6bXLGKWRD9Qyyndg/intelligence-without-causality"},{"x":"11.17002","y":"9.149051","title":"Gricean communication and meta-preferences","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Philosophy","date":"2020-02-10T05:05","url":"https://www.lesswrong.com/posts/8NpwfjFuEPMjTdriJ/gricean-communication-and-meta-preferences"},{"x":"11.760075","y":"8.729952","title":"Attainable Utility Landscape: How The World Is Changed","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/World Modeling/Exercises / Problem-Sets","date":"2020-02-10T00:58","url":"https://www.lesswrong.com/posts/fj8eyc7QzqCaB8Wgm/attainable-utility-landscape-how-the-world-is-changed"},{"x":"9.164281","y":"9.805135","title":"What can the principal-agent literature tell us about AI risk?","cluster":"3","author":"['Alexis Carlier']","source":"alignment forum","tags":"AI Risk/Principal-Agent Problems/World Modeling","date":"2020-02-08T21:28","url":"https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai"},{"x":"12.891981","y":"9.896063","title":"On the falsifiability of hypercomputation","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Falsifiability","date":"2020-02-07T08:16","url":"https://www.lesswrong.com/posts/yjC5LmjSRD2hR9Pfa/on-the-falsifiability-of-hypercomputation"},{"x":"9.589457","y":"7.6845164","title":"Plausibly, almost every powerful algorithm would be manipulative","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Deception/Instrumental Convergence/AI Risk","date":"2020-02-06T11:50","url":"https://www.lesswrong.com/posts/Ez4zZQKWgC6fE3h9G/plausibly-almost-every-powerful-algorithm-would-be"},{"x":"9.329621","y":"7.19833","title":"Synthesizing amplification and debate","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI/Debate (AI safety technique)/Iterated Amplification ","date":"2020-02-05T22:53","url":"https://www.lesswrong.com/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate"},{"x":"9.472383","y":"9.786375","title":"Writeup: Progress on AI Safety via Debate","cluster":"4","author":"['Beth Barnes', 'paulfchristiano']","source":"alignment forum","tags":"Debate (AI safety technique)/Iterated Amplification /AI/Factored Cognition","date":"2020-02-05T21:04","url":"https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"},{"x":"11.746458","y":"7.9843483","title":"Pessimism About Unknown Unknowns Inspires Conservatism","cluster":"1","author":"['michaelcohen']","source":"alignment forum","tags":"Conservatism (AI)/AI","date":"2020-02-03T14:48","url":"https://www.lesswrong.com/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism"},{"x":"11.304514","y":"9.537602","title":"[Question] Instrumental Occam?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Occam's Razor/Rationality","date":"2020-01-31T19:27","url":"https://www.lesswrong.com/posts/qqG2PdZ7pEcM6ev3S/instrumental-occam"},{"x":"9.666855","y":"9.3627825","title":"Artificial Intelligence, Values and Alignment","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2020-01-30T19:48","url":"https://www.lesswrong.com/posts/LDMqaq7JmtPiHkjA5/artificial-intelligence-values-and-alignment"},{"x":"11.025366","y":"9.1459875","title":"Towards deconfusing values","cluster":"1","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Value Learning/Motivations","date":"2020-01-29T19:28","url":"https://www.lesswrong.com/posts/WAqG5BQMzAs34mpc2/towards-deconfusing-values"},{"x":"11.34815","y":"9.757585","title":"Using vector fields to visualise preferences and make them consistent","cluster":"1","author":"['MichaelA', 'JustinShovelain']","source":"alignment forum","tags":"Value Learning/AI","date":"2020-01-28T19:44","url":"https://www.lesswrong.com/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them"},{"x":"11.121417","y":"7.886391","title":"Appendix: how a subagent could get powerful","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Impact Measures","date":"2020-01-28T15:28","url":"https://www.lesswrong.com/posts/sYjCeZTwA84pHkhBJ/appendix-how-a-subagent-could-get-powerful"},{"x":"7.926517","y":"7.8061247","title":"AI Alignment 2018-19 Review","cluster":"3","author":"['Rohin Shah']","source":"alignment forum","tags":"AI/AI Risk/Inner Alignment/Outer Alignment/Value Learning/Impact Measures/AI Takeoff/AI Timelines/Utility Functions/Corrigibility","date":"2020-01-28T02:19","url":"https://www.lesswrong.com/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review"},{"x":"10.854784","y":"9.3751955","title":"The two-layer model of human values, and problems with synthesizing preferences","cluster":"1","author":"['Kaj_Sotala']","source":"alignment forum","tags":"Value Learning/Motivations/Complexity of Value","date":"2020-01-24T15:17","url":"https://www.lesswrong.com/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with"},{"x":"11.928229","y":"10.05514","title":"Formulating Reductive Agency in Causal Models","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-01-23T17:03","url":"https://www.lesswrong.com/posts/qrWFvMnRm4SkKnpRZ/formulating-reductive-agency-in-causal-models"},{"x":"11.723106","y":"10.2406","title":"(A → B) → A in Causal DAGs","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-01-22T18:22","url":"https://www.lesswrong.com/posts/G25RBnBk5BNpv3KyF/a-greater-than-b-greater-than-a-in-causal-dags"},{"x":"12.015801","y":"10.047868","title":"Logical Representation of Causal Models","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/Causality/Logic & Mathematics ","date":"2020-01-21T20:04","url":"https://www.lesswrong.com/posts/2DQHvGaH6C7dmwtdT/logical-representation-of-causal-models"},{"x":"10.001179","y":"7.2335773","title":"Inner alignment requires making assumptions about human values","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"Inner Alignment","date":"2020-01-20T18:38","url":"https://www.lesswrong.com/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human"},{"x":"11.93401","y":"9.692881","title":"ACDT: a hack-y acausal decision theory","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Decision Theory","date":"2020-01-15T17:22","url":"https://www.lesswrong.com/posts/9m2fzjNSJmd3yxxKG/acdt-a-hack-y-acausal-decision-theory"},{"x":"12.232813","y":"9.030448","title":"Predictors exist: CDT going bonkers… forever","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Decision Theory","date":"2020-01-14T16:19","url":"https://www.lesswrong.com/posts/Kr76XzME7TFkN937z/predictors-exist-cdt-going-bonkers-forever"},{"x":"10.249931","y":"9.847178","title":"Update on Ought’s experiments on factored evaluation of arguments","cluster":"1","author":"['Owain_Evans']","source":"alignment forum","tags":"Factored Cognition/Ought","date":"2020-01-12T21:20","url":"https://www.lesswrong.com/posts/pH3eKEAEupx8c2ep9/update-on-ought-s-experiments-on-factored-evaluation-of"},{"x":"9.663231","y":"7.011944","title":"Malign generalization without internal search","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"Inner Alignment/AI","date":"2020-01-12T18:03","url":"https://www.lesswrong.com/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search"},{"x":"7.9345765","y":"7.6360846","title":"Of arguments and wagers","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"Betting","date":"2020-01-10T22:20","url":"https://www.lesswrong.com/posts/aPsdGPCpcyPqkatgc/of-arguments-and-wagers"},{"x":"11.717682","y":"10.333335","title":"Example: Markov Chain","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-01-10T20:19","url":"https://www.lesswrong.com/posts/KEZzAge6mgyo5GDi9/example-markov-chain"},{"x":"8.477251","y":"6.4816155","title":"Outer alignment and imitative amplification","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"Outer Alignment/AI","date":"2020-01-10T00:26","url":"https://www.lesswrong.com/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification"},{"x":"11.110473","y":"9.349428","title":"Preference synthesis illustrated: Star Wars","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Aesthetics/Human Values","date":"2020-01-09T16:47","url":"https://www.lesswrong.com/posts/Nfizy2uRNkZmX3AYB/preference-synthesis-illustrated-star-wars"},{"x":"10.97019","y":"10.166299","title":"(Double-)Inverse Embedded Agency Problem","cluster":"1","author":"['shminux']","source":"alignment forum","tags":"Embedded Agency","date":"2020-01-08T04:30","url":"https://www.lesswrong.com/posts/itGmH2AknmjWyAwj8/double-inverse-embedded-agency-problem"},{"x":"11.717547","y":"10.187131","title":"How to Throw Away Information in Causal DAGs","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-01-08T02:40","url":"https://www.lesswrong.com/posts/zFGGHGfhYsGNnh7Kp/how-to-throw-away-information-in-causal-dags"},{"x":"11.8438015","y":"10.060116","title":"Definitions of Causal Abstraction: Reviewing Beckers & Halpern","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2020-01-07T00:03","url":"https://www.lesswrong.com/posts/gJ76SLJAaKZrFCRTj/definitions-of-causal-abstraction-reviewing-beckers-and"},{"x":"9.749988","y":"6.8639965","title":"Exploring safe exploration","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2020-01-06T21:07","url":"https://www.lesswrong.com/posts/NBffcjqm2P4dNbjrE/exploring-safe-exploration"},{"x":"11.067899","y":"6.4834824","title":"Safe exploration and corrigibility","cluster":"0","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2019-12-28T23:12","url":"https://www.lesswrong.com/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility"},{"x":"10.347664","y":"8.09358","title":"Critiquing \"What failure looks like\"","cluster":"1","author":"['Grue_Slinky']","source":"alignment forum","tags":"AI Risk","date":"2019-12-27T23:59","url":"https://www.lesswrong.com/posts/Q8Z8yoG4tBaowBHwk/critiquing-what-failure-looks-like"},{"x":"9.038511","y":"9.603114","title":"Link: Does the following seem like a reasonable brief summary of the key disagreements regarding AI risk?","cluster":"3","author":"['Davidmanheim']","source":"alignment forum","tags":"","date":"2019-12-26T20:14","url":"https://www.lesswrong.com/posts/m9JeFRgW4DtE3yiHC/link-does-the-following-seem-like-a-reasonable-brief-summary"},{"x":"9.501128","y":"9.745938","title":"New paper: (When) is Truth-telling Favored in AI debate?","cluster":"4","author":"['VojtaKovarik']","source":"alignment forum","tags":"Debate (AI safety technique)/AI","date":"2019-12-26T19:59","url":"https://www.lesswrong.com/posts/RQoSCs9SePDMLJvfz/new-paper-when-is-truth-telling-favored-in-ai-debate"},{"x":"11.471089","y":"8.385301","title":"Some Comments on \"Goodhart Taxonomy\"","cluster":"1","author":"['Grue_Slinky']","source":"alignment forum","tags":"","date":"2019-12-24T23:59","url":"https://www.lesswrong.com/posts/jpQ3mq3XC7oHiKqXA/some-comments-on-goodhart-taxonomy"},{"x":"10.530682","y":"9.474951","title":"Humans Are Embedded Agents Too","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"Embedded Agency","date":"2019-12-23T19:21","url":"https://www.lesswrong.com/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too"},{"x":"9.254056","y":"9.807373","title":"Section 7: Foundations of Rational Agency","cluster":"4","author":"['JesseClifton']","source":"alignment forum","tags":"Center on Long-Term Risk (CLR)/Risks of Astronomical Suffering (S-risks)/AI/Research Agendas/Game Theory/Decision Theory","date":"2019-12-22T02:05","url":"https://www.lesswrong.com/posts/sMhJsRfLXAg87EEqT/section-7-foundations-of-rational-agency"},{"x":"12.3419695","y":"9.037788","title":"The Counterfactual Prisoner’s Dilemma","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Counterfactual Mugging/Prisoner's Dilemma/Counterfactuals","date":"2019-12-21T01:44","url":"https://www.lesswrong.com/posts/sY2rHNcWdg94RiSSR/the-counterfactual-prisoner-s-dilemma"},{"x":"11.632539","y":"8.477963","title":"Clarifying Power-Seeking and Instrumental Convergence","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Instrumental Convergence/AI","date":"2019-12-20T19:59","url":"https://www.lesswrong.com/posts/cwpKagyTvqSyAJB7q/clarifying-power-seeking-and-instrumental-convergence"},{"x":"10.351772","y":"6.3189726","title":"Sections 5 & 6: Contemporary Architectures, Humans in the Loop","cluster":"0","author":"['JesseClifton']","source":"alignment forum","tags":"Center on Long-Term Risk (CLR)/Risks of Astronomical Suffering (S-risks)/AI/Game Theory/Research Agendas","date":"2019-12-20T03:52","url":"https://www.lesswrong.com/posts/4GuKi9wKYnthr8QP9/sections-5-and-6-contemporary-architectures-humans-in-the"},{"x":"11.810682","y":"6.3807855","title":"When Goodharting is optimal: linear vs diminishing returns, unlikely vs likely, and other factors","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Goodhart's Law/AI/Logic & Mathematics ","date":"2019-12-19T13:55","url":"https://www.lesswrong.com/posts/megKzKKsoecdYqwb7/when-goodharting-is-optimal-linear-vs-diminishing-returns"},{"x":"8.071178","y":"9.553087","title":"2019 AI Alignment Literature Review and Charity Comparison","cluster":"3","author":"['Larks']","source":"alignment forum","tags":"Community/AI Governance/Altruism/Moral Uncertainty/Value Learning/Machine Intelligence Research Institute (MIRI)/Center on Long-Term Risk (CLR)/Future of Humanity Institute (FHI)/Center for Human-Compatible AI (CHAI)/Regulation and AI Risk/Literature Reviews/AI/AI Risk/Future of Life Institute (FLI)","date":"2019-12-19T03:00","url":"https://www.lesswrong.com/posts/SmDziGM9hBjW9DKmf/2019-ai-alignment-literature-review-and-charity-comparison"},{"x":"10.941748","y":"10.101668","title":"Utility Maximization = Description Length Minimization","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Information Theory/AI/Rationality/Optimization/Utility Functions","date":"2021-02-18T18:04","url":"https://www.lesswrong.com/posts/voLHQgNncnjjgAPH7/utility-maximization-description-length-minimization"},{"x":"10.311694","y":"6.770642","title":"Formal Solution to the Inner Alignment Problem","cluster":"0","author":"['michaelcohen']","source":"alignment forum","tags":"AI/Academic Papers/Mesa-Optimization/Inner Alignment/AI Risk/World Modeling Techniques/Conservatism (AI)","date":"2021-02-18T14:51","url":"https://www.lesswrong.com/posts/CnruhwFGQBThvgJiX/formal-solution-to-the-inner-alignment-problem"},{"x":"9.332541","y":"6.764882","title":"AXRP Episode 4 - Risks from Learned Optimization with Evan Hubinger","cluster":"2","author":"['DanielFilan']","source":"alignment forum","tags":"Community/AI/Mesa-Optimization/Inner Alignment/Audio/Interviews/AXRP","date":"2021-02-18T00:03","url":"https://www.lesswrong.com/posts/EszCTbovFfpJd5C8N/axrp-episode-4-risks-from-learned-optimization-with-evan"},{"x":"10.259532","y":"7.5803003","title":"Safely controlling the AGI agent reward function","cluster":"4","author":"['Koen.Holtman']","source":"alignment forum","tags":"AI/Counterfactuals/Corrigibility/Wireheading","date":"2021-02-17T14:47","url":"https://www.lesswrong.com/posts/o3smzgcH8MR9RcMgZ/safely-controlling-the-agi-agent-reward-function"},{"x":"11.940216","y":"8.799788","title":"Graphical World Models, Counterfactuals, and Machine Learning Agents","cluster":"1","author":"['Koen.Holtman']","source":"alignment forum","tags":"AI/Counterfactuals/Decision Theory/Myopia","date":"2021-02-17T11:07","url":"https://www.lesswrong.com/posts/q4j7qbEZRaTAA9Kxf/graphical-world-models-counterfactuals-and-machine-learning"},{"x":"10.114701","y":"8.876407","title":"Disentangling Corrigibility: 2015-2021","cluster":"4","author":"['Koen.Holtman']","source":"alignment forum","tags":"AI/Corrigibility/Wireheading","date":"2021-02-16T18:01","url":"https://www.lesswrong.com/posts/MiYkTp6QYKXdJbchu/disentangling-corrigibility-2015-2021"},{"x":"11.274027","y":"10.466108","title":"Cartesian frames as generalised models","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Category Theory/AI","date":"2021-02-16T16:09","url":"https://www.lesswrong.com/posts/wiQeYuQPwSypXXFar/cartesian-frames-as-generalised-models"},{"x":"11.42758","y":"9.934204","title":"Generalised models as a category","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling/Rationality/Category Theory","date":"2021-02-16T16:08","url":"https://www.lesswrong.com/posts/nQxqSsHfexivsd6vB/generalised-models-as-a-category"},{"x":"7.6930537","y":"7.966514","title":"[Question] Suggestions of posts on the AF to review","cluster":"3","author":"['adamShimi']","source":"alignment forum","tags":"Intellectual Progress (Society-Level)/Intellectual Progress via LessWrong/AI Risk/Community/AI","date":"2021-02-16T12:40","url":"https://www.lesswrong.com/posts/6hdxTTPWF2iAbXjAb/suggestions-of-posts-on-the-af-to-review"},{"x":"7.5801687","y":"8.729478","title":"[Question] Mathematical Models of Progress?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Progress Studies/AI Timelines/AI Takeoff/World Modeling/AI/World Optimization","date":"2021-02-16T00:21","url":"https://www.lesswrong.com/posts/ysQEJ8tvm8KYc76D5/mathematical-models-of-progress"},{"x":"10.457597","y":"7.754789","title":"Tournesol, YouTube and AI Risk","cluster":"2","author":"['adamShimi']","source":"alignment forum","tags":"AI/Social Media","date":"2021-02-12T18:56","url":"https://www.lesswrong.com/posts/8q2ySr7yxx7MSR35i/tournesol-youtube-and-ai-risk"},{"x":"8.751961","y":"9.572762","title":"Mapping the Conceptual Territory in AI Existential Safety and Alignment","cluster":"3","author":"['jbkjr']","source":"alignment forum","tags":"AI/Inner Alignment/Outer Alignment/Debate (AI safety technique)/Iterated Amplification /Humans Consulting HCH/Corrigibility/Delegation/Mesa-Optimization","date":"2021-02-12T07:55","url":"https://www.lesswrong.com/posts/HEZgGBZTpT4Bov7nH/mapping-the-conceptual-territory-in-ai-existential-safety"},{"x":"9.714756","y":"6.8801236","title":"Fixing The Good Regulator Theorem","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"World Modeling","date":"2021-02-09T20:30","url":"https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem"},{"x":"10.386753","y":"10.027648","title":"Epistemology of HCH","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Humans Consulting HCH/AI/Practice & Philosophy of Science/Epistemology","date":"2021-02-09T11:46","url":"https://www.lesswrong.com/posts/CDSXoC54CjbXQNLGr/epistemology-of-hch"},{"x":"11.999567","y":"9.893137","title":"Learning Normativity: Language","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"Rationality/World Optimization/AI/Meta-Philosophy","date":"2021-02-05T22:26","url":"https://www.lesswrong.com/posts/5eY6A4Zfu6rfeMfJS/learning-normativity-language"},{"x":"9.239534","y":"8.74049","title":"Creating AGI Safety Interlocks","cluster":"4","author":"['Koen.Holtman']","source":"alignment forum","tags":"AI/World Optimization/Counterfactuals/Corrigibility/Intelligence Explosion","date":"2021-02-05T12:01","url":"https://www.lesswrong.com/posts/BZKLf629NDNfEkZzJ/creating-agi-safety-interlocks"},{"x":"9.2089","y":"8.672648","title":"Counterfactual Planning in AGI Systems","cluster":"4","author":"['Koen.Holtman']","source":"alignment forum","tags":"AI/Counterfactuals/Corrigibility/Decision Theory/Embedded Agency/Intelligence Explosion","date":"2021-02-03T13:54","url":"https://www.lesswrong.com/posts/7EnZgaepSBwaZXA5y/counterfactual-planning-in-agi-systems"},{"x":"9.088303","y":"7.5016494","title":"Distinguishing claims about training vs deployment","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/Orthogonality Thesis/Distinctions","date":"2021-02-03T11:30","url":"https://www.lesswrong.com/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment"},{"x":"11.794249","y":"9.237046","title":"A Critique of Non-Obstruction","cluster":"1","author":"['Joe_Collman']","source":"alignment forum","tags":"AI/Impact Measures/Corrigibility","date":"2021-02-03T08:45","url":"https://www.lesswrong.com/posts/ZqfT5xTuNf6okrepY/a-critique-of-non-obstruction"},{"x":"12.273829","y":"9.925671","title":"Limiting Causality by Complexity Class","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"Causality/Rationality","date":"2021-01-30T12:18","url":"https://www.lesswrong.com/posts/QFuypcQGZK59TaKos/limiting-causality-by-complexity-class"},{"x":"8.520155","y":"9.799602","title":"AMA on EA Forum: Ajeya Cotra, researcher at Open Phil","cluster":"3","author":"['Ajeya Cotra']","source":"alignment forum","tags":"Community/AI/World Optimization/AMA","date":"2021-01-29T23:05","url":"https://www.lesswrong.com/posts/F2C6KKRXGeZ424mi7/ama-on-ea-forum-ajeya-cotra-researcher-at-open-phil"},{"x":"12.078519","y":"9.233573","title":"Extracting Money from Causal Decision Theorists","cluster":"1","author":"['Caspar42']","source":"alignment forum","tags":"Rationality","date":"2021-01-28T17:58","url":"https://www.lesswrong.com/posts/xPeWJaAzp2LeDdP4Z/extracting-money-from-causal-decision-theorists"},{"x":"9.718419","y":"9.982464","title":"Optimal play in human-judged Debate usually won’t answer your question","cluster":"1","author":"['Joe_Collman']","source":"alignment forum","tags":"AI/Debate (AI safety technique)/Rationality","date":"2021-01-27T07:34","url":"https://www.lesswrong.com/posts/35748mXjzwxDrX7yQ/optimal-play-in-human-judged-debate-usually-won-t-answer"},{"x":"12.248867","y":"8.4429865","title":"What is a VNM stable set, really?","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Rationality/Game Theory","date":"2021-01-25T05:43","url":"https://www.lesswrong.com/posts/CLuCgA2Ab7sBfvEuW/what-is-a-vnm-stable-set-really"},{"x":"8.639763","y":"8.3487625","title":"[Question] Poll: Which variables are most strategically relevant?","cluster":"3","author":"['Daniel Kokotajlo', 'Noa Nabeshima']","source":"alignment forum","tags":"AI/World Optimization/AI Takeoff/AI Timelines","date":"2021-01-22T17:17","url":"https://www.lesswrong.com/posts/yhb5BNksWcESezp7p/poll-which-variables-are-most-strategically-relevant"},{"x":"11.588589","y":"8.730061","title":"Counterfactual control incentives","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-01-21T16:54","url":"https://www.lesswrong.com/posts/67a8C6KsKn2NyW2Ry/counterfactual-control-incentives"},{"x":"10.330452","y":"9.138963","title":"Infra-Bayesianism Unwrapped","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Distillation & Pedagogy/AI/Logical Uncertainty/Decision Theory/Infra-Bayesianism","date":"2021-01-20T13:35","url":"https://www.lesswrong.com/posts/Zi7nmuSmBFbQWgFBa/infra-bayesianism-unwrapped"},{"x":"10.375677","y":"9.163536","title":"Against the Backward Approach to Goal-Directedness","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/AI","date":"2021-01-19T18:46","url":"https://www.lesswrong.com/posts/adKSWktLbxfihDANM/against-the-backward-approach-to-goal-directedness"},{"x":"8.792497","y":"9.714745","title":"Some thoughts on risks from narrow, non-agentic AI","cluster":"3","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/Narrow AI","date":"2021-01-19T00:04","url":"https://www.lesswrong.com/posts/AWbtbmC6rAg6dh75b/some-thoughts-on-risks-from-narrow-non-agentic-ai"},{"x":"10.784383","y":"7.906435","title":"Short summary of mAIry’s room","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Rationality/AI","date":"2021-01-18T18:11","url":"https://www.lesswrong.com/posts/rmBS5nTJh6pxERWEu/short-summary-of-mairy-s-room"},{"x":"9.147142","y":"7.7459826","title":"Birds, Brains, Planes, and AI: Against Appeals to the Complexity/​Mysteriousness/​Efficiency of the Brain","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/Center on Long-Term Risk (CLR)/AI Timelines/Technological Forecasting/History","date":"2021-01-18T12:08","url":"https://www.lesswrong.com/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity"},{"x":"10.247592","y":"8.905513","title":"Literature Review on Goal-Directedness","cluster":"4","author":"['adamShimi', 'Michele Campolo', 'Joe_Collman']","source":"alignment forum","tags":"Goal-Directedness/Literature Reviews/AI","date":"2021-01-18T11:15","url":"https://www.lesswrong.com/posts/cfXwr6NC9AqZ9kr8g/literature-review-on-goal-directedness"},{"x":"12.135292","y":"9.964222","title":"Excerpt from Arbital Solomonoff induction dialogue","cluster":"1","author":"['Richard_Ngo']","source":"alignment forum","tags":"Rationality/AI/Solomonoff Induction","date":"2021-01-17T03:49","url":"https://www.lesswrong.com/posts/wsBpJn7HWEPCJxYai/excerpt-from-arbital-solomonoff-induction-dialogue"},{"x":"9.206493","y":"8.78707","title":"Why I’m excited about Debate","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/Debate (AI safety technique)","date":"2021-01-15T23:37","url":"https://www.lesswrong.com/posts/LDsSqXf9Dpu3J3gHD/why-i-m-excited-about-debate"},{"x":"9.530719","y":"9.087406","title":"Thoughts on Iason Gabriel’s Artificial Intelligence, Values, and Alignment","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2021-01-14T12:58","url":"https://www.lesswrong.com/posts/Z2rkdEAJ9MvYPBeYW/thoughts-on-iason-gabriel-s-artificial-intelligence-values"},{"x":"8.554138","y":"8.534863","title":"Some recent survey papers on (mostly near-term) AI safety, security, and assurance","cluster":"3","author":"['Aryeh Englander']","source":"alignment forum","tags":"AI/Literature Reviews","date":"2021-01-13T21:50","url":"https://www.lesswrong.com/posts/GTcWrenvDMsThTQ26/some-recent-survey-papers-on-mostly-near-term-ai-safety"},{"x":"10.935237","y":"6.7604294","title":"Review of ‘Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More’","cluster":"0","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence/LessWrong Review","date":"2021-01-12T03:57","url":"https://www.lesswrong.com/posts/ZPEGLoWMN242Dob6g/review-of-debate-on-instrumental-convergence-between-lecun"},{"x":"8.586382","y":"8.49367","title":"Transparency and AGI safety","cluster":"4","author":"['jylin04']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI","date":"2021-01-11T18:51","url":"https://www.lesswrong.com/posts/QirLfXhDPYWCP8PK5/transparency-and-agi-safety"},{"x":"8.93858","y":"6.8685718","title":"Prediction can be Outer Aligned at Optimum","cluster":"2","author":"['Lanrian']","source":"alignment forum","tags":"AI/Outer Alignment/Solomonoff Induction","date":"2021-01-10T18:48","url":"https://www.lesswrong.com/posts/3D2MGF2fZhWSNb7aw/prediction-can-be-outer-aligned-at-optimum"},{"x":"7.7409086","y":"9.346918","title":"Review of Soft Takeoff Can Still Lead to DSA","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI Takeoff/LessWrong Review/AI","date":"2021-01-10T18:10","url":"https://www.lesswrong.com/posts/P448hmmAeGepQDREs/review-of-soft-takeoff-can-still-lead-to-dsa"},{"x":"8.780289","y":"7.1070776","title":"Imitative Generalisation (AKA ‘Learning the Prior’)","cluster":"2","author":"['Beth Barnes']","source":"alignment forum","tags":"AI/Debate (AI safety technique)/OpenAI/Outer Alignment/Iterated Amplification ","date":"2021-01-10T00:30","url":"https://www.lesswrong.com/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1"},{"x":"7.8590546","y":"8.364465","title":"The Case for a Journal of AI Alignment","cluster":"3","author":"['adamShimi']","source":"alignment forum","tags":"AI/World Optimization","date":"2021-01-09T18:13","url":"https://www.lesswrong.com/posts/hNNM6gP5yZcHffmpD/the-case-for-a-journal-of-ai-alignment"},{"x":"9.06202","y":"8.691282","title":"Eight claims about multi-agent AGI safety","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2021-01-07T13:34","url":"https://www.lesswrong.com/posts/dSAJdi99XmqftqXXq/eight-claims-about-multi-agent-agi-safety"},{"x":"11.194756","y":"8.272402","title":"Review of ‘But exactly how complex and fragile?’","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Complexity of Value/LessWrong Review","date":"2021-01-06T18:39","url":"https://www.lesswrong.com/posts/r6p5cqT6aWYGCYHJx/review-of-but-exactly-how-complex-and-fragile"},{"x":"11.40434","y":"9.921952","title":"The Pointers Problem: Clarifications/​Variations","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Rationality","date":"2021-01-05T17:29","url":"https://www.lesswrong.com/posts/7Zn4BwgsiPFhdB6h8/the-pointers-problem-clarifications-variations"},{"x":"10.458627","y":"7.476937","title":"Multi-dimensional rewards for AGI interpretability and control","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)/Reinforcement Learning","date":"2021-01-04T03:08","url":"https://www.lesswrong.com/posts/Lj9QXcqkcuR4iHJ7Q/multi-dimensional-rewards-for-agi-interpretability-and"},{"x":"7.995542","y":"8.320841","title":"Reflections on Larks’ 2020 AI alignment literature review","cluster":"3","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2021-01-01T22:53","url":"https://www.lesswrong.com/posts/uEo4Xhp7ziTKhR6jq/reflections-on-larks-2020-ai-alignment-literature-review"},{"x":"9.462942","y":"9.366757","title":"AI Alignment, Philosophical Pluralism, and the Relevance of Non-Western Philosophy","cluster":"4","author":"['xuan']","source":"alignment forum","tags":"AI/Philosophy/Meta-Philosophy/Value Learning/Metaethics/Suffering/World Modeling","date":"2021-01-01T00:08","url":"https://www.lesswrong.com/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of"},{"x":"10.32251","y":"9.824056","title":"Debate Minus Factored Cognition","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Debate (AI safety technique)","date":"2020-12-29T22:59","url":"https://www.lesswrong.com/posts/a2NZr87sGYpXhzsth/debate-minus-factored-cognition"},{"x":"10.962695","y":"6.954743","title":"AXRP Episode 3 - Negotiable Reinforcement Learning with Andrew Critch","cluster":"0","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Reinforcement Learning/Disagreement/Interviews/Utilitarianism/Moral Uncertainty/Audio/AXRP","date":"2020-12-29T20:45","url":"https://www.lesswrong.com/posts/u7o7HtChnZ5x8SqvA/axrp-episode-3-negotiable-reinforcement-learning-with-andrew"},{"x":"10.309818","y":"6.678108","title":"AXRP Episode 2 - Learning Human Biases with Rohin Shah","cluster":"0","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Inverse Reinforcement Learning/Heuristics & Biases/Interviews/Audio/AXRP","date":"2020-12-29T20:43","url":"https://www.lesswrong.com/posts/BJAcnMBHGua3tFKu5/axrp-episode-2-learning-human-biases-with-rohin-shah"},{"x":"10.385248","y":"6.2243695","title":"AXRP Episode 1 - Adversarial Policies with Adam Gleave","cluster":"0","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Reinforcement Learning/Adversarial Examples/Interviews/Audio/AXRP","date":"2020-12-29T20:41","url":"https://www.lesswrong.com/posts/8MZ72PYa3kRe4yRDD/axrp-episode-1-adversarial-policies-with-adam-gleave"},{"x":"7.641917","y":"8.823194","title":"Against GDP as a metric for timelines and takeoff speeds","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI Timelines/AI Takeoff/Center on Long-Term Risk (CLR)/AI","date":"2020-12-29T17:42","url":"https://www.lesswrong.com/posts/aFaKhG86tTrKvtAnT/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds"},{"x":"8.189128","y":"5.84424","title":"Why Neural Networks Generalise, and Why They Are (Kind of) Bayesian","cluster":"2","author":"['Joar Skalse']","source":"alignment forum","tags":"AI/Lottery Ticket Hypothesis","date":"2020-12-29T13:33","url":"https://www.lesswrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of"},{"x":"9.006236","y":"8.693283","title":"Defusing AGI Danger","cluster":"4","author":"['Mark Xu']","source":"alignment forum","tags":"AI","date":"2020-12-24T22:58","url":"https://www.lesswrong.com/posts/BSrfDWpHgFpzGRwJS/defusing-agi-danger"},{"x":"11.836701","y":"7.922509","title":"Operationalizing compatibility with strategy-stealing","cluster":"1","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2020-12-24T22:36","url":"https://www.lesswrong.com/posts/WwJdaymwKq6qyJqBX/operationalizing-compatibility-with-strategy-stealing"},{"x":"11.572966","y":"7.9956536","title":"2019 Review Rewrite: Seeking Power is Often Robustly Instrumental in MDPs","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence/Myopia/LessWrong Review","date":"2020-12-23T17:16","url":"https://www.lesswrong.com/posts/mxXcPzpgGx4f8eK7v/2019-review-rewrite-seeking-power-is-often-robustly"},{"x":"9.676449","y":"10.018052","title":"Debate update: Obfuscated arguments problem","cluster":"1","author":"['Beth Barnes']","source":"alignment forum","tags":"AI/Debate (AI safety technique)/Outer Alignment/Iterated Amplification /OpenAI","date":"2020-12-23T03:24","url":"https://www.lesswrong.com/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"},{"x":"8.00918","y":"9.285024","title":"TAI Safety Bibliographic Database","cluster":"3","author":"['JessRiedel']","source":"alignment forum","tags":"AI","date":"2020-12-22T17:42","url":"https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database"},{"x":"8.243918","y":"9.601522","title":"2020 AI Alignment Literature Review and Charity Comparison","cluster":"3","author":"['Larks']","source":"alignment forum","tags":"AI/Literature Reviews","date":"2020-12-21T15:27","url":"https://www.lesswrong.com/posts/pTYDdcag9pTzFQ7vw/2020-ai-alignment-literature-review-and-charity-comparison"},{"x":"10.753354","y":"8.966836","title":"Hierarchical planning: context agents","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"AI","date":"2020-12-19T11:24","url":"https://www.lesswrong.com/posts/6ayQbR5opoTN4AgFb/hierarchical-planning-context-agents"},{"x":"7.698172","y":"6.578947","title":"Extrapolating GPT-N performance","cluster":"2","author":"['Lanrian']","source":"alignment forum","tags":"AI Timelines/AI/GPT/Language Models","date":"2020-12-18T21:41","url":"https://www.lesswrong.com/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance"},{"x":"11.916448","y":"10.335365","title":"Less Basic Inframeasure Theory","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Rationality/AI/Infra-Bayesianism","date":"2020-12-16T03:52","url":"https://www.lesswrong.com/posts/idP5E5XhJGh9T5Yq9/less-basic-inframeasure-theory"},{"x":"8.609259","y":"8.074463","title":"Homogeneity vs. heterogeneity in AI takeoff scenarios","cluster":"3","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2020-12-16T01:37","url":"https://www.lesswrong.com/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios"},{"x":"8.849684","y":"9.14264","title":"Risk Map of AI Systems","cluster":"3","author":"['VojtaKovarik', 'Jan_Kulveit']","source":"alignment forum","tags":"AI/Carving / Clustering Reality/AI Risk/World Modeling","date":"2020-12-15T09:16","url":"https://www.lesswrong.com/posts/QskBy5uDd2oeEGkBB/risk-map-of-ai-systems"},{"x":"7.777016","y":"9.158285","title":"[Question] What are the best precedents for industries failing to invest in valuable AI research?","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/Efficient Market Hypothesis/History","date":"2020-12-14T23:57","url":"https://www.lesswrong.com/posts/27h99G7P6fkucKdkk/what-are-the-best-precedents-for-industries-failing-to"},{"x":"10.288513","y":"9.760716","title":"Clarifying Factored Cognition","cluster":"4","author":"['Rafael Harth']","source":"alignment forum","tags":"Factored Cognition/Humans Consulting HCH/Debate (AI safety technique)/AI","date":"2020-12-13T20:02","url":"https://www.lesswrong.com/posts/eCWkJrFff7oMLwjEp/clarifying-factored-cognition"},{"x":"7.643006","y":"8.91343","title":"[Question] What technologies could cause world GDP doubling times to be <8 years?","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"World Optimization/World Modeling","date":"2020-12-10T15:34","url":"https://www.lesswrong.com/posts/2rQ9vv9HY6i2Z2vQ4/what-technologies-could-cause-world-gdp-doubling-times-to-be"},{"x":"9.488636","y":"8.024191","title":"Conservatism in neocortex-like AGIs","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/Conservatism (AI)","date":"2020-12-08T16:37","url":"https://www.lesswrong.com/posts/c92YC89tznC7579Ej/conservatism-in-neocortex-like-agis"},{"x":"10.718714","y":"9.799161","title":"Traversing a Cognition Space","cluster":"1","author":"['Rafael Harth']","source":"alignment forum","tags":"Factored Cognition/Rationality/Debate (AI safety technique)","date":"2020-12-07T18:32","url":"https://www.lesswrong.com/posts/FNyqL7mxSkgLpck4w/traversing-a-cognition-space"},{"x":"11.285407","y":"9.271831","title":"Values Form a Shifting Landscape (and why you might care)","cluster":"1","author":"['VojtaKovarik']","source":"alignment forum","tags":"AI/Rationality","date":"2020-12-05T23:56","url":"https://www.lesswrong.com/posts/4Qd2pDWeFPgYZfkSg/values-form-a-shifting-landscape-and-why-you-might-care"},{"x":"9.138103","y":"8.963133","title":"AI Problems Shared by Non-AI Systems","cluster":"4","author":"['VojtaKovarik']","source":"alignment forum","tags":"AI/World Optimization","date":"2020-12-05T22:15","url":"https://www.lesswrong.com/posts/iGs2jHc6Mcm3jtefk/ai-problems-shared-by-non-ai-systems"},{"x":"9.312431","y":"7.2018566","title":"Recursive Quantilizers II","cluster":"2","author":"['abramdemski']","source":"alignment forum","tags":"AI/Value Learning/Meta-Philosophy","date":"2020-12-02T15:26","url":"https://www.lesswrong.com/posts/YNuJjRuxsWWzfvder/recursive-quantilizers-ii"},{"x":"10.722373","y":"7.4433823","title":"[Question] In a multipolar scenario, how do people expect systems to be trained to interact with systems developed by other labs?  ","cluster":"4","author":"['JesseClifton']","source":"alignment forum","tags":"AI","date":"2020-12-01T20:04","url":"https://www.lesswrong.com/posts/Pkthep47ukcrK3MNm/in-a-multipolar-scenario-how-do-people-expect-systems-to-be"},{"x":"10.3073","y":"9.864165","title":"Idealized Factored Cognition","cluster":"1","author":"['Rafael Harth']","source":"alignment forum","tags":"Factored Cognition/AI/Humans Consulting HCH/Debate (AI safety technique)/Rationality","date":"2020-11-30T18:49","url":"https://www.lesswrong.com/posts/S5oWwZMJBvfChSquW/idealized-factored-cognition"},{"x":"8.938418","y":"8.977688","title":"Commentary on AGI Safety from First Principles","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-11-23T21:37","url":"https://www.lesswrong.com/posts/oiuZjPfknKsSc5waC/commentary-on-agi-safety-from-first-principles"},{"x":"10.956248","y":"10.296812","title":"Syntax, semantics, and symbol grounding, simplified","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/World Modeling/Symbol Grounding","date":"2020-11-23T16:12","url":"https://www.lesswrong.com/posts/joPoxBpZjLNx8MKaF/syntax-semantics-and-symbol-grounding-simplified"},{"x":"9.39534","y":"8.475389","title":"Continuing the takeoffs debate","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI Takeoff/AI","date":"2020-11-23T15:58","url":"https://www.lesswrong.com/posts/Tpn2Fx9daLvj28kes/continuing-the-takeoffs-debate"},{"x":"10.845511","y":"8.52654","title":"Non-Obstruction: A Simple Concept Motivating Corrigibility","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Corrigibility","date":"2020-11-21T19:35","url":"https://www.lesswrong.com/posts/Xts5wm3akbemk4pDa/non-obstruction-a-simple-concept-motivating-corrigibility"},{"x":"8.709943","y":"9.822537","title":"Persuasion Tools: AI takeover without AGI or agency?","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"World Optimization/AI","date":"2020-11-20T16:54","url":"https://www.lesswrong.com/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency"},{"x":"11.39118","y":"9.768846","title":"Hiding Complexity","cluster":"1","author":"['Rafael Harth']","source":"alignment forum","tags":"Rationality/Factored Cognition","date":"2020-11-20T16:35","url":"https://www.lesswrong.com/posts/6zbRy3aADCsRmFcgv/hiding-complexity"},{"x":"9.010637","y":"9.461801","title":"Some AI research areas and their relevance to existential safety","cluster":"3","author":"['Andrew_Critch']","source":"alignment forum","tags":"AI/Existential Risk/Academic Papers/World Optimization/Agent Foundations","date":"2020-11-19T03:18","url":"https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1"},{"x":"10.189463","y":"7.8797426","title":"Inner Alignment in Salt-Starved Rats","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/Inner Alignment/Transparency / Interpretability (ML & AI)","date":"2020-11-19T02:40","url":"https://www.lesswrong.com/posts/wcNEXDHowiWkRxDNv/inner-alignment-in-salt-starved-rats"},{"x":"11.160228","y":"9.8121805","title":"The Pointers Problem: Human Values Are A Function Of Humans’ Latent Variables","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"The Pointers Problem/Rationality/AI/Value Learning","date":"2020-11-18T17:47","url":"https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans"},{"x":"10.822313","y":"8.302912","title":"Normativity","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Value Learning/Moral Uncertainty/AI/Human Values/Meta-Philosophy/World Optimization/Rationality","date":"2020-11-18T16:52","url":"https://www.lesswrong.com/posts/tCex9F9YptGMpk2sT/normativity"},{"x":"12.13758","y":"9.3587885","title":"SUDT: A toy decision theory for updateless anthropics","cluster":"1","author":"['Benya']","source":"alignment forum","tags":"Anthropics","date":"2014-02-23T23:50","url":"https://www.lesswrong.com/posts/NaZPjaLPCGZWdTyrL/sudt-a-toy-decision-theory-for-updateless-anthropics"},{"x":"11.311957","y":"10.402385","title":"Abstraction, Causality, and Embedded Maps: Here Be Monsters","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2019-12-18T20:25","url":"https://www.lesswrong.com/posts/ipCAL4tx7jcJsFasY/abstraction-causality-and-embedded-maps-here-be-monsters"},{"x":"8.5787735","y":"6.1135654","title":"Inductive biases stick around","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI/Machine Learning","date":"2019-12-18T19:52","url":"https://www.lesswrong.com/posts/nGqzNC6uNueum2w8T/inductive-biases-stick-around"},{"x":"11.287527","y":"10.250041","title":"Is Causality in the Map or the Territory?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Causality","date":"2019-12-17T23:19","url":"https://www.lesswrong.com/posts/ZBYE2F5DBiZtj6m95/is-causality-in-the-map-or-the-territory"},{"x":"12.20216","y":"9.03699","title":"[Question] Counterfactual Mugging: Why should you pay?","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Decision Theory/Counterfactual Mugging/Counterfactuals","date":"2019-12-17T22:16","url":"https://www.lesswrong.com/posts/h9qQQA3g8dwq6RRTo/counterfactual-mugging-why-should-you-pay"},{"x":"9.1639805","y":"8.178961","title":"A dilemma for prosaic AI alignment","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI","date":"2019-12-17T22:11","url":"https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment"},{"x":"9.153815","y":"9.832172","title":"Sections 3 & 4: Credibility, Peaceful Bargaining Mechanisms ","cluster":"1","author":"['JesseClifton']","source":"alignment forum","tags":"Center on Long-Term Risk (CLR)/Risks of Astronomical Suffering (S-risks)/AI/Research Agendas/Game Theory/Coordination / Cooperation","date":"2019-12-17T21:46","url":"https://www.lesswrong.com/posts/8xKhCbNrdP4gaA8c3/sections-3-and-4-credibility-peaceful-bargaining-mechanisms"},{"x":"8.416675","y":"9.718523","title":"Sections 1 & 2: Introduction, Strategy and Governance","cluster":"3","author":"['JesseClifton']","source":"alignment forum","tags":"Center on Long-Term Risk (CLR)/Risks of Astronomical Suffering (S-risks)/Research Agendas/Game Theory/AI/Coordination / Cooperation","date":"2019-12-17T21:27","url":"https://www.lesswrong.com/posts/KMocAf9jnAKc2jXri/sections-1-and-2-introduction-strategy-and-governance"},{"x":"12.383128","y":"10.001015","title":"Counterfactual Induction (Lemma 4)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Counterfactuals","date":"2019-12-17T05:05","url":"https://www.lesswrong.com/posts/Cu4v9MHGuhLnDQTuF/counterfactual-induction-lemma-4"},{"x":"12.683686","y":"9.789109","title":"Counterfactual Induction (Algorithm Sketch, Fixpoint proof)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Counterfactuals/Formal Proof","date":"2019-12-17T05:04","url":"https://www.lesswrong.com/posts/xBoBmPtgvwdfqm2r5/counterfactual-induction-algorithm-sketch-fixpoint-proof"},{"x":"12.658509","y":"9.934018","title":"Counterfactual Induction","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Counterfactuals","date":"2019-12-17T05:03","url":"https://www.lesswrong.com/posts/EAqHkKtbefvyRs4nw/counterfactual-induction"},{"x":"9.806229","y":"6.863129","title":"Is the term mesa optimizer too narrow?","cluster":"0","author":"['Matthew Barnett']","source":"alignment forum","tags":"Optimization","date":"2019-12-14T23:20","url":"https://www.lesswrong.com/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow"},{"x":"8.663891","y":"9.311923","title":"Acknowledgements & References","cluster":"3","author":"['JesseClifton']","source":"alignment forum","tags":"Research Agendas","date":"2019-12-14T07:04","url":"https://www.lesswrong.com/posts/XKWGgyCyGhkm73fhm/acknowledgements-and-references"},{"x":"8.53359","y":"9.678171","title":"Preface to CLR’s Research Agenda on Cooperation, Conflict, and TAI ","cluster":"3","author":"['JesseClifton']","source":"alignment forum","tags":"Risks of Astronomical Suffering (S-risks)/Center on Long-Term Risk (CLR)/Research Agendas/Suffering/AI","date":"2019-12-13T21:02","url":"https://www.lesswrong.com/posts/DbuCdEbkh4wL5cjJ5/preface-to-clr-s-research-agenda-on-cooperation-conflict-and"},{"x":"11.281925","y":"10.255403","title":"Examples of Causal Abstraction","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2019-12-12T22:54","url":"https://www.lesswrong.com/posts/Expvyb6nndbjqigRL/examples-of-causal-abstraction"},{"x":"11.797309","y":"10.189437","title":"Causal Abstraction Toy Model: Medical Sensor","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2019-12-11T21:12","url":"https://www.lesswrong.com/posts/S8WZ2rav9BqFAZoRM/causal-abstraction-toy-model-medical-sensor"},{"x":"10.639555","y":"8.468166","title":"Predictive coding = RL + SL + Bayes + MPC","cluster":"1","author":"['Steven Byrnes']","source":"alignment forum","tags":"Predictive Processing/World Modeling/Neocortex/Neuroscience","date":"2019-12-10T11:45","url":"https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc"},{"x":"11.980725","y":"9.212023","title":"Counterfactuals: Smoking Lesion vs. Newcomb’s","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Counterfactuals/Newcomb's Problem","date":"2019-12-08T21:02","url":"https://www.lesswrong.com/posts/8Hr95c37nadXCxzh7/counterfactuals-smoking-lesion-vs-newcomb-s"},{"x":"10.899578","y":"10.434373","title":"What is Abstraction?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction","date":"2019-12-06T20:30","url":"https://www.lesswrong.com/posts/wuJpYLcMEBz4kcgAn/what-is-abstraction-1"},{"x":"10.775679","y":"9.391888","title":"Comment on Coherence arguments do not imply goal directed behavior","cluster":"1","author":"['Ronny']","source":"alignment forum","tags":"Decision Theory/Coherence Arguments","date":"2019-12-06T09:30","url":"https://www.lesswrong.com/posts/EnN7cm3KaRrEAuWfa/comment-on-coherence-arguments-do-not-imply-goal-directed"},{"x":"8.28912","y":"5.8686457","title":"Understanding \"Deep Double Descent\"","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI/Machine Learning/Lottery Ticket Hypothesis","date":"2019-12-06T00:00","url":"https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent"},{"x":"10.491859","y":"9.218212","title":"Values, Valence, and Alignment","cluster":"1","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Value Learning","date":"2019-12-05T21:06","url":"https://www.lesswrong.com/posts/ALvnz3DrjHwmLG29F/values-valence-and-alignment"},{"x":"12.828688","y":"9.178091","title":"Oracles: reject all deals—break superrationality, with superrationality","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI/Acausal Trade/AI Boxing (Containment)","date":"2019-12-05T13:51","url":"https://www.lesswrong.com/posts/6XCTppoPAMdKCPFb4/oracles-reject-all-deals-break-superrationality-with-1"},{"x":"10.119173","y":"9.0415325","title":"Seeking Power is Often Convergently Instrumental in MDPs","cluster":"4","author":"['TurnTrout', 'Logan Riggs']","source":"alignment forum","tags":"Instrumental Convergence/AI/Myopia","date":"2019-12-05T02:33","url":"https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-convergently-instrumental-in-mdps"},{"x":"10.744099","y":"6.0662546","title":"[Question] What are some non-purely-sampling ways to do deep RL?","cluster":"0","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2019-12-05T00:09","url":"https://www.lesswrong.com/posts/Ca3sCRGfWvXvYC5YC/what-are-some-non-purely-sampling-ways-to-do-deep-rl"},{"x":"8.100562","y":"5.764721","title":"Recent Progress in the Theory of Neural Networks","cluster":"2","author":"['interstice']","source":"alignment forum","tags":"AI/Logic & Mathematics ","date":"2019-12-04T23:11","url":"https://www.lesswrong.com/posts/KrQvZM8uFjSTJ7hq3/recent-progress-in-the-theory-of-neural-networks-1"},{"x":"12.331452","y":"8.932754","title":"\"Fully\" acausal trade","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Acausal Trade","date":"2019-12-04T16:39","url":"https://www.lesswrong.com/posts/MHHzLfAQBZzieGBjq/fully-acausal-trade"},{"x":"8.8359585","y":"8.649066","title":"A list of good heuristics that the case for AI x-risk fails","cluster":"4","author":"['capybaralet']","source":"alignment forum","tags":"Existential Risk","date":"2019-12-02T19:26","url":"https://www.lesswrong.com/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails"},{"x":"8.402417","y":"7.966261","title":"What I talk about when I talk about AI x-risk: 3 core claims I want machine learning researchers to address.","cluster":"2","author":"['capybaralet']","source":"alignment forum","tags":"Existential Risk","date":"2019-12-02T18:20","url":"https://www.lesswrong.com/posts/bJdaB2Mz4mBvwFBeb/what-i-talk-about-when-i-talk-about-ai-x-risk-3-core-claims-1"},{"x":"12.14329","y":"9.124359","title":"Counterfactuals as a matter of Social Convention","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Counterfactuals","date":"2019-11-30T10:35","url":"https://www.lesswrong.com/posts/9rtWTHsPAf2mLKizi/counterfactuals-as-a-matter-of-social-convention"},{"x":"8.120381","y":"7.8386817","title":"Useful Does Not Mean Secure","cluster":"4","author":"['Ben Pace']","source":"alignment forum","tags":"AI","date":"2019-11-30T02:05","url":"https://www.lesswrong.com/posts/mdau2DBSMi5bWXPGA/useful-does-not-mean-secure"},{"x":"12.011646","y":"9.30118","title":"Transparent Newcomb’s Problem and the limitations of the Erasure framing","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"","date":"2019-11-28T11:32","url":"https://www.lesswrong.com/posts/j5CJZ566Pj3AwfrBT/transparent-newcomb-s-problem-and-the-limitations-of-the"},{"x":"10.352792","y":"9.065254","title":"A test for symbol grounding methods: true zero-sum games","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Symbol Grounding","date":"2019-11-26T14:15","url":"https://www.lesswrong.com/posts/JpEPKbXiTvmyqYdTr/a-test-for-symbol-grounding-methods-true-zero-sum-games-1"},{"x":"8.643424","y":"8.0893545","title":"Thoughts on implementing corrigible robust alignment","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Value Learning/Corrigibility","date":"2019-11-26T14:06","url":"https://www.lesswrong.com/posts/8W5gNgEKnyAscg8BF/thoughts-on-implementing-corrigible-robust-alignment"},{"x":"12.904055","y":"9.088209","title":"Breaking Oracles: superrationality and acausal trade","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Acausal Trade/Oracle AI/Superrationality/AI Boxing (Containment)/AI Risk/Coordination / Cooperation","date":"2019-11-25T10:40","url":"https://www.lesswrong.com/posts/42z4k8Co5BuHMBvER/breaking-oracles-superrationality-and-acausal-trade"},{"x":"10.990121","y":"9.298997","title":"Ultra-simplified research agenda","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Research Agendas","date":"2019-11-22T14:29","url":"https://www.lesswrong.com/posts/m2bwD87ctjJDXC3SZ/ultra-simplified-research-agenda"},{"x":"12.566644","y":"8.91261","title":"Analysing: Dangerous messages from future UFAI via Oracles","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI/AI Boxing (Containment)/AI Risk/AI","date":"2019-11-22T14:17","url":"https://www.lesswrong.com/posts/6WbLRLdmTL4JxxvCq/analysing-dangerous-messages-from-future-ufai-via-oracles"},{"x":"9.963486","y":"8.576742","title":"Defining AI wireheading","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Wireheading","date":"2019-11-21T13:04","url":"https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading"},{"x":"12.178443","y":"10.1423","title":"A Brief Intro to Domain Theory","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Domain Theory","date":"2019-11-21T03:24","url":"https://www.lesswrong.com/posts/4C4jha5SdReWgg7dF/a-brief-intro-to-domain-theory"},{"x":"9.237574","y":"6.533042","title":"The Goodhart Game","cluster":"4","author":"['John_Maxwell']","source":"alignment forum","tags":"Goodhart's Law/Adversarial Examples/AI/Research Agendas","date":"2019-11-18T23:22","url":"https://www.lesswrong.com/posts/WnPEe99YuyRxktMD3/the-goodhart-game"},{"x":"10.616286","y":"8.8657255","title":"Self-Fulfilling Prophecies Aren’t Always About Self-Awareness","cluster":"1","author":"['John_Maxwell']","source":"alignment forum","tags":"Myopia/AI/Self Fulfilling/Refuting Prophecies","date":"2019-11-18T23:11","url":"https://www.lesswrong.com/posts/yArZKCEheZt8GkK6p/self-fulfilling-prophecies-aren-t-always-about-self"},{"x":"10.221098","y":"8.529068","title":"The Value Definition Problem","cluster":"4","author":"['Sammy Martin']","source":"alignment forum","tags":"Metaethics/AI","date":"2019-11-18T19:56","url":"https://www.lesswrong.com/posts/W95gbuognJu5WxkTW/the-value-definition-problem"},{"x":"10.987963","y":"9.296222","title":"Impossible moral problems and moral authority","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Philosophy","date":"2019-11-18T09:28","url":"https://www.lesswrong.com/posts/pW6YJEzoRFe9cshuN/impossible-moral-problems-and-moral-authority"},{"x":"7.745073","y":"9.211804","title":"[Question] How common is it for one entity to have a 3+ year technological lead on its nearest competitor?","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI Takeoff/AI Timelines","date":"2019-11-17T15:23","url":"https://www.lesswrong.com/posts/yXikQ87FFw3oPPaYh/how-common-is-it-for-one-entity-to-have-a-3-year"},{"x":"10.485077","y":"7.769435","title":"Evolution of Modularity","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Evolution/Biology/Gears-Level","date":"2019-11-14T06:49","url":"https://www.lesswrong.com/posts/JBFHzfPkXHB2XfDGj/evolution-of-modularity"},{"x":"8.555095","y":"7.2728114","title":"What I’ll be doing at MIRI","cluster":"3","author":"['evhub']","source":"alignment forum","tags":"Machine Intelligence Research Institute (MIRI)","date":"2019-11-12T23:19","url":"https://www.lesswrong.com/posts/ptmmK9PWgYTuWToaZ/what-i-ll-be-doing-at-miri"},{"x":"11.016315","y":"6.6983414","title":"Platonic rewards, reward features, and rewards as information","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-11-12T19:38","url":"https://www.lesswrong.com/posts/Lb3xCRW9usoXJy9M2/platonic-rewards-reward-features-and-rewards-as-information"},{"x":"11.604049","y":"8.952497","title":"The Credit Assignment Problem","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Myopia/Optimization","date":"2019-11-08T02:50","url":"https://www.lesswrong.com/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem"},{"x":"8.506807","y":"8.9625635","title":"AI Alignment Research Overview (by Jacob Steinhardt)","cluster":"3","author":"['Ben Pace']","source":"alignment forum","tags":"AI Risk/Research Agendas","date":"2019-11-06T19:24","url":"https://www.lesswrong.com/posts/7GEviErBXcjJsbSeD/ai-alignment-research-overview-by-jacob-steinhardt"},{"x":"9.500716","y":"6.683993","title":"More variations on pseudo-alignment","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2019-11-04T23:24","url":"https://www.lesswrong.com/posts/iydwbZhATANhjoGP7/more-variations-on-pseudo-alignment"},{"x":"8.916239","y":"6.7275214","title":"Will transparency help catch deception? Perhaps not","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/Deception/AI","date":"2019-11-04T20:52","url":"https://www.lesswrong.com/posts/J9D6Bi3eFDDhCaovi/will-transparency-help-catch-deception-perhaps-not"},{"x":"10.772613","y":"9.260648","title":"But exactly how complex and fragile?","cluster":"1","author":"['KatjaGrace']","source":"alignment forum","tags":"Value Learning/Complexity of Value/AI Risk","date":"2019-11-03T18:20","url":"https://www.lesswrong.com/posts/xzFQp7bmkoKfnae9R/but-exactly-how-complex-and-fragile"},{"x":"10.9496","y":"9.389269","title":"\"embedded self-justification,\" or something like that","cluster":"1","author":"['nostalgebraist']","source":"alignment forum","tags":"Embedded Agency","date":"2019-11-03T03:20","url":"https://www.lesswrong.com/posts/kczouh3rvEoxJWFh5/embedded-self-justification-or-something-like-that"},{"x":"8.735676","y":"8.842768","title":"Chris Olah’s views on AGI safety","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)/OpenAI","date":"2019-11-01T20:13","url":"https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"},{"x":"9.001077","y":"8.192699","title":"Rohin Shah on reasons for AI optimism","cluster":"4","author":"['abergal']","source":"alignment forum","tags":"Interviews/Transcripts/Center for Human-Compatible AI (CHAI)/AI Risk/Transparency / Interpretability (ML & AI)","date":"2019-10-31T12:10","url":"https://www.lesswrong.com/posts/TdwpN484eTbPSvZkm/rohin-shah-on-reasons-for-ai-optimism"},{"x":"10.605802","y":"9.442266","title":"Deliberation as a method to find the \"actual preferences\" of humans","cluster":"1","author":"['riceissa']","source":"alignment forum","tags":"Meta-Philosophy/Value Learning","date":"2019-10-22T09:23","url":"https://www.lesswrong.com/posts/ebdf8GZxt3L9grwwN/deliberation-as-a-method-to-find-the-actual-preferences-of"},{"x":"10.22074","y":"7.007342","title":"Human-AI Collaboration","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Academic Papers/AI/Inverse Reinforcement Learning","date":"2019-10-22T06:32","url":"https://www.lesswrong.com/posts/dBMC63hjkc5wPqTC7/human-ai-collaboration"},{"x":"11.435518","y":"8.965619","title":"All I know is Goodhart","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Goodhart's Law","date":"2019-10-21T12:12","url":"https://www.lesswrong.com/posts/uL74oQv5PsnotGzt7/all-i-know-is-goodhart"},{"x":"10.944752","y":"8.282588","title":"Defining Myopia","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Myopia/Self Fulfilling/Refuting Prophecies/Decision Theory","date":"2019-10-19T21:32","url":"https://www.lesswrong.com/posts/qpZTWb2wvgSt5WQ4H/defining-myopia"},{"x":"8.618887","y":"8.986245","title":"Technical AGI safety research outside AI","cluster":"3","author":"['Richard_Ngo']","source":"alignment forum","tags":"Research Agendas/AI","date":"2019-10-18T15:00","url":"https://www.lesswrong.com/posts/4xbsi4wbourPkb47x/technical-agi-safety-research-outside-ai"},{"x":"9.305645","y":"6.9473233","title":"Random Thoughts on Predict-O-Matic","cluster":"2","author":"['abramdemski']","source":"alignment forum","tags":"Myopia/Self Fulfilling/Refuting Prophecies","date":"2019-10-17T23:39","url":"https://www.lesswrong.com/posts/25288usP5B5ytnzA4/random-thoughts-on-predict-o-matic"},{"x":"10.34861","y":"8.4885645","title":"The Dualist Predict-O-Matic ($100 prize)","cluster":"1","author":"['John_Maxwell']","source":"alignment forum","tags":"Bounties (closed)/Myopia/AI","date":"2019-10-17T06:45","url":"https://www.lesswrong.com/posts/RmPKdMqSr2xRwrqyE/the-dualist-predict-o-matic-usd100-prize"},{"x":"11.110165","y":"9.011556","title":"Full toy model for preference learning","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Value Learning","date":"2019-10-16T11:06","url":"https://www.lesswrong.com/posts/hcrFxeYYfbFrkKQEJ/full-toy-model-for-preference-learning"},{"x":"9.074284","y":"6.523222","title":"Gradient hacking","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"Inner Alignment/AI/Gradient Hacking/Mesa-Optimization/Transparency / Interpretability (ML & AI)","date":"2019-10-16T00:53","url":"https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking"},{"x":"10.20766","y":"8.513389","title":"The Parable of Predict-O-Matic","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Parables & Fables/Oracle AI/AI/Myopia/Self Fulfilling/Refuting Prophecies/Self Fulfilling/Refuting Prophecies","date":"2019-10-15T00:49","url":"https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic"},{"x":"10.911394","y":"7.874678","title":"Impact measurement and value-neutrality verification","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"Impact Measures/AI","date":"2019-10-15T00:06","url":"https://www.lesswrong.com/posts/jGB7Pd5q8ivBor8Ee/impact-measurement-and-value-neutrality-verification-1"},{"x":"8.666816","y":"8.953292","title":"AI alignment landscape","cluster":"3","author":"['paulfchristiano']","source":"alignment forum","tags":"AI Risk","date":"2019-10-13T02:10","url":"https://www.lesswrong.com/posts/zQgqAc9nQETjFjawJ/ai-alignment-landscape"},{"x":"9.295045","y":"8.091825","title":"Thoughts on \"Human-Compatible\"","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Inverse Reinforcement Learning/Book Reviews/AI","date":"2019-10-10T05:24","url":"https://www.lesswrong.com/posts/FuGDYNvA6qh4qyFah/thoughts-on-human-compatible"},{"x":"10.405054","y":"8.526453","title":"Minimization of prediction error as a foundation for human values in AI alignment","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Predictive Processing/Value Learning","date":"2019-10-09T18:23","url":"https://www.lesswrong.com/posts/Cu7yv4eM6dCeA67Af/minimization-of-prediction-error-as-a-foundation-for-human"},{"x":"8.29892","y":"8.431981","title":"Misconceptions about continuous takeoff","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"AI Takeoff/AI","date":"2019-10-08T21:31","url":"https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff"},{"x":"10.724926","y":"9.34815","title":"Characterizing Real-World Agents as a Research Meta-Strategy","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Agency/Intellectual Progress (Society-Level)","date":"2019-10-08T15:32","url":"https://www.lesswrong.com/posts/9pZtvjegYKBALFnLk/characterizing-real-world-agents-as-a-research-meta-strategy"},{"x":"9.557402","y":"8.929753","title":"What’s the dream for giving natural language commands to AI?","cluster":"4","author":"['Charlie Steiner']","source":"alignment forum","tags":"Value Learning/AI","date":"2019-10-08T13:42","url":"https://www.lesswrong.com/posts/Bxxh9GbJ6WuW5Hmkj/what-s-the-dream-for-giving-natural-language-commands-to-ai"},{"x":"8.1796875","y":"8.336782","title":"AI Alignment Writing Day Roundup #2","cluster":"3","author":"['Ben Pace']","source":"alignment forum","tags":"Community/List of Links","date":"2019-10-07T23:36","url":"https://www.lesswrong.com/posts/DG7asvufKgaqEknKd/ai-alignment-writing-day-roundup-2"},{"x":"11.563504","y":"8.788445","title":"Occam’s Razor May Be Sufficient to Infer the Preferences of Irrational Agents: A reply to Armstrong & Mindermann","cluster":"1","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"Occam's Razor","date":"2019-10-07T19:52","url":"https://www.lesswrong.com/posts/pHWTNMESuAEjZg2Qn/occam-s-razor-may-be-sufficient-to-infer-the-preferences-of"},{"x":"11.552918","y":"6.745017","title":"The Gears of Impact","cluster":"0","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/World Modeling","date":"2019-10-07T14:44","url":"https://www.lesswrong.com/posts/coQCEe962sjbcCqB9/the-gears-of-impact"},{"x":"10.021811","y":"8.725491","title":"The AI is the model","cluster":"4","author":"['Charlie Steiner']","source":"alignment forum","tags":"Value Learning","date":"2019-10-04T08:11","url":"https://www.lesswrong.com/posts/HS2E8woaF5h5QSptP/the-ai-is-the-model"},{"x":"9.502961","y":"9.435719","title":"Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More","cluster":"3","author":"['Ben Pace']","source":"alignment forum","tags":"AI/AI Risk/Instrumental Convergence/Center for Human-Compatible AI (CHAI)","date":"2019-10-04T04:08","url":"https://www.lesswrong.com/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell"},{"x":"10.756312","y":"8.415639","title":"Can we make peace with moral indeterminacy?","cluster":"4","author":"['Charlie Steiner']","source":"alignment forum","tags":"Value Learning/Metaethics","date":"2019-10-03T12:56","url":"https://www.lesswrong.com/posts/qpJbFta7RwpHcFarc/can-we-make-peace-with-moral-indeterminacy"},{"x":"10.547649","y":"8.687939","title":"What are we assuming about utility functions?","cluster":"4","author":"['Grue_Slinky']","source":"alignment forum","tags":"","date":"2019-10-02T15:11","url":"https://www.lesswrong.com/posts/Q9JKKwSFybCTtMS9d/what-are-we-assuming-about-utility-functions"},{"x":"9.126497","y":"7.349906","title":"Human instincts, symbol grounding, and the blank-slate neocortex","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuroscience/Neuromorphic AI/Symbol Grounding/Neocortex","date":"2019-10-02T12:06","url":"https://www.lesswrong.com/posts/NkSpukDkm9pjRdMdB/human-instincts-symbol-grounding-and-the-blank-slate"},{"x":"11.192836","y":"9.376204","title":"Toy model #6: Rationality and partial preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-10-02T12:04","url":"https://www.lesswrong.com/posts/CG7oi95zG3knJ74s5/toy-model-6-rationality-and-partial-preferences"},{"x":"12.031521","y":"9.102977","title":"World State is the Wrong Abstraction for Impact","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI/World Modeling/Exercises / Problem-Sets","date":"2019-10-01T21:03","url":"https://www.lesswrong.com/posts/pr3bLc2LtjARfK7nx/world-state-is-the-wrong-abstraction-for-impact"},{"x":"8.6840925","y":"8.738909","title":"List of resolved confusions about IDA","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"Outer Alignment/Updated Beliefs (examples of)","date":"2019-09-30T20:03","url":"https://www.lesswrong.com/posts/FdfzFcRvqLf4k5eoQ/list-of-resolved-confusions-about-ida"},{"x":"11.440631","y":"8.475777","title":"Partial Agency","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Myopia","date":"2019-09-27T22:04","url":"https://www.lesswrong.com/posts/4hdHto3uHejhY2F3Q/partial-agency"},{"x":"9.920572","y":"6.956296","title":"A simple environment for showing mesa misalignment","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"Inner Alignment/AI","date":"2019-09-26T04:44","url":"https://www.lesswrong.com/posts/AFdRGfYDWQqmkdhFq/a-simple-environment-for-showing-mesa-misalignment"},{"x":"9.923921","y":"6.8248897","title":"Towards an empirical investigation of inner alignment","cluster":"0","author":"['evhub']","source":"alignment forum","tags":"Inner Alignment/AI","date":"2019-09-23T20:43","url":"https://www.lesswrong.com/posts/2GycxikGnepJbxfHT/towards-an-empirical-investigation-of-inner-alignment"},{"x":"12.102045","y":"9.56103","title":"Build a Causal Decision Theorist","cluster":"1","author":"['michaelcohen']","source":"alignment forum","tags":"Causal Decision Theory/Decision Theory/AI","date":"2019-09-23T20:43","url":"https://www.lesswrong.com/posts/CvBn9vNL65AMhAAs6/build-a-causal-decision-theorist"},{"x":"11.182991","y":"8.966368","title":"Value Impact","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/Exercises / Problem-Sets/World Modeling","date":"2019-09-23T00:47","url":"https://www.lesswrong.com/posts/TxcYSRQ9giC6zmKov/value-impact"},{"x":"9.944045","y":"7.6792955","title":"[Question] What are the differences between all the iterative/​recursive approaches to AI alignment?","cluster":"4","author":"['riceissa']","source":"alignment forum","tags":"AI/Humans Consulting HCH/Iterated Amplification /Factored Cognition","date":"2019-09-21T02:09","url":"https://www.lesswrong.com/posts/cYduioQNeHALQAMre/what-are-the-differences-between-all-the-iterative-recursive"},{"x":"11.283089","y":"8.264486","title":"Reframing Impact","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures/Exercises / Problem-Sets/AI","date":"2019-09-20T19:03","url":"https://www.lesswrong.com/posts/xCxeBSHqMEaP3jDvY/reframing-impact"},{"x":"11.758351","y":"8.622944","title":"The strategy-stealing assumption","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"AI Risk/AI","date":"2019-09-16T15:23","url":"https://www.lesswrong.com/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption"},{"x":"11.6121435","y":"9.479806","title":"Utility uncertainty vs. expected information gain","cluster":"1","author":"['michaelcohen']","source":"alignment forum","tags":"","date":"2019-09-13T21:09","url":"https://www.lesswrong.com/posts/Pkr97mB9Y4rkx5DdZ/utility-uncertainty-vs-expected-information-gain"},{"x":"11.325556","y":"7.955375","title":"Do Sufficiently Advanced Agents Use Logic?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Epistemology/Rationality","date":"2019-09-13T19:53","url":"https://www.lesswrong.com/posts/3qXE6fK47JhSfkpnB/do-sufficiently-advanced-agents-use-logic"},{"x":"11.703243","y":"9.380817","title":"A Critique of Functional Decision Theory","cluster":"1","author":"['wdmacaskill']","source":"alignment forum","tags":"Decision Theory/Functional Decision Theory","date":"2019-09-13T19:23","url":"https://www.lesswrong.com/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory"},{"x":"10.191346","y":"9.248355","title":"Theory of Ideal Agents, or of Existing Agents?","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"","date":"2019-09-13T17:38","url":"https://www.lesswrong.com/posts/zQZcWkvEA8DLjKR7C/theory-of-ideal-agents-or-of-existing-agents"},{"x":"11.562521","y":"6.748269","title":"What You See Isn’t Always What You Want","cluster":"0","author":"['TurnTrout']","source":"alignment forum","tags":"AI","date":"2019-09-13T04:17","url":"https://www.lesswrong.com/posts/AeHtdxHheMjHredaq/what-you-see-isn-t-always-what-you-want"},{"x":"11.313144","y":"9.334614","title":"Toy model piece #5: combining partial preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-09-12T03:31","url":"https://www.lesswrong.com/posts/ucG63yw3JgP3rW7tH/toy-model-piece-5-combining-partial-preferences"},{"x":"11.275429","y":"9.635868","title":"Toy model piece #4: partial preferences, re-re-visited","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-09-12T03:31","url":"https://www.lesswrong.com/posts/2vnJmWDAaLKDayvwr/toy-model-piece-4-partial-preferences-re-re-visited"},{"x":"8.67364","y":"8.869513","title":"Conversation with Paul Christiano","cluster":"3","author":"['abergal']","source":"alignment forum","tags":"Transcripts/Interviews/AI/AI Risk","date":"2019-09-11T23:20","url":"https://www.lesswrong.com/posts/qnYZmtpNPZyqHpot9/conversation-with-paul-christiano"},{"x":"9.286854","y":"7.0406857","title":"Relaxed adversarial training for inner alignment","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"Inner Alignment/Transparency / Interpretability (ML & AI)/AI/Iterated Amplification /AI Risk","date":"2019-09-10T23:03","url":"https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment"},{"x":"9.100307","y":"9.238972","title":"Hackable Rewards as a Safety Valve?","cluster":"4","author":"['Davidmanheim']","source":"alignment forum","tags":"","date":"2019-09-10T10:33","url":"https://www.lesswrong.com/posts/3RdvPS5LawYxLuHLH/hackable-rewards-as-a-safety-valve"},{"x":"11.257802","y":"7.098052","title":"Counterfactual Oracles = online supervised learning with random selection of training episodes","cluster":"0","author":"['Wei_Dai']","source":"alignment forum","tags":"Oracle AI/AI Risk/AI Boxing (Containment)/Counterfactuals","date":"2019-09-10T08:29","url":"https://www.lesswrong.com/posts/yAiqLmLFxvyANSfs2/counterfactual-oracles-online-supervised-learning-with"},{"x":"10.874228","y":"8.792075","title":"Is my result wrong? Maths vs intuition vs evolution in learning human preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Intuition","date":"2019-09-10T00:46","url":"https://www.lesswrong.com/posts/2KLz6RQWkCj4Rozrk/is-my-result-wrong-maths-vs-intuition-vs-evolution-in"},{"x":"11.255418","y":"9.505959","title":"Simple and composite partial preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-09-09T23:07","url":"https://www.lesswrong.com/posts/eptGDwahJPvhqDgvF/simple-and-composite-partial-preferences"},{"x":"9.586087","y":"7.4899497","title":"Are minimal circuits deceptive?","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI Risk/Inner Alignment/AI/Deception","date":"2019-09-07T18:11","url":"https://www.lesswrong.com/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive"},{"x":"8.665445","y":"9.25899","title":"AI Safety \"Success Stories\"","cluster":"3","author":"['Wei_Dai']","source":"alignment forum","tags":"AI Risk/AI Success Models","date":"2019-09-07T02:54","url":"https://www.lesswrong.com/posts/bnY3L48TtDrKTzGRb/ai-safety-success-stories"},{"x":"7.9224024","y":"9.025232","title":"[Question] What are concrete examples of potential \"lock-in\" in AI research?","cluster":"3","author":"['Grue_Slinky']","source":"alignment forum","tags":"","date":"2019-09-06T23:39","url":"https://www.lesswrong.com/posts/7iGaZGBrPodYeur6X/what-are-concrete-examples-of-potential-lock-in-in-ai"},{"x":"10.06094","y":"6.899382","title":"Concrete experiments in inner alignment","cluster":"0","author":"['evhub']","source":"alignment forum","tags":"Inner Alignment/AI","date":"2019-09-06T22:16","url":"https://www.lesswrong.com/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inner-alignment"},{"x":"11.979147","y":"10.105532","title":"How to Throw Away Information","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Information Theory","date":"2019-09-05T21:10","url":"https://www.lesswrong.com/posts/DEDcFw6zWfW9nb2YM/how-to-throw-away-information"},{"x":"10.402277","y":"7.5919976","title":"Utility ≠ Reward","cluster":"4","author":"['vlad_m']","source":"alignment forum","tags":"Utility Functions/Mesa-Optimization/Distinctions","date":"2019-09-05T17:28","url":"https://www.lesswrong.com/posts/bG4PR9uSsZqHg2gYY/utility-reward"},{"x":"12.332208","y":"10.027321","title":"Logical Counterfactuals and Proposition graphs, Part 3","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"Counterfactuals","date":"2019-09-05T15:03","url":"https://www.lesswrong.com/posts/fhbb8MGEs3t5dTCLD/logical-counterfactuals-and-proposition-graphs-part-3"},{"x":"12.192106","y":"9.519637","title":"Counterfactuals are an Answer, Not a Question","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Counterfactuals","date":"2019-09-03T15:36","url":"https://www.lesswrong.com/posts/ao7KLoBEvMdHFjrNZ/counterfactuals-are-an-answer-not-a-question"},{"x":"11.450913","y":"9.483564","title":"Best utility normalisation method to date?","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-09-02T18:24","url":"https://www.lesswrong.com/posts/GfMGa9e79AfDMLj36/best-utility-normalisation-method-to-date"},{"x":"11.721029","y":"10.0391035","title":"Probability as Minimal Map","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Logic & Mathematics ","date":"2019-09-01T19:19","url":"https://www.lesswrong.com/posts/Lz2nCYnBeaZyS68Xb/probability-as-minimal-map"},{"x":"12.54115","y":"10.019419","title":"Logical Counterfactuals and Proposition graphs, Part 2","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"Counterfactuals","date":"2019-08-31T20:58","url":"https://www.lesswrong.com/posts/due5BtsbpTzSZbeKT/logical-counterfactuals-and-proposition-graphs-part-2"},{"x":"9.494132","y":"6.623751","title":"2-D Robustness","cluster":"0","author":"['vlad_m']","source":"alignment forum","tags":"Mesa-Optimization/Inner Alignment/Robust Agents","date":"2019-08-30T20:27","url":"https://www.lesswrong.com/posts/2mhFMgtAjFJesaSYR/2-d-robustness"},{"x":"12.474758","y":"9.703214","title":"AI Alignment Writing Day Roundup #1","cluster":"1","author":"['Ben Pace']","source":"alignment forum","tags":"AI/List of Links/Community","date":"2019-08-30T01:26","url":"https://www.lesswrong.com/posts/ZYGjDpGQaHvg8HLfw/ai-alignment-writing-day-roundup-1"},{"x":"10.596869","y":"7.6167326","title":"Humans can be assigned any values whatsoever...","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-10-24T12:03","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754e8/humans-can-be-assigned-any-values-whatsoever"},{"x":"12.282999","y":"9.292846","title":"Comparing LICDT and LIEDT","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-10-21T23:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754ba/comparing-licdt-and-liedt"},{"x":"11.519549","y":"6.897696","title":"Delegative Reinforcement Learning with a Merely Sane Advisor","cluster":"0","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Reinforcement Learning","date":"2017-10-05T14:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor"},{"x":"11.788875","y":"9.0736475","title":"Smoking Lesion Steelman III: Revenge of the Tickle Defense","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-10-05T07:28","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754dc/smoking-lesion-steelman-iii-revenge-of-the-tickle-defense"},{"x":"11.38652","y":"7.8464746","title":"Resolving human inconsistency in a simple model","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-10-04T15:02","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754dd/resolving-human-inconsistency-in-a-simple-model"},{"x":"11.985495","y":"9.277246","title":"Smoking Lesion Steelman II","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-10-02T22:11","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754db/smoking-lesion-steelman-ii"},{"x":"11.492387","y":"9.175229","title":"The Doomsday argument in anthropic decision theory","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Anthropics","date":"2017-08-31T13:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754d4/the-doomsday-argument-in-anthropic-decision-theory"},{"x":"12.266229","y":"10.068578","title":"Using modal fixed points to formalize logical causality","cluster":"1","author":"['cousin_it']","source":"alignment forum","tags":"Fixed Point Theorems","date":"2017-08-24T14:33","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374e61/using-modal-fixed-points-to-formalize-logical-causality"},{"x":"9.384508","y":"8.624741","title":"Autopoietic systems and difficulty of AGI alignment","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-08-20T01:05","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754b9/autopoietic-systems-and-difficulty-of-agi-alignment"},{"x":"11.8239","y":"9.396986","title":"Conditioning on Conditionals","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2017-08-17T01:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754b5/conditioning-on-conditionals"},{"x":"11.994416","y":"8.097069","title":"Density Zero Exploration","cluster":"1","author":"['AlexMennen']","source":"alignment forum","tags":"","date":"2017-08-17T00:43","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754b8/density-zero-exploration"},{"x":"12.765805","y":"9.906831","title":"Logical Induction with incomputable sequences","cluster":"1","author":"['AlexMennen']","source":"alignment forum","tags":"Logical Induction/Logical Uncertainty","date":"2017-08-17T00:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754b7/logical-induction-with-incomputable-sequences"},{"x":"11.383737","y":"7.8174706","title":"Stable Pointers to Value: An Agent Embedded in Its Own Utility Function","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"The Pointers Problem/AI/Value Learning","date":"2017-08-17T00:22","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754b3/stable-pointers-to-value-an-agent-embedded-in-its-own-utility-function"},{"x":"11.554964","y":"6.6165586","title":"CIRL Wireheading","cluster":"0","author":"['tom4everitt']","source":"alignment forum","tags":"Inverse Reinforcement Learning","date":"2017-08-08T06:33","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375410/cirl-wireheading"},{"x":"12.324064","y":"9.671751","title":"On the computational feasibility of forecasting using gamblers","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2017-07-18T14:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375496/on-the-computational-feasibility-of-forecasting-using-gamblers"},{"x":"8.897064","y":"8.33454","title":"Current thoughts on Paul Christano’s research agenda","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-07-16T21:08","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037545b/current-thoughts-on-paul-christano-s-research-agenda"},{"x":"11.451895","y":"6.998895","title":"Delegative Inverse Reinforcement Learning","cluster":"0","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Inverse Reinforcement Learning/Reinforcement Learning","date":"2017-07-12T12:18","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning"},{"x":"12.090129","y":"9.411786","title":"Smoking Lesion Steelman","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-07-02T02:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375452/smoking-lesion-steelman"},{"x":"12.67857","y":"8.794472","title":"A cheating approach to the tiling agents problem","cluster":"1","author":"['cousin_it']","source":"alignment forum","tags":"","date":"2017-06-30T13:56","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375468/a-cheating-approach-to-the-tiling-agents-problem"},{"x":"12.512709","y":"8.599725","title":"Loebian cooperation in the tiling agents problem","cluster":"4","author":"['cousin_it']","source":"alignment forum","tags":"","date":"2017-06-26T14:52","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375459/loebian-cooperation-in-the-tiling-agents-problem"},{"x":"12.656911","y":"8.577809","title":"Cooperative Oracles: Nonexploited Bargaining","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Oracle AI","date":"2017-06-03T00:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037541a/cooperative-oracles-nonexploited-bargaining"},{"x":"12.730686","y":"8.707574","title":"Cooperative Oracles: Stratified Pareto Optima and Almost Stratified Pareto Optima","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Oracle AI","date":"2017-06-03T00:38","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375441/cooperative-oracles-stratified-pareto-optima-and-almost-stratified-pareto-optima"},{"x":"12.898455","y":"8.829442","title":"Cooperative Oracles: Introduction","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Oracle AI","date":"2017-06-03T00:36","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375419/cooperative-oracles-introduction"},{"x":"12.465564","y":"8.391942","title":"Entangled Equilibria and the Twin Prisoners’ Dilemma","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2017-06-02T22:09","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037535c/entangled-equilibria-and-the-twin-prisoners-dilemma"},{"x":"11.248586","y":"6.8596997","title":"An algorithm with preferences: from zero to one variable","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T16:35","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037529c/an-algorithm-with-preferences-from-zero-to-one-variable"},{"x":"12.258434","y":"8.988663","title":"Counterfactuals on POMDP","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Counterfactuals","date":"2017-06-02T16:30","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703752a9/counterfactuals-on-pomdp"},{"x":"11.540056","y":"7.4172754","title":"Uninfluenceable learning agents","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T16:30","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703752aa/uninfluenceable-learning-agents"},{"x":"9.761855","y":"8.985506","title":"Corrigibility thoughts I: caring about multiple things","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Tripwire","date":"2017-06-02T16:27","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037531d/corrigibility-thoughts-i-caring-about-multiple-things"},{"x":"11.612495","y":"7.9412384","title":"Thoughts on Quantilizers","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Mild Optimization","date":"2017-06-02T16:24","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037532c/thoughts-on-quantilizers"},{"x":"10.50035","y":"6.7290754","title":"Emergency learning","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T16:23","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375338/emergency-learning"},{"x":"9.445241","y":"8.640177","title":"Humans as a truth channel","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T16:22","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037533e/humans-as-a-truth-channel-1"},{"x":"11.199774","y":"7.6750784","title":"All the indifference designs","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T16:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375362/all-the-indifference-designs"},{"x":"11.576633","y":"7.612073","title":"Counterfactually uninfluenceable agents","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Counterfactuals","date":"2017-06-02T16:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037536b/counterfactually-uninfluenceable-agents"},{"x":"8.575745","y":"8.323568","title":"Low impact versus low side effects","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T16:14","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753a7/low-impact-versus-low-side-effects"},{"x":"8.830151","y":"8.830697","title":"AI safety: three human problems and one AI issue","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T16:12","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753c9/ai-safety-three-human-problems-and-one-ai-issue"},{"x":"10.934313","y":"8.780252","title":"Divergent preferences and meta-preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T15:51","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375431/divergent-preferences-and-meta-preferences"},{"x":"12.289907","y":"8.968497","title":"Acausal trade: double decrease","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Acausal Trade","date":"2017-06-02T15:33","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375414/acausal-trade-double-decrease"},{"x":"11.979087","y":"8.887087","title":"Futarchy, Xrisks, and near misses","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-06-02T08:02","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037543e/futarchy-xrisks-and-near-misses"},{"x":"8.824047","y":"7.540207","title":"Why I am not currently working on the AAMLS agenda","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"Research Agendas/Postmortems & Retrospectives/Machine Intelligence Research Institute (MIRI)","date":"2017-06-01T17:57","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037541b/why-i-am-not-currently-working-on-the-aamls-agenda"},{"x":"12.018367","y":"8.922136","title":"Futarchy Fix","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Futarchy","date":"2017-05-30T05:46","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375432/futarchy-fix"},{"x":"11.752466","y":"8.937563","title":"Example of double indifference","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-05-24T08:58","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375196/example-of-double-indifference"},{"x":"12.636111","y":"8.623965","title":"Reflexive Oracles and superrationality: Pareto","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI","date":"2017-05-24T08:35","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375068/reflexive-oracles-and-superrationality-pareto"},{"x":"12.866347","y":"8.775057","title":"Reflexive Oracles and superrationality: prisoner’s dilemma","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI/Prisoner's Dilemma","date":"2017-05-24T08:34","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037505e/reflexive-oracles-and-superrationality-prisoner-s-dilemma"},{"x":"11.590825","y":"6.816896","title":"Removing interrupted histories doesn’t debias","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-05-24T08:23","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751a1/removing-interrupted-histories-doesn-t-debias"},{"x":"10.909365","y":"8.205434","title":"Learning values versus indifference","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-05-24T08:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751aa/learning-values-versus-indifference"},{"x":"11.089395","y":"7.484488","title":"Guarded learning","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-05-23T16:53","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751bc/guarded-learning"},{"x":"11.112394","y":"8.3978815","title":"Simplified explanation of stratification","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-05-23T16:37","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751fc/simplified-explanation-of-stratification"},{"x":"10.063708","y":"8.532204","title":"A toy model of the treacherous turn","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-05-22T18:03","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750b0/a-toy-model-of-the-treacherous-turn"},{"x":"12.383132","y":"9.715644","title":"An Approach to Logically Updateless Decisions","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-05-21T23:02","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037541d/an-approach-to-logically-updateless-decisions"},{"x":"13.015307","y":"8.934792","title":"A correlated analogue of reflective oracles","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-05-07T07:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753f8/a-correlated-analogue-of-reflective-oracles"},{"x":"12.288197","y":"9.833885","title":"Generalizing Foundations of Decision Theory II","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-05-06T20:40","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037539a/generalizing-foundations-of-decision-theory-ii"},{"x":"11.211668","y":"9.382628","title":"Infinite ethics comparisons","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Infinities In Ethics","date":"2017-05-06T19:24","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037540c/infinite-ethics-comparisons"},{"x":"11.461836","y":"9.443115","title":"Intertheoretic utility comparison: simple theory","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-05-03T08:36","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753e7/intertheoretic-utility-comparison-simple-theory"},{"x":"11.484234","y":"7.3071847","title":"Equilibria in adversarial supervised learning","cluster":"2","author":"['RyanCarey']","source":"alignment forum","tags":"","date":"2017-05-03T08:14","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375269/equilibria-in-adversarial-supervised-learning"},{"x":"13.017482","y":"9.087851","title":"Finding reflective oracle distributions using a Kakutani map","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Oracle AI","date":"2017-05-02T02:12","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375401/finding-reflective-oracle-distributions-using-a-kakutani-map"},{"x":"9.573121","y":"9.377798","title":"Change utility, reduce extortion","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2017-04-28T13:58","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753d7/change-utility-reduce-extortion"},{"x":"12.444485","y":"9.505361","title":"Learning incomplete models using dominant markets","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2017-04-28T09:57","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753a4/learning-incomplete-models-using-dominant-markets"},{"x":"12.505849","y":"9.3925","title":"Two Major Obstacles for Logical Inductor Decision Theory","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Logical Induction","date":"2017-04-17T21:10","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753d4/two-major-obstacles-for-logical-inductor-decision-theory"},{"x":"9.756224","y":"7.643664","title":"Some problems with making induction benign, and approaches to them","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2017-03-27T06:49","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037534c/some-problems-with-making-induction-benign-and-approaches-to-them"},{"x":"11.450544","y":"8.8033905","title":"HCH as a measure of manipulation","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"Humans Consulting HCH","date":"2017-03-11T03:02","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375393/hch-as-a-measure-of-manipulation"},{"x":"12.283911","y":"8.895648","title":"Index of some decision theory posts","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2017-03-08T22:30","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037525f/index-of-some-decision-theory-posts"},{"x":"11.816028","y":"9.883334","title":"Generalizing Foundations of Decision Theory","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"VNM Theorem/Bayesian Decision Theory","date":"2017-03-04T16:46","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375373/generalizing-foundations-of-decision-theory"},{"x":"12.656314","y":"8.667669","title":"Modal Combat for games other than the prisoner’s dilemma","cluster":"1","author":"['AlexMennen']","source":"alignment forum","tags":"","date":"2017-02-26T08:10","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375375/modal-combat-for-games-other-than-the-prisoner-s-dilemma"},{"x":"8.808535","y":"9.668095","title":"Disentangling Perspectives On Strategy-Stealing in AI Safety","cluster":"3","author":"['shawnghu']","source":"alignment forum","tags":"AI/Existential Risk","date":"2021-12-18T20:13","url":"https://www.lesswrong.com/posts/CtGwGgxfoefiwfcor/disentangling-perspectives-on-strategy-stealing-in-ai-safety"},{"x":"9.68471","y":"8.311625","title":"Introducing the Principles of Intelligent Behaviour in Biological and Social Systems (PIBBSS) Fellowship","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"AI","date":"2021-12-18T15:23","url":"https://www.lesswrong.com/posts/4Tjz4EJ8DozE9z5nQ/introducing-the-principles-of-intelligent-behaviour-in"},{"x":"8.544845","y":"7.99333","title":"Evidence Sets: Towards Inductive-Biases based Analysis of Prosaic AGI","cluster":"4","author":"['bayesian_kitten']","source":"alignment forum","tags":"AI/Adversarial Examples/Machine Learning/Inner Alignment/Language Models","date":"2021-12-16T22:41","url":"https://www.lesswrong.com/posts/aW5CPtqtvs2EYKMMK/evidence-sets-towards-inductive-biases-based-analysis-of"},{"x":"8.683697","y":"9.735817","title":"Elicitation for Modeling Transformative AI Risks","cluster":"3","author":"['Davidmanheim']","source":"alignment forum","tags":"AI","date":"2021-12-16T15:24","url":"https://www.lesswrong.com/posts/Kz9NHBMeJxzSwb7R9/elicitation-for-modeling-transformative-ai-risks"},{"x":"9.1371","y":"7.5750084","title":"Motivations, Natural Selection, and Curriculum Engineering","cluster":"4","author":"['Oliver Sourbut']","source":"alignment forum","tags":"AI/Mesa-Optimization/Evolution/Adaptation Executors/General Intelligence/Corrigibility","date":"2021-12-16T01:07","url":"https://www.lesswrong.com/posts/RwYh4grJs4pbJdTh3/motivations-natural-selection-and-curriculum-engineering"},{"x":"10.758368","y":"9.812797","title":"Universality and the \"Filter\"","cluster":"1","author":"['maggiehayes']","source":"alignment forum","tags":"AI Risk/AI/Humans Consulting HCH","date":"2021-12-16T00:47","url":"https://www.lesswrong.com/posts/R67qpBj5doGcjQzmc/universality-and-the-filter"},{"x":"8.19368","y":"8.367226","title":"My Overview of the AI Alignment Landscape: A Bird’s Eye View","cluster":"3","author":"['Neel Nanda']","source":"alignment forum","tags":"AI/Inner Alignment/Outer Alignment/Transparency / Interpretability (ML & AI)/AI Risk/Existential Risk/Threat Models/AI Timelines/AI Takeoff/Coordination / Cooperation/Iterated Amplification /Debate (AI safety technique)","date":"2021-12-15T23:44","url":"https://www.lesswrong.com/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view"},{"x":"10.600097","y":"10.287358","title":"The Natural Abstraction Hypothesis: Implications and Evidence","cluster":"4","author":"['TheMcDouglas']","source":"alignment forum","tags":"AI/Abstraction/Transparency / Interpretability (ML & AI)","date":"2021-12-14T23:14","url":"https://www.lesswrong.com/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence"},{"x":"8.370621","y":"7.444907","title":"Ngo’s view on alignment difficulty","cluster":"4","author":"['Richard_Ngo', 'Eliezer Yudkowsky']","source":"alignment forum","tags":"AI Governance/AI","date":"2021-12-14T21:34","url":"https://www.lesswrong.com/posts/gf9hhmSvpZfyfS34B/ngo-s-view-on-alignment-difficulty"},{"x":"9.670498","y":"7.2714505","title":"Should we rely on the speed prior for safety?","cluster":"4","author":"['Marc-Everin Carauleanu']","source":"alignment forum","tags":"AI","date":"2021-12-14T20:45","url":"https://www.lesswrong.com/posts/iALu99gYbodt4mLqg/should-we-rely-on-the-speed-prior-for-safety"},{"x":"7.9752817","y":"7.9435067","title":"ARC’s first technical report: Eliciting Latent Knowledge","cluster":"4","author":"['paulfchristiano', 'Mark Xu', 'Ajeya Cotra']","source":"alignment forum","tags":"Eliciting Latent Knowledge (ELK)/AI","date":"2021-12-14T20:09","url":"https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge"},{"x":"8.1516485","y":"7.31457","title":"ARC is hiring!","cluster":"2","author":"['paulfchristiano', 'Mark Xu']","source":"alignment forum","tags":"AI/Community","date":"2021-12-14T20:09","url":"https://www.lesswrong.com/posts/dLoK6KGcHAoudtwdo/arc-is-hiring"},{"x":"9.957176","y":"8.578185","title":"Interlude: Agents as Automobiles","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI","date":"2021-12-14T18:49","url":"https://www.lesswrong.com/posts/cxkwQmys6mCB6bjDA/interlude-agents-as-automobiles"},{"x":"10.966488","y":"9.071425","title":"Consequentialism & corrigibility","cluster":"1","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Corrigibility/Utility Functions","date":"2021-12-14T13:23","url":"https://www.lesswrong.com/posts/KDMLJEXTWtkZWheXt/consequentialism-and-corrigibility"},{"x":"7.5813265","y":"7.054937","title":"Language Model Alignment Research Internship","cluster":"2","author":"['Ethan Perez']","source":"alignment forum","tags":"Language Models/AI","date":"2021-12-13T19:53","url":"https://www.lesswrong.com/posts/vHcGGrnzcshybrCJD/language-model-alignment-research-internship"},{"x":"7.9961333","y":"8.1140995","title":"Solving Interpretability Week ","cluster":"1","author":"['Logan Riggs']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI","date":"2021-12-13T17:09","url":"https://www.lesswrong.com/posts/jbdDxmhxBygDqbQMD/solving-interpretability-week"},{"x":"11.52538","y":"7.5555716","title":"Understanding and controlling auto-induced distributional shift","cluster":"0","author":"['LRudL']","source":"alignment forum","tags":"Machine Learning/AI/Myopia","date":"2021-12-13T14:59","url":"https://www.lesswrong.com/posts/rTYGMbmEsFkxyyXuR/understanding-and-controlling-auto-induced-distributional"},{"x":"7.9575753","y":"6.028611","title":"What’s the backward-forward FLOP ratio for Neural Networks?","cluster":"2","author":"['Marius Hobbhahn', 'Jsevillamol']","source":"alignment forum","tags":"AI/AI Timelines/AI Capabilities","date":"2021-12-13T08:54","url":"https://www.lesswrong.com/posts/fnjKpBoWJXcSDwhZk/what-s-the-backward-forward-flop-ratio-for-neural-networks"},{"x":"10.1549835","y":"9.295071","title":"Summary of the Acausal Attack Issue for AIXI","cluster":"4","author":"['Diffractor']","source":"alignment forum","tags":"AI/AI Risk/AIXI/Solomonoff Induction","date":"2021-12-13T08:16","url":"https://www.lesswrong.com/posts/YbahERfcjTu7LZNQ6/summary-of-the-acausal-attack-issue-for-aixi"},{"x":"7.6925917","y":"6.46667","title":"Hard-Coding Neural Computation","cluster":"2","author":"['MadHatter']","source":"alignment forum","tags":"AI/Language Models","date":"2021-12-13T04:35","url":"https://www.lesswrong.com/posts/HkghiK6Rt35nbgwKA/hard-coding-neural-computation"},{"x":"9.264383","y":"9.174568","title":"Redwood’s Technique-Focused Epistemic Strategy","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"Redwood Research/AI","date":"2021-12-12T16:36","url":"https://www.lesswrong.com/posts/2xrBxhRhde7Xddt38/redwood-s-technique-focused-epistemic-strategy"},{"x":"8.5866375","y":"8.776377","title":"Some abstract, non-technical reasons to be non-maximally-pessimistic about AI alignment","cluster":"3","author":"['Rob Bensinger']","source":"alignment forum","tags":"AI Risk/AI","date":"2021-12-12T02:08","url":"https://www.lesswrong.com/posts/vT4tsttHgYJBoKi4n/some-abstract-non-technical-reasons-to-be-non-maximally"},{"x":"8.587543","y":"6.005228","title":"Transforming myopic optimization to ordinary optimization—Do we want to seek convergence for myopic optimization problems?","cluster":"2","author":"['tailcalled']","source":"alignment forum","tags":"AI/Myopia/Optimization","date":"2021-12-11T20:38","url":"https://www.lesswrong.com/posts/Ayt24gxcjfY3tDmwK/transforming-myopic-optimization-to-ordinary-optimization-do"},{"x":"8.467322","y":"8.073763","title":"Moore’s Law, AI, and the pace of progress","cluster":"3","author":"['Veedrac']","source":"alignment forum","tags":"Moore's Law/Nanotechnology/AI/Computer Science","date":"2021-12-11T03:02","url":"https://www.lesswrong.com/posts/aNAFrGbzXddQBMDqh/moore-s-law-ai-and-the-pace-of-progress"},{"x":"9.038882","y":"8.252537","title":"The Plan","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Research Agendas/Deconfusion","date":"2021-12-10T23:41","url":"https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan"},{"x":"9.030949","y":"6.6138415","title":"Understanding Gradient Hacking","cluster":"2","author":"['peterbarnett']","source":"alignment forum","tags":"AI/Inner Alignment/Gradient Hacking/Optimization/Mesa-Optimization","date":"2021-12-10T15:58","url":"https://www.lesswrong.com/posts/bdayaswyewjxxrQmB/understanding-gradient-hacking"},{"x":"10.198454","y":"8.668828","title":"There is essentially one best-validated theory of cognition.","cluster":"4","author":"['abramdemski']","source":"alignment forum","tags":"Cognitive Science/World Modeling","date":"2021-12-10T15:51","url":"https://www.lesswrong.com/posts/NB9QrBa335GDijuyn/there-is-essentially-one-best-validated-theory-of-cognition"},{"x":"11.373345","y":"10.0207","title":"The Promise and Peril of Finite Sets","cluster":"1","author":"['davidad']","source":"alignment forum","tags":"Logic & Mathematics /World Modeling","date":"2021-12-10T12:29","url":"https://www.lesswrong.com/posts/qLaShfcnXGnYeKFJW/the-promise-and-peril-of-finite-sets"},{"x":"7.72618","y":"8.814661","title":"Conversation on technology forecasting and gradualism","cluster":"1","author":"['Richard_Ngo', 'Eliezer Yudkowsky', 'Rohin Shah', 'Rob Bensinger']","source":"alignment forum","tags":"Technological Forecasting/AI Timelines/AI Takeoff/Progress Studies","date":"2021-12-09T21:23","url":"https://www.lesswrong.com/posts/nPauymrHwpoNr6ipx/conversation-on-technology-forecasting-and-gradualism"},{"x":"8.238376","y":"4.994092","title":"[MLSN #2]: Adversarial Training","cluster":"2","author":"['Daniel Hendrycks']","source":"alignment forum","tags":"AI","date":"2021-12-09T17:16","url":"https://www.lesswrong.com/posts/7GQZyooNi5nqgoyyJ/mlsn-2-adversarial-training"},{"x":"8.598735","y":"6.8910155","title":" Supervised learning and self-modeling: What’s \"superhuman?\"","cluster":"2","author":"['Charlie Steiner']","source":"alignment forum","tags":"AI","date":"2021-12-09T12:44","url":"https://www.lesswrong.com/posts/pz84sQKsgg3GBHQpd/supervised-learning-and-self-modeling-what-s-superhuman"},{"x":"10.156979","y":"8.8291855","title":"Introduction to inaccessible information","cluster":"4","author":"['Ryan Kidd']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)","date":"2021-12-09T01:28","url":"https://www.lesswrong.com/posts/CYKeDjD7FEvAnzBBF/introduction-to-inaccessible-information"},{"x":"10.288819","y":"6.2245665","title":"Finding the multiple ground truths of CoinRun and image classification","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-12-08T18:13","url":"https://www.lesswrong.com/posts/oCWk8QpjgyqbFHKtK/finding-the-multiple-ground-truths-of-coinrun-and-image"},{"x":"8.588395","y":"5.32129","title":"Some thoughts on why adversarial training might be useful","cluster":"2","author":"['Beth Barnes']","source":"alignment forum","tags":"AI","date":"2021-12-08T01:28","url":"https://www.lesswrong.com/posts/Gi8HPM8iYZcdAEteJ/some-thoughts-on-why-adversarial-training-might-be-useful"},{"x":"9.549094","y":"8.943616","title":"Theoretical Neuroscience For Alignment Theory","cluster":"4","author":"['Cameron Berg']","source":"alignment forum","tags":"AI/Neuroscience/Inner Alignment","date":"2021-12-07T21:50","url":"https://www.lesswrong.com/posts/ZJY3eotLdfBPCLP3z/theoretical-neuroscience-for-alignment-theory"},{"x":"9.107805","y":"9.316903","title":"Considerations on interaction between AI and expected value of the future ","cluster":"3","author":"['Beth Barnes']","source":"alignment forum","tags":"AI","date":"2021-12-07T02:46","url":"https://www.lesswrong.com/posts/Dr3owdPqEAFK4pq8S/considerations-on-interaction-between-ai-and-expected-value"},{"x":"8.453301","y":"6.381517","title":"Declustering, reclustering, and filling in thingspace","cluster":"2","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling","date":"2021-12-06T20:53","url":"https://www.lesswrong.com/posts/WikzbCsFjpLTRQmXn/declustering-reclustering-and-filling-in-thingspace"},{"x":"9.2598505","y":"9.150374","title":"More Christiano, Cotra, and Yudkowsky on AI progress","cluster":"4","author":"['Eliezer Yudkowsky', 'Ajeya Cotra']","source":"alignment forum","tags":"AI Takeoff/AI Timelines/AI/Technological Forecasting/Double-Crux","date":"2021-12-06T20:33","url":"https://www.lesswrong.com/posts/fS7Zdj2e2xMqE6qja/more-christiano-cotra-and-yudkowsky-on-ai-progress"},{"x":"10.59934","y":"8.696993","title":"Are there alternative to solving value transfer and extrapolation?","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-12-06T18:53","url":"https://www.lesswrong.com/posts/DjTKMEwRqpuKkJzTo/are-there-alternative-to-solving-value-transfer-and"},{"x":"12.268469","y":"9.485797","title":"A Possible Resolution To Spurious Counterfactuals","cluster":"1","author":"['JoshuaOSHickman']","source":"alignment forum","tags":"World Modeling/Logic & Mathematics /Embedded Agency/Decision Theory","date":"2021-12-06T18:26","url":"https://www.lesswrong.com/posts/TnkDtTAqCGetvLsgr/a-possible-resolution-to-spurious-counterfactuals"},{"x":"8.570438","y":"8.748709","title":"Modeling Failure Modes of High-Level Machine Intelligence","cluster":"3","author":"['Ben Cottier', 'Daniel_Eth', 'Sammy Martin']","source":"alignment forum","tags":"AI/World Modeling/AI Risk/Threat Models","date":"2021-12-06T13:54","url":"https://www.lesswrong.com/posts/3Eq5Rq5uQ97kt8B8f/modeling-failure-modes-of-high-level-machine-intelligence"},{"x":"12.105708","y":"10.053876","title":"A Framework to Explain Bayesian Models","cluster":"1","author":"['Jsevillamol']","source":"alignment forum","tags":"World Modeling","date":"2021-12-06T10:38","url":"https://www.lesswrong.com/posts/SPfZiEwHotPncJBLz/a-framework-to-explain-bayesian-models"},{"x":"7.731787","y":"8.538972","title":"ML Alignment Theory Program under Evan Hubinger","cluster":"3","author":"['Oliver Zhang', 'evhub', 'Victor W']","source":"alignment forum","tags":"AI/Project Announcement","date":"2021-12-06T00:03","url":"https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger"},{"x":"9.26388","y":"8.567699","title":"[Question] Are limited-horizon agents a good heuristic for the off-switch problem?","cluster":"4","author":"['Yonadav Shavit']","source":"alignment forum","tags":"AI","date":"2021-12-05T19:27","url":"https://www.lesswrong.com/posts/itTLCFj5NCHhFbK2Q/are-limited-horizon-agents-a-good-heuristic-for-the-off"},{"x":"10.275097","y":"10.063365","title":"Interpreting Yudkowsky on Deep vs Shallow Knowledge","cluster":"2","author":"['adamShimi']","source":"alignment forum","tags":"AI/AI Capabilities","date":"2021-12-05T17:32","url":"https://www.lesswrong.com/posts/GSBCw94DsxLgDat6r/interpreting-yudkowsky-on-deep-vs-shallow-knowledge"},{"x":"10.498527","y":"6.880063","title":"Behavior Cloning is Miscalibrated","cluster":"4","author":"['leogao']","source":"alignment forum","tags":"AI/Machine Learning/Reinforcement Learning/Calibration/Outer Alignment","date":"2021-12-05T01:36","url":"https://www.lesswrong.com/posts/BgoKdAzogxmgkuuAt/behavior-cloning-is-miscalibrated"},{"x":"10.789919","y":"8.87469","title":"Agents as P₂B Chain Reactions","cluster":"1","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"Agency/Rationality","date":"2021-12-04T21:35","url":"https://www.lesswrong.com/posts/oiftkZnFBqyHGALwv/agents-as-p-b-chain-reactions"},{"x":"9.691894","y":"8.05302","title":"Agency: What it is and why it matters","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"Agency/Rationality/AI","date":"2021-12-04T21:32","url":"https://www.lesswrong.com/posts/qJBkcGW4GitfQ4BBy/agency-what-it-is-and-why-it-matters"},{"x":"10.080613","y":"7.391695","title":"[Question] Misc. questions about EfficientZero","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"EfficientZero/AI","date":"2021-12-04T19:45","url":"https://www.lesswrong.com/posts/rtBpBNgXjwtsLJDbG/misc-questions-about-efficientzero-1"},{"x":"9.373037","y":"8.278129","title":"Shulman and Yudkowsky on AI progress","cluster":"4","author":"['Eliezer Yudkowsky', 'CarlShulman']","source":"alignment forum","tags":"AI/AI Timelines/AI Takeoff","date":"2021-12-03T20:05","url":"https://www.lesswrong.com/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress"},{"x":"7.985507","y":"7.275511","title":"[Linkpost] A General Language Assistant as a Laboratory for Alignment","cluster":"4","author":"['Quintin Pope']","source":"alignment forum","tags":"AI","date":"2021-12-03T19:42","url":"https://www.lesswrong.com/posts/dktT3BiinsBZLw96h/linkpost-a-general-language-assistant-as-a-laboratory-for"},{"x":"9.9785","y":"6.33367","title":"$100/​$50 rewards for good references","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Reward Functions","date":"2021-12-03T16:55","url":"https://www.lesswrong.com/posts/ex2qcux8TQXigGAfv/usd100-usd50-rewards-for-good-references"},{"x":"11.232321","y":"8.114038","title":"Formalizing Policy-Modification Corrigibility","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Corrigibility/AI","date":"2021-12-03T01:31","url":"https://www.lesswrong.com/posts/RAnb2A5vML95rBMyd/formalizing-policy-modification-corrigibility"},{"x":"8.426857","y":"9.083945","title":"Sydney AI Safety Fellowship","cluster":"3","author":"['Chris_Leong']","source":"alignment forum","tags":"AI/Community","date":"2021-12-02T07:34","url":"https://www.lesswrong.com/posts/jotEekAQxmrwcMf9e/sydney-ai-safety-fellowship"},{"x":"10.680622","y":"9.28164","title":"Morality is Scary","cluster":"1","author":"['Wei_Dai']","source":"alignment forum","tags":"Ethics & Morality/AI","date":"2021-12-02T06:35","url":"https://www.lesswrong.com/posts/y5jAuKqkShdjMNZab/morality-is-scary"},{"x":"9.183375","y":"8.801941","title":"AXRP Episode 12 - AI Existential Risk with Paul Christiano","cluster":"4","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Audio/Outer Alignment/Interviews/Existential Risk/AXRP","date":"2021-12-02T02:20","url":"https://www.lesswrong.com/posts/krsjmpDB4kgDq6pdu/axrp-episode-12-ai-existential-risk-with-paul-christiano"},{"x":"7.619712","y":"8.74972","title":"Biology-Inspired AGI Timelines: The Trick That Never Works","cluster":"4","author":"['Eliezer Yudkowsky']","source":"alignment forum","tags":"AI/AI Timelines/Dialogue (format)/Forecasting & Prediction/Technological Forecasting/History","date":"2021-12-01T22:35","url":"https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works"},{"x":"11.60167","y":"10.158233","title":"Hypotheses about Finding Knowledge and One-Shot Causal Entanglements","cluster":"1","author":"['Jemist']","source":"alignment forum","tags":"World Modeling","date":"2021-12-01T17:01","url":"https://www.lesswrong.com/posts/YKBbqMXSKetTQ2HBW/hypotheses-about-finding-knowledge-and-one-shot-causal"},{"x":"11.730853","y":"9.878051","title":"Infra-Bayesian physicalism: proofs part II","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"World Modeling/Infra-Bayesianism","date":"2021-11-30T22:27","url":"https://www.lesswrong.com/posts/CPr8bRGekTyvh7nGC/infra-bayesian-physicalism-proofs-part-ii"},{"x":"11.892809","y":"9.983325","title":"Infra-Bayesian physicalism: proofs part I","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"World Modeling/Infra-Bayesianism","date":"2021-11-30T22:26","url":"https://www.lesswrong.com/posts/cj3PRu8QoFm4BA8oc/infra-bayesian-physicalism-proofs-part-i"},{"x":"11.578113","y":"9.665863","title":"Infra-Bayesian physicalism: a formal theory of naturalized induction","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Infra-Bayesianism/AI","date":"2021-11-30T22:25","url":"https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized"},{"x":"12.339046","y":"8.661505","title":"My take on higher-order game theory","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Game Theory/Domain Theory/World Modeling","date":"2021-11-30T05:56","url":"https://www.lesswrong.com/posts/Cw84NJXAma85AmpnH/my-take-on-higher-order-game-theory"},{"x":"7.6027594","y":"7.3442187","title":"Visible Thoughts Project and Bounty Announcement","cluster":"2","author":"['So8res']","source":"alignment forum","tags":"AI/Bounties (active)/Transparency / Interpretability (ML & AI)/Project Announcement","date":"2021-11-30T00:19","url":"https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement"},{"x":"9.443124","y":"8.378997","title":"Soares, Tallinn, and Yudkowsky discuss AGI cognition","cluster":"4","author":"['So8res', 'Eliezer Yudkowsky', 'jaan']","source":"alignment forum","tags":"Treacherous Turn/AI/General Intelligence/AI Takeoff","date":"2021-11-29T19:26","url":"https://www.lesswrong.com/posts/oKYWbXioKaANATxKY/soares-tallinn-and-yudkowsky-discuss-agi-cognition"},{"x":"8.734027","y":"9.6678","title":"Comments on Allan Dafoe on AI Governance","cluster":"3","author":"['Alex Flint']","source":"alignment forum","tags":"AI Governance/AI","date":"2021-11-29T16:16","url":"https://www.lesswrong.com/posts/rjvLpRzd8mqDyZmcF/comments-on-allan-dafoe-on-ai-governance"},{"x":"8.003339","y":"6.054732","title":"How to measure FLOP/​s for Neural Networks empirically?","cluster":"2","author":"['Marius Hobbhahn']","source":"alignment forum","tags":"AI/AI Risk/AI Capabilities/Scaling Laws","date":"2021-11-29T15:18","url":"https://www.lesswrong.com/posts/jJApGWG95495pYM7C/how-to-measure-flop-s-for-neural-networks-empirically"},{"x":"7.7246537","y":"8.058014","title":"Solve Corrigibility Week","cluster":"3","author":"['Logan Riggs']","source":"alignment forum","tags":"AI/Corrigibility","date":"2021-11-28T17:00","url":"https://www.lesswrong.com/posts/Lv3emECEjkCSHG7L7/solve-corrigibility-week"},{"x":"7.4687214","y":"6.872521","title":"larger language models may disappoint you [or, an eternally unfinished draft]","cluster":"2","author":"['nostalgebraist']","source":"alignment forum","tags":"AI/GPT/Language Models","date":"2021-11-26T23:08","url":"https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally"},{"x":"10.214218","y":"5.8430653","title":"EfficientZero: How It Works","cluster":"0","author":"['1a3orn']","source":"alignment forum","tags":"AI/AI Capabilities/Reinforcement Learning/Machine Learning/EfficientZero","date":"2021-11-26T15:17","url":"https://www.lesswrong.com/posts/mRwJce3npmzbKfxws/efficientzero-how-it-works"},{"x":"7.9941683","y":"8.405803","title":"Christiano, Cotra, and Yudkowsky on AI progress","cluster":"4","author":"['Eliezer Yudkowsky', 'Ajeya Cotra']","source":"alignment forum","tags":"AI Timelines/Inside/Outside View/AI/Forecasting & Prediction/Technological Forecasting/AI Takeoff","date":"2021-11-25T16:45","url":"https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-and-yudkowsky-on-ai-progress"},{"x":"8.216859","y":"7.97056","title":"AI Tracker: monitoring current and near-future risks from superscale models","cluster":"3","author":"['Edouard Harris', 'Jeremie Harris']","source":"alignment forum","tags":"AI/AI Risk/AI Timelines/AI Governance/AI Capabilities","date":"2021-11-23T19:16","url":"https://www.lesswrong.com/posts/eELny7y7JtCs6fQaA/ai-tracker-monitoring-current-and-near-future-risks-from"},{"x":"8.607441","y":"8.442439","title":"AI Safety Needs Great Engineers","cluster":"4","author":"['Andy Jones']","source":"alignment forum","tags":"AI/Community/Careers","date":"2021-11-23T15:40","url":"https://www.lesswrong.com/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers"},{"x":"10.059023","y":"8.539959","title":"Integrating Three Models of (Human) Cognition","cluster":"4","author":"['jbkjr']","source":"alignment forum","tags":"World Modeling/Rationality/AI/Neuroscience/Consciousness/Neocortex/Subagents/Motivations/Intentionality","date":"2021-11-23T01:06","url":"https://www.lesswrong.com/posts/6chtMKXpLcJ26t7n5/integrating-three-models-of-human-cognition"},{"x":"9.117189","y":"7.098833","title":"Potential Alignment mental tool:","cluster":"4","author":"['Donald Hobson']","source":"alignment forum","tags":"AIXI/AI","date":"2021-11-22T20:05","url":"https://www.lesswrong.com/posts/QEHb8tWLztMyvrv6f/potential-alignment-mental-tool-keeping-track-of-the-types"},{"x":"8.5025015","y":"8.215296","title":"Yudkowsky and Christiano discuss \"Takeoff Speeds\"","cluster":"4","author":"['Eliezer Yudkowsky']","source":"alignment forum","tags":"AI Takeoff/AI/AI Timelines/Forecasting & Prediction/General Intelligence/Inside/Outside View","date":"2021-11-22T19:35","url":"https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds"},{"x":"10.368524","y":"9.216131","title":"Morally underdefined situations can be deadly","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning/AI/Moral Uncertainty","date":"2021-11-22T14:48","url":"https://www.lesswrong.com/posts/7q6jQ7y9xQNAkbTtt/morally-underdefined-situations-can-be-deadly"},{"x":"9.457415","y":"9.402572","title":"From language to ethics by automated reasoning","cluster":"4","author":"['Michele Campolo']","source":"alignment forum","tags":"AI","date":"2021-11-21T15:16","url":"https://www.lesswrong.com/posts/uyvnjaRaKdGXoKrv7/from-language-to-ethics-by-automated-reasoning"},{"x":"11.08841","y":"8.211429","title":"Corrigibility Can Be VNM-Incoherent","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Corrigibility/Instrumental Convergence/Open Problems","date":"2021-11-20T00:30","url":"https://www.lesswrong.com/posts/WCX3EwnWAx7eyucqH/corrigibility-can-be-vnm-incoherent"},{"x":"8.5228195","y":"7.780153","title":"More detailed proposal for measuring alignment of current models","cluster":"4","author":"['Beth Barnes']","source":"alignment forum","tags":"AI","date":"2021-11-20T00:03","url":"https://www.lesswrong.com/posts/GbAymLbJdGbqTumCN/more-detailed-proposal-for-measuring-alignment-of-current"},{"x":"11.006458","y":"8.498234","title":"Goodhart: Endgame","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"AI","date":"2021-11-19T01:26","url":"https://www.lesswrong.com/posts/dmp9PZjpSSX5NeXHM/goodhart-endgame"},{"x":"8.048574","y":"8.966724","title":"How To Get Into Independent Research On Alignment/​Agency","cluster":"3","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Careers","date":"2021-11-19T00:00","url":"https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency"},{"x":"8.352387","y":"8.163486","title":"Ngo and Yudkowsky on AI capability gains","cluster":"4","author":"['Eliezer Yudkowsky', 'Richard_Ngo']","source":"alignment forum","tags":"AI/Recursive Self-Improvement/AI Takeoff/Utility Functions/Optimization/Effective Altruism/Modest Epistemology/AI Governance","date":"2021-11-18T22:19","url":"https://www.lesswrong.com/posts/hwxj4gieR7FWNwYfa/ngo-and-yudkowsky-on-ai-capability-gains-1"},{"x":"11.26708","y":"7.4487076","title":"Satisficers Tend To Seek Power: Instrumental Convergence Via Retargetability","cluster":"0","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence/Satisficer","date":"2021-11-18T01:54","url":"https://www.lesswrong.com/posts/nZY8Np759HYFawdjH/satisficers-tend-to-seek-power-instrumental-convergence-via"},{"x":"8.008426","y":"8.8825","title":"Applications for AI Safety Camp 2022 Now Open!","cluster":"3","author":"['adamShimi']","source":"alignment forum","tags":"Community/AI/AI Risk/AI Safety Camp","date":"2021-11-17T21:42","url":"https://www.lesswrong.com/posts/QeetPm8yvFf7mAGj9/applications-for-ai-safety-camp-2022-now-open"},{"x":"8.479843","y":"7.815904","title":"A positive case for how we might succeed at prosaic AI alignment","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI Success Models/AI/Outer Alignment","date":"2021-11-16T01:49","url":"https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai"},{"x":"11.85816","y":"7.8826885","title":"Quantilizer ≡ Optimizer with a Bounded Amount of Output","cluster":"1","author":"['itaibn0']","source":"alignment forum","tags":"Quantilization/AI","date":"2021-11-16T01:03","url":"https://www.lesswrong.com/posts/ZjDh3BmbDrWJRckEb/quantilizer-optimizer-with-a-bounded-amount-of-output-1"},{"x":"7.820349","y":"7.6078396","title":"Ngo and Yudkowsky on alignment difficulty","cluster":"2","author":"['Eliezer Yudkowsky', 'Richard_Ngo']","source":"alignment forum","tags":"AI/AI Risk","date":"2021-11-15T20:31","url":"https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty"},{"x":"8.401061","y":"7.1135426","title":"My understanding of the alignment problem","cluster":"2","author":"['danieldewey']","source":"alignment forum","tags":"AI","date":"2021-11-15T18:13","url":"https://www.lesswrong.com/posts/WckCfXpfrb9Bms8aA/my-understanding-of-the-alignment-problem"},{"x":"9.262851","y":"9.258183","title":"Attempted Gears Analysis of AGI Intervention Discussion With Eliezer","cluster":"3","author":"['Zvi']","source":"alignment forum","tags":"AI/Gears-Level","date":"2021-11-15T03:50","url":"https://www.lesswrong.com/posts/xHnuX42WNZ9hq53bz/attempted-gears-analysis-of-agi-intervention-discussion-with-1"},{"x":"7.889798","y":"8.971884","title":"What would we do if alignment were futile?","cluster":"3","author":"['Grant Demaree']","source":"alignment forum","tags":"AI/AI Risk","date":"2021-11-14T08:09","url":"https://www.lesswrong.com/posts/Xv77XjuZEkjRsvkJp/what-would-we-do-if-alignment-were-futile"},{"x":"8.845377","y":"9.885945","title":"Comments on Carlsmith’s \"Is power-seeking AI an existential risk?\"","cluster":"3","author":"['So8res']","source":"alignment forum","tags":"AI/AI Timelines/Nanotechnology","date":"2021-11-13T04:29","url":"https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential"},{"x":"8.446019","y":"6.67173","title":"Why I’m excited about Redwood Research’s current project","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Redwood Research","date":"2021-11-12T19:26","url":"https://www.lesswrong.com/posts/pXLqpguHJzxSjDdx7/why-i-m-excited-about-redwood-research-s-current-project"},{"x":"8.8793","y":"8.507812","title":"Discussion with Eliezer Yudkowsky on AGI interventions","cluster":"4","author":"['Rob Bensinger', 'Eliezer Yudkowsky']","source":"alignment forum","tags":"AI/AI Risk/Transcripts/World Optimization/Existential Risk","date":"2021-11-11T03:01","url":"https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions"},{"x":"8.082368","y":"6.8981676","title":"[Question] What exactly is GPT-3′s base objective?","cluster":"2","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/GPT/Inner Alignment","date":"2021-11-10T00:57","url":"https://www.lesswrong.com/posts/Nq58w4SiZMjHdAPaX/what-exactly-is-gpt-3-s-base-objective"},{"x":"8.696202","y":"6.3928647","title":"Possible research directions to improve the mechanistic explanation of neural networks","cluster":"2","author":"['delton137']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)","date":"2021-11-09T02:36","url":"https://www.lesswrong.com/posts/HiutLvY2x7zrsTQkx/possible-research-directions-to-improve-the-mechanistic"},{"x":"8.614118","y":"7.299401","title":"How do we become confident in the safety of a machine learning system?","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2021-11-08T22:49","url":"https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine"},{"x":"10.336125","y":"9.00028","title":"What are red flags for Neural Network suffering?","cluster":"1","author":"['Marius Hobbhahn']","source":"alignment forum","tags":"AI/World Modeling/AI Risk/Neuroscience/Suffering","date":"2021-11-08T12:51","url":"https://www.lesswrong.com/posts/Bpw2HXjMa3GaouDnC/what-are-red-flags-for-neural-network-suffering"},{"x":"11.4175","y":"10.511964","title":"Chu are you?","cluster":"1","author":"['Adele Lopez']","source":"alignment forum","tags":"World Modeling/Logic & Mathematics ","date":"2021-11-06T17:39","url":"https://www.lesswrong.com/posts/89EvBkc4nkbEctzR3/chu-are-you"},{"x":"8.88893","y":"6.6706023","title":"Comments on OpenPhil’s Interpretability RFP","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2021-11-05T22:36","url":"https://www.lesswrong.com/posts/oWN9fgYnFYJEWdAs9/comments-on-openphil-s-interpretability-rfp"},{"x":"9.689948","y":"9.13155","title":"Drug addicts and deceptively aligned agents—a comparative analysis","cluster":"4","author":"['Jan']","source":"alignment forum","tags":"AI Risk/AI","date":"2021-11-05T21:42","url":"https://www.lesswrong.com/posts/pFXEG9C5m2X5h2yiq/drug-addicts-and-deceptively-aligned-agents-a-comparative"},{"x":"8.4883995","y":"9.738235","title":"Modeling the impact of safety agendas","cluster":"3","author":"['Ben Cottier']","source":"alignment forum","tags":"AI","date":"2021-11-05T19:46","url":"https://www.lesswrong.com/posts/t7f6gF2kpafCMw6rv/modeling-the-impact-of-safety-agendas"},{"x":"8.262456","y":"6.9222918","title":"Apply to the ML for Alignment Bootcamp (MLAB) in Berkeley [Jan 3 - Jan 22]","cluster":"2","author":"['habryka', 'Buck']","source":"alignment forum","tags":"Practical/Community/AI","date":"2021-11-03T18:22","url":"https://www.lesswrong.com/posts/YgpDYjTx7DCEgziG5/apply-to-the-ml-for-alignment-bootcamp-mlab-in-berkeley-jan"},{"x":"10.865241","y":"9.0089855","title":"Models Modeling Models","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Goodhart's Law/AI","date":"2021-11-02T07:08","url":"https://www.lesswrong.com/posts/nA3n2vfCy3ffnjapw/models-modeling-models"},{"x":"10.123447","y":"5.5915294","title":"EfficientZero: human ALE sample-efficiency w/​MuZero+self-supervised","cluster":"2","author":"['gwern']","source":"alignment forum","tags":"AI Capabilities/AI/EfficientZero/Reinforcement Learning","date":"2021-11-02T02:32","url":"https://www.lesswrong.com/posts/jYNT3Qihn2aAYaaPb/efficientzero-human-ale-sample-efficiency-w-muzero-self"},{"x":"9.218736","y":"8.424852","title":"Stuart Russell and Melanie Mitchell on Munk Debates","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2021-10-29T19:13","url":"https://www.lesswrong.com/posts/GDnFsyfKedevKHAuJ/stuart-russell-and-melanie-mitchell-on-munk-debates"},{"x":"8.196142","y":"7.654093","title":"A very crude deception eval is already passed","cluster":"1","author":"['Beth Barnes']","source":"alignment forum","tags":"AI/Treacherous Turn","date":"2021-10-29T17:57","url":"https://www.lesswrong.com/posts/22GrdspteQc8EonMn/a-very-crude-deception-eval-is-already-passed"},{"x":"8.68748","y":"7.792531","title":"Truthful and honest AI","cluster":"4","author":"['abergal', 'Nick_Beckstead', 'Owain_Evans']","source":"alignment forum","tags":"AI/Language Models/AI Risk","date":"2021-10-29T07:28","url":"https://www.lesswrong.com/posts/sdxZdGFtAwHGFGKhg/truthful-and-honest-ai"},{"x":"8.729191","y":"7.311662","title":"Interpretability","cluster":"4","author":"['abergal', 'Nick_Beckstead']","source":"alignment forum","tags":"AI","date":"2021-10-29T07:28","url":"https://www.lesswrong.com/posts/CzZ6Fch4JSpwCpu6C/interpretability"},{"x":"10.797999","y":"8.154799","title":"Techniques for enhancing human feedback","cluster":"1","author":"['abergal', 'Ajeya Cotra', 'Nick_Beckstead']","source":"alignment forum","tags":"AI","date":"2021-10-29T07:27","url":"https://www.lesswrong.com/posts/ybThg9nA7u6f8qfZZ/techniques-for-enhancing-human-feedback"},{"x":"8.636226","y":"7.810557","title":"Measuring and forecasting risks","cluster":"3","author":"['abergal', 'Nick_Beckstead', 'jsteinhardt']","source":"alignment forum","tags":"AI","date":"2021-10-29T07:27","url":"https://www.lesswrong.com/posts/7DhwRLoKm4nMrFFsH/measuring-and-forecasting-risks"},{"x":"9.184069","y":"7.0989313","title":"Request for proposals for projects in AI alignment that work with deep learning systems","cluster":"4","author":"['abergal', 'Nick_Beckstead']","source":"alignment forum","tags":"AI/Grants & Fundraising Opportunities","date":"2021-10-29T07:26","url":"https://www.lesswrong.com/posts/H5iePjNKaaYQyZpgR/request-for-proposals-for-projects-in-ai-alignment-that-work"},{"x":"7.530867","y":"6.718087","title":"Forecasting progress in language models","cluster":"2","author":"['Matthew Barnett', 'Metaculus']","source":"alignment forum","tags":"Language Models/Forecasts (Specific Predictions)/AI","date":"2021-10-28T20:40","url":"https://www.lesswrong.com/posts/tepqESMuRmyhtmDS7/forecasting-progress-in-language-models"},{"x":"11.222776","y":"9.408421","title":"Selfishness, preference falsification, and AI alignment","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"World Optimization/AI/Human Values","date":"2021-10-28T00:16","url":"https://www.lesswrong.com/posts/ZddY8BZbvoXHEvDHf/selfishness-preference-falsification-and-ai-alignment"},{"x":"7.693658","y":"9.05035","title":"Phil Trammell on Economic Growth Under Transformative AI","cluster":"3","author":"['Michaël Trazzi']","source":"alignment forum","tags":"Economics/World Modeling","date":"2021-10-24T18:11","url":"https://www.lesswrong.com/posts/BNoHokwCiPGmFHnp8/phil-trammell-on-economic-growth-under-transformative-ai"},{"x":"10.693856","y":"8.612225","title":"P₂B: Plan to P₂B Better","cluster":"4","author":"['Ramana Kumar', 'Daniel Kokotajlo']","source":"alignment forum","tags":"Goal-Directedness/Instrumental Convergence/Practical/World Modeling/AI","date":"2021-10-24T15:21","url":"https://www.lesswrong.com/posts/CAwwFpbteYBQw2Gkp/p-b-plan-to-p-b-better"},{"x":"8.95564","y":"6.6040597","title":"Towards Deconfusing Gradient Hacking","cluster":"2","author":"['leogao']","source":"alignment forum","tags":"AI/Gradient Hacking/Inner Alignment/Mesa-Optimization","date":"2021-10-24T00:43","url":"https://www.lesswrong.com/posts/u3fP8vjGsDCT7X54H/towards-deconfusing-gradient-hacking"},{"x":"9.59035","y":"9.1234","title":"General alignment plus human values, or alignment via human values?","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-10-22T10:11","url":"https://www.lesswrong.com/posts/3e6pmovj6EJ729M2i/general-alignment-plus-human-values-or-alignment-via-human"},{"x":"10.363031","y":"9.42674","title":"Epistemic Strategies of Safety-Capabilities Tradeoffs","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Tradeoffs/AI Risk/AI Capabilities/AI","date":"2021-10-22T08:22","url":"https://www.lesswrong.com/posts/hWag6E7XPCbdfaoKZ/epistemic-strategies-of-safety-capabilities-tradeoffs"},{"x":"8.851891","y":"6.809606","title":"Emergent modularity and safety","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2021-10-21T01:54","url":"https://www.lesswrong.com/posts/zvEbeZ6opjPJiQnFE/emergent-modularity-and-safety"},{"x":"8.765772","y":"8.802321","title":"AGI Safety Fundamentals curriculum and application","cluster":"3","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/Community","date":"2021-10-20T21:44","url":"https://www.lesswrong.com/posts/Zmwkz2BMvuFFR8bi3/agi-safety-fundamentals-curriculum-and-application"},{"x":"8.760875","y":"9.156475","title":"Beyond the human training distribution: would the AI CEO create almost-illegal teddies?","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Complexity of Value","date":"2021-10-18T21:10","url":"https://www.lesswrong.com/posts/tHChCJB9piCTD7HEx/beyond-the-human-training-distribution-would-the-ai-ceo"},{"x":"8.846449","y":"7.232988","title":"On The Risks of Emergent Behavior in Foundation Models","cluster":"4","author":"['jsteinhardt']","source":"alignment forum","tags":"AI","date":"2021-10-18T20:00","url":"https://www.lesswrong.com/posts/QmdrkuArFHphqANRE/on-the-risks-of-emergent-behavior-in-foundation-models"},{"x":"8.789656","y":"9.150262","title":"Truthful AI: Developing and governing AI that does not lie","cluster":"3","author":"['Owain_Evans', 'owencb', 'Lanrian']","source":"alignment forum","tags":"AI/Truth, Semantics, & Meaning/AI Governance/GPT/Honesty/AI Risk/Epistemology","date":"2021-10-18T18:37","url":"https://www.lesswrong.com/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie"},{"x":"7.9599805","y":"5.177554","title":"[MLSN #1]: ICLR Safety Paper Roundup","cluster":"2","author":"['Daniel Hendrycks']","source":"alignment forum","tags":"AI/Machine Learning","date":"2021-10-18T15:19","url":"https://www.lesswrong.com/posts/8Gv5zSCnGeLxK5FAF/mlsn-1-iclr-safety-paper-roundup"},{"x":"11.034847","y":"9.559972","title":"Epistemic Strategies of Selection Theorems","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"World Modeling/Epistemology/Epistemic Review/Selection Effects/AI Risk/Intellectual Progress (Individual-Level)/AI","date":"2021-10-18T08:57","url":"https://www.lesswrong.com/posts/LWmmfTvptiJp7wvFg/epistemic-strategies-of-selection-theorems"},{"x":"10.302987","y":"7.660265","title":"Optimization Concepts in the Game of Life","cluster":"4","author":"['Vika', 'Ramana Kumar']","source":"alignment forum","tags":"Goal-Directedness/Optimization/Embedded Agency","date":"2021-10-16T20:51","url":"https://www.lesswrong.com/posts/mL8KdftNGBScmBcBg/optimization-concepts-in-the-game-of-life"},{"x":"8.353516","y":"8.91538","title":"[Question] Memetic hazards of AGI architecture posts","cluster":"3","author":"['Ozyrus']","source":"alignment forum","tags":"AI","date":"2021-10-16T16:10","url":"https://www.lesswrong.com/posts/XTGceK7xE8rxrRLcr/memetic-hazards-of-agi-architecture-posts-1"},{"x":"7.458696","y":"6.9437246","title":"NLP Position Paper: When Combatting Hype, Proceed with Caution","cluster":"2","author":"['Sam Bowman']","source":"alignment forum","tags":"Machine Learning/Language Models/AI","date":"2021-10-15T20:57","url":"https://www.lesswrong.com/posts/RLHkSBQ7zmTzAjsio/nlp-position-paper-when-combatting-hype-proceed-with-caution"},{"x":"11.236984","y":"10.241098","title":"Classical symbol grounding and causal graphs","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Symbol Grounding","date":"2021-10-14T18:04","url":"https://www.lesswrong.com/posts/SnKfFscgC8Nj5ddi3/classical-symbol-grounding-and-causal-graphs"},{"x":"8.4705","y":"6.7575855","title":"[Proposal] Method of locating useful subnets in large models","cluster":"2","author":"['Quintin Pope']","source":"alignment forum","tags":"Outer Alignment/Transparency / Interpretability (ML & AI)/AI/Reinforcement Learning/Mesa-Optimization/Machine Learning","date":"2021-10-13T20:52","url":"https://www.lesswrong.com/posts/Ji6hQbwH7tK7mejhk/proposal-method-of-locating-useful-subnets-in-large-models"},{"x":"9.314477","y":"7.394545","title":"Modeling Risks From Learned Optimization","cluster":"4","author":"['Ben Cottier']","source":"alignment forum","tags":"AI/Mesa-Optimization","date":"2021-10-12T20:54","url":"https://www.lesswrong.com/posts/T9oFjteStcE2ijCJi/modeling-risks-from-learned-optimization"},{"x":"7.4841743","y":"6.728227","title":"NVIDIA and Microsoft releases 530B parameter transformer model, Megatron-Turing NLG","cluster":"2","author":"['Ozyrus']","source":"alignment forum","tags":"AI/Scaling Laws/Machine Learning/GPT/Language Models","date":"2021-10-11T15:28","url":"https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer"},{"x":"8.165854","y":"8.170985","title":"On Solving Problems Before They Appear: The Weird Epistemologies of Alignment","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"Epistemology/Intellectual Progress (Society-Level)/Practice & Philosophy of Science/AI Risk/AI","date":"2021-10-11T08:20","url":"https://www.lesswrong.com/posts/FQqcejhNWGG8vHDch/on-solving-problems-before-they-appear-the-weird"},{"x":"10.7866535","y":"9.113267","title":"Intelligence or Evolution?","cluster":"1","author":"['Ramana Kumar']","source":"alignment forum","tags":"AI","date":"2021-10-09T17:14","url":"https://www.lesswrong.com/posts/TATWqHvxKEpL34yKz/intelligence-or-evolution"},{"x":"8.79098","y":"9.235043","title":"Safety-capabilities tradeoff dials are inevitable in AGI","cluster":"3","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/World Optimization","date":"2021-10-07T19:03","url":"https://www.lesswrong.com/posts/tmyTb4bQQi7C47sde/safety-capabilities-tradeoff-dials-are-inevitable-in-agi"},{"x":"7.8545494","y":"7.462112","title":"Automated Fact Checking: A Look at the Field","cluster":"1","author":"['Hoagy']","source":"alignment forum","tags":"Machine Learning/AI","date":"2021-10-06T23:52","url":"https://www.lesswrong.com/posts/CWD8FxA3yJPmZE9o3/automated-fact-checking-a-look-at-the-field"},{"x":"11.251555","y":"8.747447","title":"Preferences from (real and hypothetical) psychology papers","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Machine Learning","date":"2021-10-06T09:06","url":"https://www.lesswrong.com/posts/FsxPNRJ5NQkrSKyDx/preferences-from-real-and-hypothetical-psychology-papers"},{"x":"8.087482","y":"7.95239","title":"We’re Redwood Research, we do applied alignment research, AMA","cluster":"4","author":"['Nate Thomas']","source":"alignment forum","tags":"Q&A (format)/AI/AMA/Redwood Research","date":"2021-10-06T05:51","url":"https://www.lesswrong.com/posts/qHvHvBR8L6oycnMXe/we-re-redwood-research-we-do-applied-alignment-research-ama"},{"x":"10.139119","y":"6.04707","title":"Force neural nets to use models, then detect these","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2021-10-05T11:31","url":"https://www.lesswrong.com/posts/pdJqEzbQrucTEF6DW/force-neural-nets-to-use-models-then-detect-these"},{"x":"10.675254","y":"9.563274","title":"The Simulation Hypothesis Undercuts the SIA/​Great Filter Doomsday Argument","cluster":"1","author":"['Mark Xu', 'CarlShulman']","source":"alignment forum","tags":"Anthropics/Simulation Hypothesis/World Modeling","date":"2021-10-01T22:23","url":"https://www.lesswrong.com/posts/HF2vpnmgqmHyLGRrA/the-simulation-hypothesis-undercuts-the-sia-great-filter"},{"x":"8.684836","y":"6.2648478","title":"Meta learning to gradient hack","cluster":"2","author":"['Quintin Pope']","source":"alignment forum","tags":"AI/Mesa-Optimization/Inner Alignment/Gradient Hacking","date":"2021-10-01T19:25","url":"https://www.lesswrong.com/posts/jh6dkqN2wd7fCRfB5/meta-learning-to-gradient-hack"},{"x":"11.18148","y":"9.301994","title":"What Selection Theorems Do We Expect/​Want?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"World Modeling","date":"2021-10-01T16:03","url":"https://www.lesswrong.com/posts/RuDD3aQWLDSb4eTXP/what-selection-theorems-do-we-expect-want"},{"x":"11.52356","y":"9.0165615","title":"Some Existing Selection Theorems","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"AI/World Modeling","date":"2021-09-30T16:13","url":"https://www.lesswrong.com/posts/N2NebPD78ioyWHhNm/some-existing-selection-theorems"},{"x":"8.594377","y":"8.674875","title":"Takeoff Speeds and Discontinuities","cluster":"3","author":"['Sammy Martin', 'Daniel_Eth']","source":"alignment forum","tags":"AI Takeoff/AI/World Modeling","date":"2021-09-30T13:50","url":"https://www.lesswrong.com/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities"},{"x":"8.958805","y":"8.61847","title":"My take on Vanessa Kosoy’s take on AGI safety","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Infra-Bayesianism/Reinforcement Learning","date":"2021-09-30T12:23","url":"https://www.lesswrong.com/posts/SzrmsbkqydpZyPuEh/my-take-on-vanessa-kosoy-s-take-on-agi-safety"},{"x":"10.350993","y":"6.415337","title":"AI learns betrayal and how to avoid it","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Treacherous Turn/Research Agendas","date":"2021-09-30T09:39","url":"https://www.lesswrong.com/posts/oeCXS2ZCn4rPyq7LQ/ai-learns-betrayal-and-how-to-avoid-it"},{"x":"10.12672","y":"7.561695","title":"A brief review of the reasons multi-objective RL could be important in AI Safety Research","cluster":"4","author":"['Ben Smith']","source":"alignment forum","tags":"AI/AI Safety Camp/Psychology/Reinforcement Learning/AI Risk","date":"2021-09-29T17:09","url":"https://www.lesswrong.com/posts/i5dLfi6m6FCexReK9/a-brief-review-of-the-reasons-multi-objective-rl-could-be"},{"x":"8.681315","y":"5.0719757","title":"Unsolved ML Safety Problems","cluster":"2","author":"['jsteinhardt']","source":"alignment forum","tags":"Machine Learning/AI","date":"2021-09-29T16:00","url":"https://www.lesswrong.com/posts/AwMb7C72etphiRvah/unsolved-ml-safety-problems"},{"x":"9.188931","y":"7.711645","title":"Brain-inspired AGI and the \"lifetime anchor\"","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI Timelines/Computing Overhang/Neuromorphic AI/AI","date":"2021-09-29T13:09","url":"https://www.lesswrong.com/posts/W6wBmQheDiFmfJqZy/brain-inspired-agi-and-the-lifetime-anchor"},{"x":"8.3131895","y":"8.361257","title":"[Question] Collection of arguments to expect (outer and inner) alignment failure?","cluster":"4","author":"['Sam Clarke']","source":"alignment forum","tags":"Inner Alignment/Outer Alignment/AI","date":"2021-09-28T16:55","url":"https://www.lesswrong.com/posts/m5or8yzrw9GLavz9b/collection-of-arguments-to-expect-outer-and-inner-alignment"},{"x":"10.579687","y":"8.830402","title":"Selection Theorems: A Program For Understanding Agents","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"Inner Alignment/Outer Alignment/World Modeling/Rationality","date":"2021-09-28T05:03","url":"https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents"},{"x":"8.161705","y":"6.2829685","title":"AI takeoff story: a continuation of progress by other means","cluster":"2","author":"['Edouard Harris']","source":"alignment forum","tags":"Fiction/AI Takeoff/AI/AI Risk/Existential Risk","date":"2021-09-27T15:55","url":"https://www.lesswrong.com/posts/Fq8ybxtcFvKEsWmF8/ai-takeoff-story-a-continuation-of-progress-by-other-means"},{"x":"9.543987","y":"9.251054","title":"AXRP Episode 11 - Attainable Utility and Power with Alex Turner","cluster":"4","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Impact Measures/Audio/Interviews/Instrumental Convergence/AXRP","date":"2021-09-25T21:10","url":"https://www.lesswrong.com/posts/vcrXS5DmvBuJaKucp/axrp-episode-11-attainable-utility-and-power-with-alex"},{"x":"7.514237","y":"6.929861","title":"Cognitive Biases in Large Language Models","cluster":"2","author":"['Jan']","source":"alignment forum","tags":"Language Models/AI","date":"2021-09-25T20:59","url":"https://www.lesswrong.com/posts/fFF3G4W8FbXigS4gr/cognitive-biases-in-large-language-models"},{"x":"8.0391","y":"6.2335443","title":"Pathways: Google’s AGI","cluster":"2","author":"['Lê Nguyên Hoang']","source":"alignment forum","tags":"AI","date":"2021-09-25T07:02","url":"https://www.lesswrong.com/posts/bEKW5gBawZirJXREb/pathways-google-s-agi"},{"x":"7.5555267","y":"7.242167","title":"Redwood Research’s current project","cluster":"2","author":"['Buck']","source":"alignment forum","tags":"Organization Updates/AI/Redwood Research","date":"2021-09-21T23:30","url":"https://www.lesswrong.com/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project"},{"x":"11.230742","y":"10.073034","title":"David Wolpert on Knowledge","cluster":"1","author":"['Alex Flint']","source":"alignment forum","tags":"Epistemology/World Modeling","date":"2021-09-21T01:54","url":"https://www.lesswrong.com/posts/hPPGuiXf3zhKqgCMb/david-wolpert-on-knowledge"},{"x":"8.64715","y":"9.570161","title":"Announcing the Vitalik Buterin Fellowships in AI Existential Safety!","cluster":"3","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Project Announcement/Existential Risk","date":"2021-09-21T00:33","url":"https://www.lesswrong.com/posts/hSdgugekxgdyacXTu/announcing-the-vitalik-buterin-fellowships-in-ai-existential"},{"x":"10.8397255","y":"7.5894613","title":"AI, learn to be conservative, then learn to be less so: reducing side-effects, learning preserved features, and going beyond conservatism","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Research Agendas/AI","date":"2021-09-20T11:56","url":"https://www.lesswrong.com/posts/y2XyxomuEpMaRYDQw/ai-learn-to-be-conservative-then-learn-to-be-less-so"},{"x":"8.380883","y":"7.0165358","title":"[Book Review] \"The Alignment Problem\" by Brian Christian","cluster":"2","author":"['lsusr']","source":"alignment forum","tags":"Book Reviews/AI/AI Risk","date":"2021-09-20T06:36","url":"https://www.lesswrong.com/posts/ZYDkHWjShKazTywbg/book-review-the-alignment-problem-by-brian-christian"},{"x":"10.931566","y":"10.384568","title":"Testing The Natural Abstraction Hypothesis: Project Update","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/Research Agendas","date":"2021-09-20T03:44","url":"https://www.lesswrong.com/posts/dNzhdiFE398KcGDc9/testing-the-natural-abstraction-hypothesis-project-update"},{"x":"8.998645","y":"7.6752505","title":"The theory-practice gap","cluster":"4","author":"['Buck']","source":"alignment forum","tags":"AI","date":"2021-09-17T22:51","url":"https://www.lesswrong.com/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap"},{"x":"8.647137","y":"9.754755","title":"Investigating AI Takeover Scenarios","cluster":"3","author":"['Sammy Martin']","source":"alignment forum","tags":"Threat Models/AI/AI Risk","date":"2021-09-17T18:47","url":"https://www.lesswrong.com/posts/zkF9PNSyDKusoyLkP/investigating-ai-takeover-scenarios"},{"x":"11.06193","y":"8.248402","title":"Goodhart Ethology","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"AI/Goodhart's Law/Value Learning","date":"2021-09-17T17:31","url":"https://www.lesswrong.com/posts/z2BPxcFfhKho89D8L/goodhart-ethology"},{"x":"10.164299","y":"6.674807","title":"Immobile AI makes a move: anti-wireheading, ontology change, and model splintering","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Research Agendas/AI","date":"2021-09-17T15:24","url":"https://www.lesswrong.com/posts/ZqSESJYcA8m2Hh7Qm/immobile-ai-makes-a-move-anti-wireheading-ontology-change"},{"x":"10.29946","y":"6.2676215","title":"Jitters No Evidence of Stupidity in RL","cluster":"0","author":"['1a3orn']","source":"alignment forum","tags":"Reinforcement Learning/AI/World Modeling","date":"2021-09-16T22:43","url":"https://www.lesswrong.com/posts/Fx8gCJu5zuLdZezTN/jitters-no-evidence-of-stupidity-in-rl"},{"x":"11.5671215","y":"8.777211","title":"Economic AI Safety","cluster":"1","author":"['jsteinhardt']","source":"alignment forum","tags":"AI/Privacy","date":"2021-09-16T20:50","url":"https://www.lesswrong.com/posts/RLcEQtoc5EqTxdan8/economic-ai-safety"},{"x":"10.000982","y":"9.709695","title":"Extortion beats brinksmanship, but the audience matters","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Rationality/World Optimization/Blackmail / Extortion/Game Theory","date":"2020-11-16T21:13","url":"https://www.lesswrong.com/posts/ofx82Y9a4zcETfT6Q/extortion-beats-brinksmanship-but-the-audience-matters"},{"x":"8.431052","y":"7.307777","title":"A guide to Iterated Amplification & Debate","cluster":"4","author":"['Rafael Harth']","source":"alignment forum","tags":"Iterated Amplification /Debate (AI safety technique)/Factored Cognition/Humans Consulting HCH/AI Risk/AI","date":"2020-11-15T17:14","url":"https://www.lesswrong.com/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate"},{"x":"11.108503","y":"10.058352","title":"Early Thoughts on Ontology/​Grounding Problems","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Rationality/Symbol Grounding","date":"2020-11-14T23:19","url":"https://www.lesswrong.com/posts/an29DQgYaKbyQprns/early-thoughts-on-ontology-grounding-problems"},{"x":"11.6797285","y":"9.837635","title":"A Self-Embedded Probabilistic Model","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"World Modeling/AI","date":"2020-11-13T20:36","url":"https://www.lesswrong.com/posts/SN8wFZsZiyBygc27k/a-self-embedded-probabilistic-model"},{"x":"8.341528","y":"9.536142","title":"Misalignment and misuse: whose values are manifest?","cluster":"3","author":"['KatjaGrace']","source":"alignment forum","tags":"AI","date":"2020-11-13T10:10","url":"https://www.lesswrong.com/posts/AomSXpFcqmgeDyWWo/misalignment-and-misuse-whose-values-are-manifest"},{"x":"9.687482","y":"9.048149","title":"Communication Prior as Alignment Strategy","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Value Learning","date":"2020-11-12T22:06","url":"https://www.lesswrong.com/posts/zAvhvGa6ToieNGuy2/communication-prior-as-alignment-strategy"},{"x":"11.743528","y":"10.413712","title":"A Correspondence Theorem in the Maximum Entropy Framework","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"World Modeling","date":"2020-11-11T22:46","url":"https://www.lesswrong.com/posts/XMGWdfTC7XjgTz3X7/a-correspondence-theorem-in-the-maximum-entropy-framework"},{"x":"8.239514","y":"7.014589","title":"Learning Normativity: A Research Agenda","cluster":"2","author":"['abramdemski']","source":"alignment forum","tags":"AI/Rationality","date":"2020-11-11T21:59","url":"https://www.lesswrong.com/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda"},{"x":"11.262352","y":"10.420362","title":"Time in Cartesian Frames","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Embedded Agency/Decision Theory/AI","date":"2020-11-11T20:25","url":"https://www.lesswrong.com/posts/JTzLjARpevuNpGPZm/time-in-cartesian-frames"},{"x":"11.487245","y":"10.312342","title":"Eight Definitions of Observability","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Embedded Agency/Subagents","date":"2020-11-10T23:37","url":"https://www.lesswrong.com/posts/5R9dRqTREZriN9iL7/eight-definitions-of-observability"},{"x":"9.73917","y":"7.097994","title":"Clarifying inner alignment terminology","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2020-11-09T20:40","url":"https://www.lesswrong.com/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology"},{"x":"11.280266","y":"10.1633005","title":"Committing, Assuming, Externalizing, and Internalizing","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Embedded Agency/Subagents","date":"2020-11-09T16:59","url":"https://www.lesswrong.com/posts/5HMqSGQ9ad9r9Hibw/committing-assuming-externalizing-and-internalizing"},{"x":"9.861849","y":"9.107285","title":"Why You Should Care About Goal-Directedness","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"AI","date":"2020-11-09T12:48","url":"https://www.lesswrong.com/posts/q9BmNh35xgXPRgJhm/why-you-should-care-about-goal-directedness"},{"x":"7.825919","y":"6.061529","title":"the scaling \"inconsistency\": openAI’s new insight","cluster":"2","author":"['nostalgebraist']","source":"alignment forum","tags":"AI/GPT/OpenAI/Machine Learning","date":"2020-11-07T07:40","url":"https://www.lesswrong.com/posts/diutNaWF669WgEt3v/the-scaling-inconsistency-openai-s-new-insight"},{"x":"9.672865","y":"7.1214404","title":"Does SGD Produce Deceptive Alignment?","cluster":"4","author":"['Mark Xu']","source":"alignment forum","tags":"AI/Inner Alignment/Machine Learning/Mesa-Optimization","date":"2020-11-06T23:48","url":"https://www.lesswrong.com/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment"},{"x":"10.993893","y":"10.251959","title":"Additive and Multiplicative Subagents","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Embedded Agency/Subagents","date":"2020-11-06T14:26","url":"https://www.lesswrong.com/posts/hxbjEgjNTSbdXqDFE/additive-and-multiplicative-subagents"},{"x":"11.188626","y":"10.495919","title":"Sub-Sums and Sub-Tensors","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Embedded Agency/AI","date":"2020-11-05T18:06","url":"https://www.lesswrong.com/posts/LAHXvi4qwXogmdTHd/sub-sums-and-sub-tensors-1"},{"x":"8.81937","y":"6.526578","title":"Defining capability and alignment in gradient descent","cluster":"2","author":"['Edouard Harris']","source":"alignment forum","tags":"Inner Alignment/AI/Mesa-Optimization","date":"2020-11-05T14:36","url":"https://www.lesswrong.com/posts/Xg2YycEfCnLYrCcjy/defining-capability-and-alignment-in-gradient-descent"},{"x":"11.252916","y":"10.544532","title":"Multiplicative Operations on Cartesian Frames","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Embedded Agency/Category Theory","date":"2020-11-03T19:27","url":"https://www.lesswrong.com/posts/srTD8DgTCR27udzoe/multiplicative-operations-on-cartesian-frames"},{"x":"11.120397","y":"10.503232","title":"Subagents of Cartesian Frames","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Subagents/Embedded Agency","date":"2020-11-02T22:02","url":"https://www.lesswrong.com/posts/nwrkwTd6uKBesYYfx/subagents-of-cartesian-frames"},{"x":"8.979287","y":"9.759322","title":"Confucianism in AI Alignment","cluster":"3","author":"['johnswentworth']","source":"alignment forum","tags":"AI","date":"2020-11-02T21:16","url":"https://www.lesswrong.com/posts/3aDeaJzxinoGNWNpC/confucianism-in-ai-alignment"},{"x":"10.61466","y":"7.858496","title":"\"Inner Alignment Failures\" Which Are Actually Outer Alignment Failures","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Outer Alignment/Inner Alignment/Evolution","date":"2020-10-31T20:18","url":"https://www.lesswrong.com/posts/HYERofGZE6j9Tuigi/inner-alignment-failures-which-are-actually-outer-alignment"},{"x":"11.220166","y":"10.338886","title":"Functors and Coarse Worlds","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Embedded Agency/Abstraction","date":"2020-10-30T15:19","url":"https://www.lesswrong.com/posts/GYQwJsChoRosjdW2r/functors-and-coarse-worlds"},{"x":"11.815004","y":"10.038017","title":"Controllables and Observables, Revisited","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Embedded Agency","date":"2020-10-29T16:38","url":"https://www.lesswrong.com/posts/z3S2xnoDYfohrQQoe/controllables-and-observables-revisited"},{"x":"8.333786","y":"9.791536","title":"AI risk hub in Singapore?","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI Risk/World Optimization/The SF Bay Area/Community/AI","date":"2020-10-29T11:45","url":"https://www.lesswrong.com/posts/QTL5tRz7Q54bpcwdE/ai-risk-hub-in-singapore-1"},{"x":"10.954884","y":"7.5562587","title":"Draft papers for REALab and Decoupled Approval on tampering","cluster":"1","author":"['Jonathan Uesato', 'Ramana Kumar']","source":"alignment forum","tags":"Embedded Agency/Reinforcement Learning/Reward Functions/Wireheading/AI","date":"2020-10-28T16:01","url":"https://www.lesswrong.com/posts/X23q6T4CDifHykqi4/draft-papers-for-realab-and-decoupled-approval-on-tampering"},{"x":"11.288691","y":"10.396721","title":"Biextensional Equivalence","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Embedded Agency/AI/Category Theory","date":"2020-10-28T14:07","url":"https://www.lesswrong.com/posts/pWruFSY7494vnucCE/biextensional-equivalence"},{"x":"11.988966","y":"9.422624","title":"Dutch-Booking CDT: Revised Argument","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory/AI","date":"2020-10-27T04:31","url":"https://www.lesswrong.com/posts/X7k23zk9aBjjpgLd3/dutch-booking-cdt-revised-argument"},{"x":"8.768791","y":"8.508459","title":"Security Mindset and Takeoff Speeds","cluster":"4","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Security Mindset/Dialogue (format)/AI Timelines/AI Takeoff","date":"2020-10-27T03:20","url":"https://www.lesswrong.com/posts/Lfk2FXBwrpoM6Jm7p/security-mindset-and-takeoff-speeds"},{"x":"11.497325","y":"10.428812","title":"A Correspondence Theorem","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Practice & Philosophy of Science/Logic & Mathematics /Rationality","date":"2020-10-26T23:28","url":"https://www.lesswrong.com/posts/FWuByzM9T5qq2PF2n/a-correspondence-theorem"},{"x":"11.280234","y":"10.47638","title":"Additive Operations on Cartesian Frames","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Embedded Agency/Category Theory","date":"2020-10-26T15:12","url":"https://www.lesswrong.com/posts/ewkYgtZapQRtDPT2F/additive-operations-on-cartesian-frames"},{"x":"9.486544","y":"6.5508714","title":"Supervised learning of outputs in the brain","cluster":"0","author":"['Steven Byrnes']","source":"alignment forum","tags":"Neuroscience/AI/World Modeling/Machine Learning","date":"2020-10-26T14:32","url":"https://www.lesswrong.com/posts/jNrDzyc8PJ9HXtGFm/supervised-learning-of-outputs-in-the-brain"},{"x":"9.845519","y":"8.24229","title":"Reply to Jebari and Lundborg on Artificial Superintelligence","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-10-25T13:50","url":"https://www.lesswrong.com/posts/rokpjK3jcy5aKKwiT/reply-to-jebari-and-lundborg-on-artificial-superintelligence-1"},{"x":"10.904146","y":"8.237085","title":"Humans are stunningly rational and stunningly irrational","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Rationality/AI","date":"2020-10-23T14:13","url":"https://www.lesswrong.com/posts/X8KQBjszbSDXzBwgP/humans-are-stunningly-rational-and-stunningly-irrational"},{"x":"11.289685","y":"10.475497","title":"Introduction to Cartesian Frames","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Embedded Agency/AI","date":"2020-10-22T13:00","url":"https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames"},{"x":"8.46113","y":"9.341948","title":"The date of AI Takeover is not the day the AI takes over","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/AI Timelines/AI Takeoff","date":"2020-10-22T10:41","url":"https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over"},{"x":"10.965816","y":"10.430434","title":"[Question] Problems Involving Abstraction?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/Open Problems/Rationality","date":"2020-10-20T16:49","url":"https://www.lesswrong.com/posts/XArPqdkwCtEekgYxv/problems-involving-abstraction"},{"x":"9.396651","y":"8.8474245","title":"Box inversion hypothesis","cluster":"4","author":"['Jan Kulveit']","source":"alignment forum","tags":"AI/AI Services (CAIS)","date":"2020-10-20T16:20","url":"https://www.lesswrong.com/posts/TQwXPHfyyQwr22NMh/box-inversion-hypothesis"},{"x":"12.591569","y":"10.248649","title":"The Solomonoff Prior is Malign","cluster":"2","author":"['Mark Xu']","source":"alignment forum","tags":"Solomonoff Induction/Inner Alignment/Priors/AI","date":"2020-10-14T01:33","url":"https://www.lesswrong.com/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign"},{"x":"7.822049","y":"7.25647","title":"Toy Problem: Detective Story Alignment","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"AI","date":"2020-10-13T21:02","url":"https://www.lesswrong.com/posts/4kYkYSKSALH4JaQ99/toy-problem-detective-story-alignment"},{"x":"11.973654","y":"9.353969","title":"Knowledge, manipulation, and free will","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Rationality","date":"2020-10-13T17:47","url":"https://www.lesswrong.com/posts/2dKvTYYN4PTT7g4of/knowledge-manipulation-and-free-will"},{"x":"10.434411","y":"7.6044526","title":"The Alignment Problem: Machine Learning and Human Values","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"AI","date":"2020-10-06T17:41","url":"https://www.lesswrong.com/posts/gYfgWSxCpFdk2cZfE/the-alignment-problem-machine-learning-and-human-values"},{"x":"9.018919","y":"8.519142","title":"AGI safety from first principles: Conclusion","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-10-04T23:06","url":"https://www.lesswrong.com/posts/Ni8ocGupB2kGG2fA7/agi-safety-from-first-principles-conclusion"},{"x":"8.885804","y":"8.839399","title":"AGI safety from first principles: Control","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-10-02T21:51","url":"https://www.lesswrong.com/posts/eGihD5jnD6LFzgDZA/agi-safety-from-first-principles-control"},{"x":"7.81879","y":"7.292496","title":"Hiring engineers and researchers to help align GPT-3","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/GPT/Practical/Community/OpenAI","date":"2020-10-01T18:54","url":"https://www.lesswrong.com/posts/dJQo7xPn4TyGnKgeC/hiring-engineers-and-researchers-to-help-align-gpt-3"},{"x":"9.568596","y":"9.476416","title":"AGI safety from first principles: Alignment","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-10-01T03:13","url":"https://www.lesswrong.com/posts/PvA2gFMAaHCHfMXrw/agi-safety-from-first-principles-alignment"},{"x":"12.029182","y":"8.417239","title":"\"Zero Sum\" is a misnomer.","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Game Theory/Rationality/Terminology / Jargon (meta)","date":"2020-09-30T18:25","url":"https://www.lesswrong.com/posts/D8ds9idKWbwzCseCh/zero-sum-is-a-misnomer"},{"x":"7.959275","y":"7.234077","title":"\"Unsupervised\" translation as an (intent) alignment problem","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2020-09-30T00:50","url":"https://www.lesswrong.com/posts/saRRRdMnMPXXtQBNi/unsupervised-translation-as-an-intent-alignment-problem"},{"x":"9.372358","y":"9.027329","title":"AGI safety from first principles: Goals and Agency","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-09-29T19:06","url":"https://www.lesswrong.com/posts/bz5GdmCWj8o48726N/agi-safety-from-first-principles-goals-and-agency"},{"x":"9.249073","y":"8.175974","title":"AGI safety from first principles: Superintelligence","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-09-28T19:53","url":"https://www.lesswrong.com/posts/eG3WhHS8CLNxuH6rT/agi-safety-from-first-principles-superintelligence"},{"x":"8.893462","y":"8.362919","title":"AGI safety from first principles: Introduction","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-09-28T19:53","url":"https://www.lesswrong.com/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction"},{"x":"11.074638","y":"8.788075","title":"[Question] What Decision Theory is Implied By Predictive Processing?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Predictive Processing/AI/Rationality/Decision Theory","date":"2020-09-28T17:20","url":"https://www.lesswrong.com/posts/8Ziz5BQjtuhr9orm4/what-decision-theory-is-implied-by-predictive-processing"},{"x":"9.67216","y":"7.9618387","title":"[Question] What to do with imitation humans, other than asking them what the right thing to do is?","cluster":"4","author":"['Charlie Steiner']","source":"alignment forum","tags":"AI","date":"2020-09-27T21:51","url":"https://www.lesswrong.com/posts/LAR2ajpFDueNg45Mk/what-to-do-with-imitation-humans-other-than-asking-them-what"},{"x":"10.896349","y":"8.988011","title":"Dehumanisation *errors*","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-09-23T09:51","url":"https://www.lesswrong.com/posts/SfNwpyL7o49ohYyWB/dehumanisation-errors"},{"x":"10.925292","y":"8.435587","title":"Anthropomorphisation vs value learning: type 1 vs type 2 errors","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-09-22T10:46","url":"https://www.lesswrong.com/posts/LkytHQSKbQFf6toW5/anthropomorphisation-vs-value-learning-type-1-vs-type-2"},{"x":"8.423624","y":"9.74304","title":"Needed: AI infohazard policy","cluster":"3","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"AI/Information Hazards","date":"2020-09-21T15:26","url":"https://www.lesswrong.com/posts/3D3DsX5rMbk3jEZ5h/needed-ai-infohazard-policy"},{"x":"8.505083","y":"8.821949","title":"Clarifying \"What failure looks like\"","cluster":"3","author":"['Sam Clarke']","source":"alignment forum","tags":"AI/AI Risk/World Modeling","date":"2020-09-20T20:40","url":"https://www.lesswrong.com/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like"},{"x":"7.797581","y":"6.9881215","title":"Why GPT wants to mesa-optimize & how we might change this","cluster":"2","author":"['John_Maxwell']","source":"alignment forum","tags":"AI/GPT/Mesa-Optimization/Myopia","date":"2020-09-19T13:48","url":"https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this"},{"x":"7.732191","y":"8.742066","title":"Draft report on AI timelines","cluster":"3","author":"['Ajeya Cotra']","source":"alignment forum","tags":"AI Timelines/AI","date":"2020-09-18T23:47","url":"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"},{"x":"9.668725","y":"7.2632933","title":"The \"Backchaining to Local Search\" Technique in AI Alignment","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"AI","date":"2020-09-18T15:05","url":"https://www.lesswrong.com/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment"},{"x":"12.428898","y":"9.249886","title":"Applying the Counterfactual Prisoner’s Dilemma to Logical Uncertainty","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Counterfactuals/Counterfactual Mugging/Decision Theory/Rationality/AI","date":"2020-09-16T10:34","url":"https://www.lesswrong.com/posts/XzvR3QKkt9EPbAYyT/applying-the-counterfactual-prisoner-s-dilemma-to-logical"},{"x":"11.402616","y":"9.508693","title":"Comparing Utilities","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Utility Functions/Utilitarianism/Population Ethics","date":"2020-09-14T20:56","url":"https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities"},{"x":"8.951096","y":"7.020051","title":"My computational framework for the brain","cluster":"2","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/Neocortex","date":"2020-09-14T14:19","url":"https://www.lesswrong.com/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain"},{"x":"11.983737","y":"9.076943","title":"Decision Theory is multifaceted","cluster":"1","author":"['Michele Campolo']","source":"alignment forum","tags":"AI","date":"2020-09-13T22:30","url":"https://www.lesswrong.com/posts/vrJBQZJpvswXFFkcd/decision-theory-is-multifaceted"},{"x":"11.483693","y":"10.256276","title":"[Question] Egan’s Theorem?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Rationality/Adding Up to Normality","date":"2020-09-13T17:47","url":"https://www.lesswrong.com/posts/74crqQnH8v9JtJcda/egan-s-theorem"},{"x":"9.603129","y":"6.64351","title":"[Question] Do mesa-optimizer risk arguments rely on the train-test paradigm?","cluster":"0","author":"['Ben Cottier']","source":"alignment forum","tags":"AI/Mesa-Optimization","date":"2020-09-10T15:36","url":"https://www.lesswrong.com/posts/j5foHZhZ7RBhwRL7Z/do-mesa-optimizer-risk-arguments-rely-on-the-train-test"},{"x":"9.562182","y":"8.612392","title":"Safety via selection for obedience","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-09-10T10:04","url":"https://www.lesswrong.com/posts/7jNveWML34EsjCD4c/safety-via-selection-for-obedience"},{"x":"9.047182","y":"8.693525","title":"Safer sandboxing via collective separation","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-09-09T19:49","url":"https://www.lesswrong.com/posts/Fji2nHBaB6SjdSscr/safer-sandboxing-via-collective-separation"},{"x":"7.998122","y":"6.9219203","title":"Using GPT-N to Solve Interpretability of Neural Networks: A Research Agenda","cluster":"2","author":"['Logan Riggs', 'Gurkenglas']","source":"alignment forum","tags":"GPT/Machine Learning/Transparency / Interpretability (ML & AI)/Research Agendas/AI","date":"2020-09-03T18:27","url":"https://www.lesswrong.com/posts/zXfqftW8y69YzoXLj/using-gpt-n-to-solve-interpretability-of-neural-networks-a"},{"x":"7.780656","y":"6.024073","title":"interpreting GPT: the logit lens","cluster":"2","author":"['nostalgebraist']","source":"alignment forum","tags":"GPT/Machine Learning/AI/Transparency / Interpretability (ML & AI)/Gears-Level","date":"2020-08-31T02:47","url":"https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"},{"x":"9.299068","y":"7.312691","title":"Safe Scrambling?","cluster":"4","author":"['Hoagy']","source":"alignment forum","tags":"AI","date":"2020-08-29T14:31","url":"https://www.lesswrong.com/posts/jHzb5SmviScXdtT2m/safe-scrambling"},{"x":"10.876376","y":"9.521935","title":"Updates and additions to \"Embedded Agency\"","cluster":"1","author":"['Rob Bensinger', 'abramdemski']","source":"alignment forum","tags":"Embedded Agency/Site Meta/AI","date":"2020-08-29T04:22","url":"https://www.lesswrong.com/posts/9vYg8MyLL4cMMaPQJ/updates-and-additions-to-embedded-agency"},{"x":"11.508086","y":"10.015656","title":"Technical model refinement formalism","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Machine Learning/Definitions","date":"2020-08-27T11:54","url":"https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/technical-model-refinement-formalism"},{"x":"9.627857","y":"7.7483525","title":"Model splintering: moving from one imperfect model to another","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Machine Learning/Iterated Amplification ","date":"2020-08-27T11:53","url":"https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1"},{"x":"11.651836","y":"8.869953","title":"Introduction To The Infra-Bayesianism Sequence","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Infra-Bayesianism/AI/Bayes' Theorem/Decision Theory","date":"2020-08-27T08:02","url":"https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence"},{"x":"11.837395","y":"10.434103","title":"Basic Inframeasure Theory","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Infra-Bayesianism/AI/Logic & Mathematics /Decision Theory","date":"2020-08-27T08:02","url":"https://www.lesswrong.com/posts/YAa4qcMyoucRS2Ykr/basic-inframeasure-theory"},{"x":"12.106282","y":"9.240354","title":"Belief Functions And Decision Theory","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Infra-Bayesianism/AI/Decision Theory/Logic & Mathematics ","date":"2020-08-27T08:00","url":"https://www.lesswrong.com/posts/e8qFDMzs2u9xf5ie6/belief-functions-and-decision-theory"},{"x":"11.862016","y":"10.221846","title":"Proofs Section 1.1 (Initial results to LF-duality)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"AI/Infra-Bayesianism/Logic & Mathematics /Formal Proof","date":"2020-08-27T07:59","url":"https://www.lesswrong.com/posts/PTcktJADsAmpYEjoP/proofs-section-1-1-initial-results-to-lf-duality"},{"x":"11.991401","y":"10.359658","title":"Proofs Section 1.2 (Mixtures, Updates, Pushforwards)","cluster":"2","author":"['Diffractor']","source":"alignment forum","tags":"AI/Infra-Bayesianism/Logic & Mathematics /Formal Proof","date":"2020-08-27T07:57","url":"https://www.lesswrong.com/posts/b9jubzqz866CModHB/proofs-section-1-2-mixtures-updates-pushforwards"},{"x":"12.187487","y":"9.431596","title":"Proofs Section 2.1 (Theorem 1, Lemmas)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"AI/Infra-Bayesianism/Logic & Mathematics /Decision Theory/Formal Proof","date":"2020-08-27T07:54","url":"https://www.lesswrong.com/posts/xQYF3LR64NYn8vkoy/proofs-section-2-1-theorem-1-lemmas"},{"x":"12.063757","y":"10.066003","title":"Proofs Section 2.2 (Isomorphism to Expectations)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"AI/Infra-Bayesianism/Decision Theory/Logic & Mathematics /Formal Proof","date":"2020-08-27T07:52","url":"https://www.lesswrong.com/posts/8tLPYYQJM8SwL2xn9/proofs-section-2-2-isomorphism-to-expectations"},{"x":"12.392001","y":"10.088786","title":"Proofs Section 2.3 (Updates, Decision Theory)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"AI/Infra-Bayesianism/Decision Theory/Logic & Mathematics /Formal Proof","date":"2020-08-27T07:49","url":"https://www.lesswrong.com/posts/9ekP8FojvLa8Pr6P7/proofs-section-2-3-updates-decision-theory"},{"x":"12.030037","y":"10.128066","title":"[Question] What is the interpretation of the do() operator?","cluster":"1","author":"['Bunthut']","source":"alignment forum","tags":"Causality/World Modeling","date":"2020-08-26T21:54","url":"https://www.lesswrong.com/posts/uwfstudGoNFSEtMAT/what-is-the-interpretation-of-the-do-operator"},{"x":"11.011713","y":"8.889134","title":"Learning human preferences: black-box, white-box, and structured white-box access","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Value Learning","date":"2020-08-24T11:42","url":"https://www.lesswrong.com/posts/9rjW9rhyhJijHTM92/learning-human-preferences-black-box-white-box-and"},{"x":"7.7207108","y":"8.685444","title":"[Question] Forecasting Thread: AI Timelines","cluster":"3","author":"['Amandango', 'Daniel Kokotajlo', 'Ben Pace']","source":"alignment forum","tags":"AI Timelines/AI/Forecasts (Specific Predictions)","date":"2020-08-22T02:33","url":"https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines"},{"x":"10.644738","y":"10.123262","title":"Universality Unwrapped","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Humans Consulting HCH/AI/Deception","date":"2020-08-21T18:53","url":"https://www.lesswrong.com/posts/farherQcqFQXqRcvv/universality-unwrapped"},{"x":"9.8614235","y":"8.747889","title":"AI safety as featherless bipeds *with broad flat nails*","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-08-19T10:22","url":"https://www.lesswrong.com/posts/gWxMZisqE2j2kHCd2/ai-safety-as-featherless-bipeds-with-broad-flat-nails"},{"x":"9.826307","y":"9.918316","title":"Looking for adversarial collaborators to test our Debate protocol","cluster":"1","author":"['Beth Barnes']","source":"alignment forum","tags":"AI/Debate (AI safety technique)/Adversarial Collaboration","date":"2020-08-19T03:15","url":"https://www.lesswrong.com/posts/w7mS6syTderWihHPM/looking-for-adversarial-collaborators-to-test-our-debate"},{"x":"12.145715","y":"10.071541","title":"Radical Probabilism","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Radical Probabilism/Logical Induction/Bayes' Theorem/Epistemology/Probabilistic Reasoning/Probability & Statistics/Conservation of Expected Evidence/Rationality/Problem of Old Evidence","date":"2020-08-18T21:14","url":"https://www.lesswrong.com/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1"},{"x":"9.880648","y":"6.8608775","title":"Mesa-Search vs Mesa-Control","cluster":"0","author":"['abramdemski']","source":"alignment forum","tags":"Mesa-Optimization/Inner Alignment/AI/Selection vs Control","date":"2020-08-18T18:51","url":"https://www.lesswrong.com/posts/WmBukJkEFM72Xr397/mesa-search-vs-mesa-control"},{"x":"11.326701","y":"9.083295","title":"Learning human preferences: optimistic and pessimistic scenarios","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Rationality","date":"2020-08-18T13:05","url":"https://www.lesswrong.com/posts/6XLyM22PBd9qDtin8/learning-human-preferences-optimistic-and-pessimistic"},{"x":"10.847073","y":"9.383195","title":"Goal-Directedness: What Success Looks Like","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/AI","date":"2020-08-16T18:33","url":"https://www.lesswrong.com/posts/jP4cx3TCweDngSLS6/goal-directedness-what-success-looks-like"},{"x":"9.891678","y":"7.795275","title":"Search versus design","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"Optimization/Machine Learning/AI/Transparency / Interpretability (ML & AI)/Distinctions","date":"2020-08-16T16:53","url":"https://www.lesswrong.com/posts/r3NHPD3dLFNk9QE2Y/search-versus-design-1"},{"x":"8.539637","y":"9.050272","title":"My Understanding of Paul Christiano’s Iterated Amplification AI Safety Research Agenda","cluster":"3","author":"['Chi Nguyen']","source":"alignment forum","tags":"AI/Iterated Amplification ","date":"2020-08-15T20:02","url":"https://www.lesswrong.com/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification"},{"x":"8.066798","y":"7.430819","title":"Mapping Out Alignment","cluster":"4","author":"['Logan Riggs', 'adamShimi', 'Gurkenglas', 'AlexMennen', 'Gyrodiot']","source":"alignment forum","tags":"AI","date":"2020-08-15T01:02","url":"https://www.lesswrong.com/posts/jeiz7WfCnGQWoShkT/mapping-out-alignment"},{"x":"8.568256","y":"8.772833","title":"Blog post: A tale of two research communities","cluster":"4","author":"['Aryeh Englander']","source":"alignment forum","tags":"AI","date":"2020-08-12T20:41","url":"https://www.lesswrong.com/posts/FzF4Xok63ZCZNjmGY/blog-post-a-tale-of-two-research-communities"},{"x":"11.467522","y":"9.660363","title":"Strong implication of preference uncertainty","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-08-12T19:02","url":"https://www.lesswrong.com/posts/q9GZyfm8xKAD2BGdi/strong-implication-of-preference-uncertainty"},{"x":"8.691822","y":"7.1210227","title":"Alignment By Default","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Abstraction","date":"2020-08-12T18:54","url":"https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default"},{"x":"9.655472","y":"6.511079","title":"Matt Botvinick on the spontaneous emergence of learning algorithms","cluster":"0","author":"['Adam Scholl']","source":"alignment forum","tags":"Mesa-Optimization/Neuroscience/Neocortex/AI/Machine Learning/Inner Alignment/Emergent Behavior","date":"2020-08-12T07:47","url":"https://www.lesswrong.com/posts/Wnqua6eQkewL3bqsF/matt-botvinick-on-the-spontaneous-emergence-of-learning"},{"x":"8.617957","y":"9.3631735","title":"[Question] Will OpenAI’s work unintentionally increase existential risks related to AI?","cluster":"3","author":"['adamShimi']","source":"alignment forum","tags":"OpenAI/GPT/AI Risk/AI","date":"2020-08-11T18:16","url":"https://www.lesswrong.com/posts/CD8gcugDu5z2Eeq7k/will-openai-s-work-unintentionally-increase-existential"},{"x":"8.443053","y":"9.288057","title":"Book review: Architects of Intelligence by Martin Ford (2018)","cluster":"3","author":"['ofer']","source":"alignment forum","tags":"AI/Book Reviews/AI Risk/AI Governance","date":"2020-08-11T17:30","url":"https://www.lesswrong.com/posts/iZS3am4acMh8g4Ycb/book-review-architects-of-intelligence-by-martin-ford-2018"},{"x":"9.636634","y":"8.781866","title":"The Fusion Power Generator Scenario","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"Information Hazards/AI/AI Risk/Tool AI","date":"2020-08-08T18:31","url":"https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario"},{"x":"8.105136","y":"7.2577066","title":"Analyzing the Problem GPT-3 is Trying to Solve","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"GPT/Computer Science/AI","date":"2020-08-06T21:58","url":"https://www.lesswrong.com/posts/XdnCyorFzYskS7EtP/analyzing-the-problem-gpt-3-is-trying-to-solve"},{"x":"8.04658","y":"6.1825967","title":"Measuring hardware overhang","cluster":"2","author":"['hippke']","source":"alignment forum","tags":"AI/Computing Overhang/AI Timelines","date":"2020-08-05T19:59","url":"https://www.lesswrong.com/posts/75dnjiD8kv2khe9eQ/measuring-hardware-overhang"},{"x":"9.876093","y":"7.983558","title":"Infinite Data/​Compute Arguments in Alignment","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"AI","date":"2020-08-04T20:21","url":"https://www.lesswrong.com/posts/7CJBiHYxebTmMfGs3/infinite-data-compute-arguments-in-alignment"},{"x":"8.692073","y":"6.3995442","title":"Interpretability in ML: A Broad Overview","cluster":"2","author":"['lifelonglearner']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/Machine Learning/AI","date":"2020-08-04T19:03","url":"https://www.lesswrong.com/posts/57fTWCpsAyjeAimTp/interpretability-in-ml-a-broad-overview-2"},{"x":"9.376445","y":"9.522346","title":"Three mental images from thinking about AGI debate & corrigibility","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Corrigibility/Debate (AI safety technique)","date":"2020-08-03T14:29","url":"https://www.lesswrong.com/posts/WjY9y7r52vaNZ2WmH/three-mental-images-from-thinking-about-agi-debate-and"},{"x":"8.330236","y":"6.7521563","title":"Inner Alignment: Explain like I’m 12 Edition","cluster":"2","author":"['Rafael Harth']","source":"alignment forum","tags":"Inner Alignment/AI/Mesa-Optimization","date":"2020-08-01T15:24","url":"https://www.lesswrong.com/posts/AHhCrJ2KpTjsCSwbt/inner-alignment-explain-like-i-m-12-edition"},{"x":"11.65628","y":"8.551323","title":"Power as Easily Exploitable Opportunities","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence/LessWrong Event Transcripts","date":"2020-08-01T02:14","url":"https://www.lesswrong.com/posts/eqov4SEYEbeFMXegR/power-as-easily-exploitable-opportunities"},{"x":"11.012702","y":"9.419428","title":"\"Go west, young man!\"—Preferences in (imperfect) maps","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling","date":"2020-07-31T07:50","url":"https://www.lesswrong.com/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps"},{"x":"9.775409","y":"8.405111","title":"[Question] What if memes are common in highly capable minds?","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/Memetics/Robust Agents","date":"2020-07-30T20:45","url":"https://www.lesswrong.com/posts/6iedrXht3GpKTQWRF/what-if-memes-are-common-in-highly-capable-minds"},{"x":"8.665801","y":"5.842158","title":"Learning the prior and generalization","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"Priors/AI","date":"2020-07-29T22:49","url":"https://www.lesswrong.com/posts/YhQr36yGkhe6x8Fyn/learning-the-prior-and-generalization"},{"x":"8.434476","y":"9.01412","title":"What Failure Looks Like: Distilling the Discussion","cluster":"3","author":"['Ben Pace']","source":"alignment forum","tags":"Threat Models/Site Meta/AI/AI Risk","date":"2020-07-29T21:49","url":"https://www.lesswrong.com/posts/6jkGf5WEKMpMFXZp2/what-failure-looks-like-distilling-the-discussion"},{"x":"13.023395","y":"9.967908","title":"The \"best predictor is malicious optimiser\" problem","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"AI/AIXI","date":"2020-07-29T11:49","url":"https://www.lesswrong.com/posts/ARGXciEGtuhMKtYb8/the-best-predictor-is-malicious-optimiser-problem"},{"x":"8.326273","y":"5.9470277","title":"[Question] What happens to variance as neural network training is scaled? What does it imply about \"lottery tickets\"?","cluster":"2","author":"['abramdemski']","source":"alignment forum","tags":"AI/AI Timelines/Lottery Ticket Hypothesis","date":"2020-07-28T20:22","url":"https://www.lesswrong.com/posts/kFm9ZMreqeNYpg8m8/what-happens-to-variance-as-neural-network-training-is"},{"x":"8.006422","y":"5.8908806","title":"[Question] To what extent are the scaling properties of Transformer networks exceptional?","cluster":"2","author":"['abramdemski']","source":"alignment forum","tags":"GPT/AI","date":"2020-07-28T20:06","url":"https://www.lesswrong.com/posts/KyM9p6q5SELM3Ncdu/to-what-extent-are-the-scaling-properties-of-transformer-1"},{"x":"8.838951","y":"6.026975","title":"[Question] Does the lottery ticket hypothesis suggest the scaling hypothesis?","cluster":"2","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/Lottery Ticket Hypothesis","date":"2020-07-28T19:52","url":"https://www.lesswrong.com/posts/wFJqi75y9eW8mf8TR/does-the-lottery-ticket-hypothesis-suggest-the-scaling"},{"x":"7.73996","y":"7.492381","title":"[Question] What specific dangers arise when asking GPT-N to write an Alignment Forum post?","cluster":"2","author":"['Matthew Barnett']","source":"alignment forum","tags":"AI/Mesa-Optimization/Community/GPT","date":"2020-07-28T02:56","url":"https://www.lesswrong.com/posts/Et2pWrj4nWfdNAawh/what-specific-dangers-arise-when-asking-gpt-n-to-write-an"},{"x":"8.031505","y":"6.979861","title":"Are we in an AI overhang?","cluster":"2","author":"['Andy Jones']","source":"alignment forum","tags":"AI Timelines/GPT/AI/Computing Overhang","date":"2020-07-27T12:48","url":"https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang"},{"x":"11.731268","y":"8.158759","title":"Generalizing the Power-Seeking Theorems","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Instrumental Convergence","date":"2020-07-27T00:28","url":"https://www.lesswrong.com/posts/nyDnLif4cjeRe9DSv/generalizing-the-power-seeking-theorems"},{"x":"8.403417","y":"6.9374075","title":"Developmental Stages of GPTs","cluster":"2","author":"['orthonormal']","source":"alignment forum","tags":"GPT/AI Timelines/AI/Existential Risk/OpenAI/AI Risk","date":"2020-07-26T22:03","url":"https://www.lesswrong.com/posts/3nDR23ksSQJ98WNDm/developmental-stages-of-gpts"},{"x":"10.636457","y":"9.44188","title":"Constraints from naturalized ethics.","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"World Optimization/Value Learning","date":"2020-07-25T14:54","url":"https://www.lesswrong.com/posts/xN8MTRN7GchFJB8WJ/constraints-from-naturalized-ethics"},{"x":"11.30377","y":"7.042746","title":"Cooperative Inverse Reinforcement Learning vs. Irrational Human Preferences","cluster":"0","author":"['orthonormal']","source":"alignment forum","tags":"Inverse Reinforcement Learning/Reinforcement Learning","date":"2016-06-18T00:55","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751bf/cooperative-inverse-reinforcement-learning-vs-irrational-human-preferences"},{"x":"11.710514","y":"7.1151214","title":"General Cooperative Inverse RL Convergence","cluster":"0","author":"['janleike']","source":"alignment forum","tags":"","date":"2016-06-17T02:26","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751bd/general-cooperative-inverse-rl-convergence"},{"x":"10.550341","y":"8.092378","title":"Conservation of Expected Ethics isn’t enough","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-06-15T18:08","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751b9/conservation-of-expected-ethics-isn-t-enough"},{"x":"10.54363","y":"8.422606","title":"The overfitting utility problem for value learning AIs","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-06-12T23:25","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751b3/the-overfitting-utility-problem-for-value-learning-ais"},{"x":"11.802737","y":"9.480167","title":"In memoryless Cartesian environments, every UDT policy is a CDT+SIA policy","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Decision Theory","date":"2016-06-11T04:05","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751b2/in-memoryless-cartesian-environments-every-udt-policy-is-a"},{"x":"10.531055","y":"9.004473","title":"Cake or Death toy model for corrigibility","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-06-10T21:56","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751ae/cake-or-death-toy-model-for-corrigibility"},{"x":"11.851984","y":"9.119513","title":"Two problems with causal-counterfactual utility indifference","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-05-26T06:21","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751a4/two-problems-with-causal-counterfactual-utility-indifference"},{"x":"12.819241","y":"9.512718","title":"Stabilizing logical counterfactuals by pseudorandomization","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Counterfactuals","date":"2016-05-25T12:05","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750cf/stabilizing-logical-counterfactuals-by-pseudorandomization"},{"x":"12.33401","y":"9.940043","title":"Is logic epistemically appropriate?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2016-05-25T04:07","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703751a3/is-logic-epistemically-appropriate"},{"x":"12.309137","y":"8.274847","title":"Anything you can do with n AIs, you can do with two (with directly opposed objectives)","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-05-04T23:14","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375190/anything-you-can-do-with-n-ais-you-can-do-with-two-with-directly-opposed-objectives"},{"x":"11.169033","y":"8.3343","title":"Double indifference is better indifference","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-05-04T14:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037518b/double-indifference-is-better-indifference"},{"x":"11.719967","y":"7.9024878","title":"Lagrangian duality for constraints on expectations","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-05-04T04:37","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037518d/lagrangian-duality-for-constraints-on-expectations"},{"x":"11.765715","y":"10.523176","title":"Games for factoring out variables","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-04-28T13:38","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375157/games-for-factoring-out-variables"},{"x":"12.844922","y":"10.250742","title":"Time hierarchy theorems for distributional estimation problems","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"","date":"2016-04-20T17:13","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375166/time-hierarchy-theorems-for-distributional-estimation-problems"},{"x":"12.301319","y":"9.03348","title":"The many counterfactuals of counterfactual mugging","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Counterfactuals","date":"2016-04-12T20:04","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375158/the-many-counterfactuals-of-counterfactual-mugging"},{"x":"11.503297","y":"9.644702","title":"A brief note on factoring out certain variables","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-04-07T16:38","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375144/a-brief-note-on-factoring-out-certain-variables"},{"x":"10.413425","y":"6.7634625","title":"Rényi divergence as a secondary objective","cluster":"2","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-04-06T02:08","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037513a/renyi-divergence-as-a-secondary-objective"},{"x":"11.760421","y":"8.18046","title":"Maximizing a quantity while ignoring effect through some channel","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-04-02T01:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037513c/maximizing-a-quantity-while-ignoring-effect-through-some-channel"},{"x":"13.102422","y":"10.103061","title":"Predictor schemes with logarithmic advice","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2016-03-27T08:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fe1/predictor-schemes-with-logarithmic-advice"},{"x":"12.235791","y":"9.084356","title":"Reflection with optimal predictors","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2016-03-22T17:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375067/reflection-with-optimal-predictors"},{"x":"9.612943","y":"8.772296","title":"Virtual models of virtual AIs in virtual worlds","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-03-11T09:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750ee/virtual-models-of-virtual-ais-in-virtual-worlds"},{"x":"9.15444","y":"7.824915","title":"What does it mean for correct operation to rely on transfer learning?","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-03-05T03:24","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037510a/what-does-it-mean-for-correct-operation-to-rely-on-transfer-learning"},{"x":"11.08357","y":"6.9977736","title":"Corrigibility and interruptibility for various agents","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-03-03T15:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375109/corrigibility-and-interruptibility-for-various-agents"},{"x":"12.633179","y":"9.778002","title":"Failures of throttling logical information","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2016-02-24T22:05","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750e9/failures-of-throttling-logical-information"},{"x":"12.546296","y":"9.902771","title":"Speculations on information under logical uncertainty","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"Information Theory","date":"2016-02-24T21:58","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750e8/speculations-on-information-under-logical-uncertainty"},{"x":"10.09314","y":"8.507608","title":"Notes from a conversation on act-based and goal-directed systems","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-02-19T00:42","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750d7/notes-from-a-conversation-on-act-based-and-goal-directed-systems"},{"x":"9.587695","y":"7.7088556","title":"A possible training procedure for human-imitators","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2016-02-16T22:43","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750d5/a-possible-training-procedure-for-human-imitators"},{"x":"10.625843","y":"9.607964","title":"Things that can go wrong with decomposed modules","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-02-11T12:29","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750d0/things-that-can-go-wrong-with-decomposed-modules"},{"x":"12.600122","y":"10.179238","title":"Proof Length and Logical Counterfactuals Revisited","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2016-02-10T18:56","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375019/proof-length-and-logical-counterfactuals-revisited"},{"x":"12.378342","y":"9.680385","title":"Naturalistic Logical Updates","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2016-02-09T01:28","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750ce/naturalistic-logical-updates"},{"x":"12.363199","y":"10.189191","title":"Thoughts on Logical Dutch Book Arguments","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2016-02-07T23:10","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750a3/thoughts-on-logical-dutch-book-arguments"},{"x":"12.914499","y":"10.18639","title":"Another Concise Open Problem","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2016-01-28T23:04","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750c2/another-concise-open-problem"},{"x":"12.99073","y":"10.244378","title":"Second Failure of Inductive Learning with a Delay in Feedback","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2016-01-28T21:32","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750c1/second-failure-of-inductive-learning-with-a-delay-in-feedback"},{"x":"11.594919","y":"6.1945953","title":"Goal completion: algorithm ideas","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2016-01-26T10:01","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750bb/goal-completion-algorithm-ideas"},{"x":"11.9128275","y":"8.095773","title":"Another view of quantilizers: avoiding Goodhart’s Law","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Quantilization","date":"2016-01-09T04:02","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750b1/another-view-of-quantilizers-avoiding-goodhart-s-law"},{"x":"12.678207","y":"9.3666","title":"Logical counterfactuals for random algorithms","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Counterfactuals","date":"2016-01-06T13:29","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750a5/logical-counterfactuals-for-random-algorithms"},{"x":"12.849505","y":"9.011313","title":"Active learning for opaque predictors","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"","date":"2016-01-03T21:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703750ab/active-learning-for-opaque-predictors"},{"x":"13.060841","y":"10.137221","title":"Quasi-optimal predictors","cluster":"2","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2015-12-25T14:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fb9/quasi-optimal-predictors"},{"x":"10.835397","y":"8.653901","title":"A sketch of a value-learning sovereign","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Ontology","date":"2015-12-20T21:32","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375072/a-sketch-of-a-value-learning-sovereign"},{"x":"12.702955","y":"9.049803","title":"Implementing CDT with optimal predictor systems","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2015-12-20T12:58","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375082/implementing-cdt-with-optimal-predictor-systems"},{"x":"12.487933","y":"10.0533285","title":"Logical Counterfactuals Consistent Under Self-Modification","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Counterfactuals","date":"2015-12-15T06:38","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375085/logical-counterfactuals-consistent-under-self-modification"},{"x":"12.9183655","y":"10.177009","title":"Concise Open Problem in Logical Uncertainty","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-12-12T03:03","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375084/concise-open-problem-in-logical-uncertainty"},{"x":"12.884815","y":"10.293215","title":"Benford Test with Gaps","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-12-11T21:40","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375083/benford-test-with-gaps"},{"x":"12.515672","y":"10.282892","title":"Existence of distributions that are expectation-reflective and know it","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2015-12-10T07:35","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375081/existence-of-distributions-that-are-expectation-reflective-and-know-it"},{"x":"12.594195","y":"10.375875","title":"Reflective Probability Distributions and Standard Models of Arithmetic","cluster":"1","author":"['IAFF-User-4']","source":"alignment forum","tags":"","date":"2015-12-07T22:04","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037503d/reflective-probability-distributions-and-standard-models-of-arithmetic"},{"x":"12.742533","y":"10.185865","title":"Strict Dominance for the Modified Demski Prior","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-12-04T05:57","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037507a/strict-dominance-for-the-modified-demski-prior"},{"x":"11.597872","y":"9.19175","title":"Desiderata for Normalizers","cluster":"1","author":"['IAFF-User-4']","source":"alignment forum","tags":"","date":"2015-12-03T20:45","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037507c/desiderata-for-normalizers"},{"x":"10.466456","y":"8.572249","title":"Three preference frameworks for goal-directed agents","cluster":"4","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-12-02T00:06","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375078/three-preference-frameworks-for-goal-directed-agents"},{"x":"10.833762","y":"8.305587","title":"What do we need value learning for?","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-11-29T01:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375077/what-do-we-need-value-learning-for"},{"x":"12.631944","y":"10.319137","title":"A limit-computable, self-reflective distribution","cluster":"1","author":"['TsviBT']","source":"alignment forum","tags":"","date":"2015-11-15T21:43","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375060/a-limit-computable-self-reflective-distribution"},{"x":"13.02666","y":"10.203299","title":"Bounded Solomonoff induction using optimal predictor schemes","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2015-11-10T13:59","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375039/bounded-solomonoff-induction-using-optimal-predictor-schemes"},{"x":"12.66881","y":"10.269095","title":"Modified Demski Prior Problem: Revision","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-11-06T21:45","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037505a/modified-demski-prior-problem-revision"},{"x":"12.459599","y":"8.739672","title":"Superrationality in arbitrary games","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Superrationality","date":"2015-11-04T18:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375058/superrationality-in-arbitrary-games"},{"x":"13.055542","y":"10.0330925","title":"Optimal predictor schemes","cluster":"2","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2015-11-01T17:28","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fd2/optimal-predictor-schemes"},{"x":"12.934886","y":"10.18721","title":"Bounded Solomonoff Induction: Three Difficulty Settings","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-10-26T05:20","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037504a/bounded-solomonoff-induction-three-difficulty-settings"},{"x":"9.911184","y":"8.201151","title":"Superintelligence and wireheading","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-10-23T14:54","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375049/superintelligence-and-wireheading"},{"x":"10.780536","y":"9.117043","title":"A first look at the hard problem of corrigibility","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-10-15T20:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375041/a-first-look-at-the-hard-problem-of-corrigibility"},{"x":"10.000908","y":"9.439569","title":"Trustworthy answers via indifference","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-10-13T16:34","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375040/trustworthy-answers-via-indifference"},{"x":"12.918257","y":"10.166432","title":"Subsequence Induction (draft)","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-10-12T06:19","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037503f/subsequence-induction-draft"},{"x":"8.224415","y":"8.039796","title":"Chatbots or set answers, not WBEs","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Whole Brain Emulation","date":"2015-10-09T15:48","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037503c/chatbots-or-set-answers-not-wbes"},{"x":"12.713452","y":"10.339316","title":"Why the Modified Demski Prior is better than the original","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-10-06T18:50","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375035/why-the-modified-demski-prior-is-better-than-the-original"},{"x":"12.879649","y":"10.333476","title":"The Bayes Score Meliorizer","cluster":"2","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-10-05T05:50","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375037/the-bayes-score-meliorizer"},{"x":"12.999437","y":"10.216744","title":"Logical predictors cannot dominate their own complexity class","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-10-05T04:45","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375036/logical-predictors-cannot-dominate-their-own-complexity-class"},{"x":"12.914265","y":"10.198535","title":"Two Questions about Solomonoff Induction","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-10-04T05:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375032/two-questions-about-solomonoff-induction"},{"x":"10.882367","y":"7.8891335","title":"Conservative classifiers","cluster":"2","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-10-02T03:56","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375030/conservative-classifiers"},{"x":"12.665512","y":"10.262554","title":"The Two-Update Problem, Part 3","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-09-30T21:52","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037501d/the-two-update-problem-part-3"},{"x":"13.061485","y":"10.058159","title":"Towards reflection with relative optimal predictor schemes","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2015-09-30T15:44","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037502b/towards-reflection-with-relative-optimal-predictor-schemes"},{"x":"11.791797","y":"7.879091","title":"Quantilizers maximize expected utility subject to a conservative cost constraint","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Quantilization/Mild Optimization/AI","date":"2015-09-28T02:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375029/quantilizers-maximize-expected-utility-subject-to-a-conservative-cost-constraint"},{"x":"12.966533","y":"10.210851","title":"Fixing Resource Bounded Solomonoff Induction (draft)","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-09-21T17:51","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375023/fixing-resource-bounded-solomonoff-induction-draft"},{"x":"12.684716","y":"10.332571","title":"A Problem with the Modified Demski Prior","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-09-13T23:37","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375012/a-problem-with-the-modified-demski-prior"},{"x":"12.552897","y":"10.252009","title":"The Two-Update Problem, Part 1","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-09-13T23:01","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375008/the-two-update-problem-part-1"},{"x":"11.355637","y":"10.236119","title":"Cartographic Processes","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Truth, Semantics, & Meaning/Map and Territory","date":"2019-08-27T20:02","url":"https://www.lesswrong.com/posts/t5DFpygMqpnFsmJ3b/cartographic-processes"},{"x":"8.338875","y":"9.260588","title":"Six AI Risk/​Strategy Ideas","cluster":"3","author":"['Wei_Dai']","source":"alignment forum","tags":"AI/AI Services (CAIS)/AI Risk","date":"2019-08-27T00:40","url":"https://www.lesswrong.com/posts/dt4z82hpvvPFTDTfZ/six-ai-risk-strategy-ideas"},{"x":"10.740456","y":"9.892384","title":"Embedded Agency via Abstraction","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Subagents/Embedded Agency/Abstraction","date":"2019-08-26T23:03","url":"https://www.lesswrong.com/posts/hLFD6qSN9MmQxKjG5/embedded-agency-via-abstraction"},{"x":"10.755961","y":"8.004923","title":"Reversible changes: consider a bucket of water","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Complexity of Value/Impact Measures","date":"2019-08-26T22:55","url":"https://www.lesswrong.com/posts/zrunBA8B5bmm2XZ59/reversible-changes-consider-a-bucket-of-water"},{"x":"11.3198","y":"9.693996","title":"Toy model piece #3: close and distant situations","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-08-26T22:41","url":"https://www.lesswrong.com/posts/EYmNEPt4jqQar4Wx8/toy-model-piece-3-close-and-distant-situations"},{"x":"9.640161","y":"10.004874","title":"Problems with AI debate","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Debate (AI safety technique)","date":"2019-08-26T19:21","url":"https://www.lesswrong.com/posts/fNTCveSa4HvqvZR2F/problems-with-ai-debate"},{"x":"11.159787","y":"9.092134","title":"Gratification: a useful concept, maybe new","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Hedonism","date":"2019-08-25T18:58","url":"https://www.lesswrong.com/posts/GnPSQAi3QzHjK8ZQR/gratification-a-useful-concept-maybe-new"},{"x":"9.750614","y":"8.834733","title":"Optimization Provenance","cluster":"4","author":"['Adele Lopez']","source":"alignment forum","tags":"Optimization","date":"2019-08-23T20:08","url":"https://www.lesswrong.com/posts/Zj2PgP5A8vY2G3gYw/optimization-provenance"},{"x":"12.072113","y":"9.068468","title":"Troll Bridge","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory/Embedded Agency","date":"2019-08-23T18:36","url":"https://www.lesswrong.com/posts/hpAbfXtqYC2BrpeiC/troll-bridge-5"},{"x":"11.753562","y":"10.113052","title":"Understanding understanding","cluster":"1","author":"['mthq']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI","date":"2019-08-23T18:10","url":"https://www.lesswrong.com/posts/WGE6nqbWzzDxRmMs7/understanding-understanding"},{"x":"10.936617","y":"9.060618","title":"When do utility functions constrain?","cluster":"1","author":"['Hoagy']","source":"alignment forum","tags":"Utility Functions","date":"2019-08-23T17:19","url":"https://www.lesswrong.com/posts/yGuo5R9fgrrFLYWuv/when-do-utility-functions-constrain-1"},{"x":"8.812193","y":"6.345502","title":"Thoughts on Retrieving Knowledge from Neural Networks","cluster":"2","author":"['Jaime Ruiz']","source":"alignment forum","tags":"","date":"2019-08-23T16:41","url":"https://www.lesswrong.com/posts/22gPT6L5XEvHqW7bi/thoughts-on-retrieving-knowledge-from-neural-networks"},{"x":"8.570323","y":"7.193331","title":"Algorithmic Similarity","cluster":"2","author":"['LukasM']","source":"alignment forum","tags":"Computer Science/AI","date":"2019-08-23T16:39","url":"https://www.lesswrong.com/posts/BS7Syu2buhLYRjkwY/algorithmic-similarity"},{"x":"7.7827325","y":"9.303218","title":"Soft takeoff can still lead to decisive strategic advantage","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI Takeoff/Industrial Revolution/AI Risk/AI Governance","date":"2019-08-23T16:39","url":"https://www.lesswrong.com/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage"},{"x":"11.334109","y":"9.656842","title":"Torture and Dust Specks and Joy—Oh my!","cluster":"1","author":"['Louis_Brown']","source":"alignment forum","tags":"","date":"2019-08-23T11:11","url":"https://www.lesswrong.com/posts/3eKqeq8kzEmEsoFiy/torture-and-dust-specks-and-joy-oh-my-or-non-archimedean"},{"x":"10.152823","y":"9.634604","title":"Metalignment: Deconfusing metaethics for AI alignment. ","cluster":"1","author":"['Tancrede']","source":"alignment forum","tags":"","date":"2019-08-23T10:25","url":"https://www.lesswrong.com/posts/BL7QmtAgvkjMqKe29/metalignment-deconfusing-metaethics-for-ai-alignment-1"},{"x":"10.699803","y":"9.601877","title":"Towards an Intentional Research Agenda","cluster":"1","author":"['romeostevensit']","source":"alignment forum","tags":"Predictive Processing/Hansonian Pre-Rationality/Intentionality","date":"2019-08-23T05:27","url":"https://www.lesswrong.com/posts/CHSRhSKcrSmQWnD6A/towards-an-intentional-research-agenda"},{"x":"10.98433","y":"9.752697","title":"Vague Thoughts and Questions about Agent Structures","cluster":"1","author":"['loriphos']","source":"alignment forum","tags":"","date":"2019-08-23T04:01","url":"https://www.lesswrong.com/posts/24R8j9WGD36qxGmDJ/vague-thoughts-and-questions-about-agent-structures"},{"x":"11.8902645","y":"9.588226","title":"Formalising decision theory is hard","cluster":"1","author":"['Lanrian']","source":"alignment forum","tags":"Decision Theory","date":"2019-08-23T03:27","url":"https://www.lesswrong.com/posts/S3W4Xrmp6AL7nxRHd/formalising-decision-theory-is-hard"},{"x":"10.237116","y":"9.082512","title":"Creating Environments to Design and Test Embedded Agents","cluster":"4","author":"['lcmgcd']","source":"alignment forum","tags":"","date":"2019-08-23T03:17","url":"https://www.lesswrong.com/posts/qdqYrcGZTh9Lp49Nj/creating-environments-to-design-and-test-embedded-agents"},{"x":"8.702563","y":"7.9093933","title":"Tabooing ‘Agent’ for Prosaic Alignment","cluster":"4","author":"['Hjalmar_Wijk']","source":"alignment forum","tags":"AI/Machine Learning","date":"2019-08-23T02:55","url":"https://www.lesswrong.com/posts/zCcmJzbenAXu6qugS/tabooing-agent-for-prosaic-alignment"},{"x":"10.337851","y":"9.5533695","title":"Vaniver’s View on Factored Cognition","cluster":"4","author":"['Vaniver']","source":"alignment forum","tags":"Factored Cognition/AI","date":"2019-08-23T02:54","url":"https://www.lesswrong.com/posts/J7Rnt8aJPH7MALkmq/vaniver-s-view-on-factored-cognition"},{"x":"8.722911","y":"8.535894","title":"Redefining Fast Takeoff","cluster":"4","author":"['VojtaKovarik']","source":"alignment forum","tags":"","date":"2019-08-23T02:15","url":"https://www.lesswrong.com/posts/cxgtQXnH2uDGBJJGa/redefining-fast-takeoff"},{"x":"10.504643","y":"9.485011","title":"[Question] Does Agent-like Behavior Imply Agent-like Architecture?","cluster":"4","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Consciousness/Goal-Directedness/Agency","date":"2019-08-23T02:01","url":"https://www.lesswrong.com/posts/osxNg6yBCJ4ur9hpi/does-agent-like-behavior-imply-agent-like-architecture"},{"x":"12.09564","y":"9.174337","title":"The Commitment Races problem","cluster":"1","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"Game Theory/AI","date":"2019-08-23T01:58","url":"https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem"},{"x":"11.601592","y":"10.008619","title":"Thoughts from a Two Boxer","cluster":"1","author":"['jaek']","source":"alignment forum","tags":"Newcomb's Problem/Decision Theory","date":"2019-08-23T00:24","url":"https://www.lesswrong.com/posts/9QztnkMiKJ7jYZhL8/thoughts-from-a-two-boxer"},{"x":"10.296405","y":"8.931545","title":"Deconfuse Yourself about Agency","cluster":"4","author":"['VojtaKovarik']","source":"alignment forum","tags":"","date":"2019-08-23T00:21","url":"https://www.lesswrong.com/posts/ZigRhB4pAGdr6beQh/deconfuse-yourself-about-agency"},{"x":"12.280538","y":"9.565329","title":"Logical Optimizers  ","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"","date":"2019-08-22T23:54","url":"https://www.lesswrong.com/posts/ENEYeeyPXyuEQdtZy/logical-optimizers"},{"x":"10.999082","y":"7.8142676","title":"Towards a mechanistic understanding of corrigibility","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"Myopia/Corrigibility/Hansonian Pre-Rationality/AI","date":"2019-08-22T23:20","url":"https://www.lesswrong.com/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility"},{"x":"12.4337225","y":"9.986384","title":"Logical Counterfactuals and Proposition graphs, Part 1","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"Counterfactuals","date":"2019-08-22T22:06","url":"https://www.lesswrong.com/posts/Pxvq2RMAKCuY6SHm9/logical-counterfactuals-and-proposition-graphs-part-1"},{"x":"11.644996","y":"9.972003","title":"Embedded Naive Bayes","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"","date":"2019-08-22T21:40","url":"https://www.lesswrong.com/posts/bKa3JEQYv9sSJ3d2h/embedded-naive-bayes"},{"x":"11.679563","y":"10.110029","title":"Computational Model: Causal Diagrams with Symmetry","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Causality","date":"2019-08-22T17:54","url":"https://www.lesswrong.com/posts/mZy6AMgCw9CPjNCoK/computational-model-causal-diagrams-with-symmetry"},{"x":"8.690406","y":"7.6202154","title":"Implications of Quantum Computing for Artificial Intelligence Alignment Research","cluster":"4","author":"['Jsevillamol', 'PabloAMC']","source":"alignment forum","tags":"Academic Papers/Computer Science/AI/Quantum Mechanics/AI Timelines/Tripwire","date":"2019-08-22T10:33","url":"https://www.lesswrong.com/posts/ZkgqsyWgyDx4ZssqJ/implications-of-quantum-computing-for-artificial"},{"x":"12.62384","y":"9.548496","title":"Markets are Universal for Logical Induction","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Logical Induction/Logical Uncertainty","date":"2019-08-22T06:44","url":"https://www.lesswrong.com/posts/WmNeCipNwg9CmGy3T/markets-are-universal-for-logical-induction"},{"x":"7.6887927","y":"7.4966254","title":"Call for contributors to the Alignment Newsletter","cluster":"2","author":"['Rohin Shah']","source":"alignment forum","tags":"Newsletters/Community","date":"2019-08-21T18:21","url":"https://www.lesswrong.com/posts/cejX4S3Dex3C2dt79/call-for-contributors-to-the-alignment-newsletter"},{"x":"10.526392","y":"7.661969","title":"Two senses of \"optimizer\"","cluster":"4","author":"['Joar Skalse']","source":"alignment forum","tags":"Optimization","date":"2019-08-21T16:02","url":"https://www.lesswrong.com/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer"},{"x":"9.378122","y":"7.5792093","title":"Self-supervised learning & manipulative predictions","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"","date":"2019-08-20T10:55","url":"https://www.lesswrong.com/posts/L3Ryxszc3X2J7WRwt/self-supervised-learning-and-manipulative-predictions"},{"x":"8.974934","y":"8.060875","title":"Goodhart’s Curse and Limitations on AI Alignment","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Goodhart's Law/Optimization","date":"2019-08-19T07:57","url":"https://www.lesswrong.com/posts/NqQxTn5MKEYhSnbuB/goodhart-s-curse-and-limitations-on-ai-alignment"},{"x":"8.909192","y":"9.3740425","title":"Problems in AI Alignment that philosophers could potentially contribute to","cluster":"3","author":"['Wei_Dai']","source":"alignment forum","tags":"AI Risk/Decision Theory/Ethics & Morality/Forecasting & Prediction","date":"2019-08-17T17:38","url":"https://www.lesswrong.com/posts/rASeoR7iZ9Fokzh7L/problems-in-ai-alignment-that-philosophers-could-potentially"},{"x":"9.02538","y":"9.314918","title":"Clarifying some key hypotheses in AI alignment","cluster":"3","author":"['Ben Cottier', 'Rohin Shah']","source":"alignment forum","tags":"AI Risk/AI","date":"2019-08-15T21:29","url":"https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment"},{"x":"11.736414","y":"9.534419","title":"Distance Functions are Hard","cluster":"1","author":"['Grue_Slinky']","source":"alignment forum","tags":"AI/Decision Theory","date":"2019-08-13T17:33","url":"https://www.lesswrong.com/posts/YuJNoCEgeWJfBtdtQ/distance-functions-are-hard-1"},{"x":"9.858787","y":"6.8336945","title":"Mesa-Optimizers and Over-optimization Failure (Optimizing and Goodhart Effects, Clarifying Thoughts—Part 4)","cluster":"0","author":"['Davidmanheim']","source":"alignment forum","tags":"Optimization/Selection vs Control","date":"2019-08-12T08:07","url":"https://www.lesswrong.com/posts/89YgRc3NevJEDEen5/mesa-optimizers-and-over-optimization-failure-optimizing-and"},{"x":"11.583292","y":"10.46829","title":"Verification and Transparency","cluster":"1","author":"['DanielFilan']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI/Computer Science","date":"2019-08-08T01:50","url":"https://www.lesswrong.com/posts/n3YRDJYCnQcDAw29G/verification-and-transparency"},{"x":"11.211221","y":"9.477169","title":"Toy model piece #2: Combining short and long range partial preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-08-08T00:11","url":"https://www.lesswrong.com/posts/gzwM9edW4fXPwJuAW/toy-model-piece-2-combining-short-and-long-range-partial"},{"x":"9.108691","y":"7.437685","title":"Four Ways An Impact Measure Could Help Alignment","cluster":"4","author":"['Matthew Barnett']","source":"alignment forum","tags":"Impact Measures/AI","date":"2019-08-08T00:10","url":"https://www.lesswrong.com/posts/wJK944YqvFwjdbqCP/four-ways-an-impact-measure-could-help-alignment"},{"x":"9.346075","y":"7.9237847","title":"Self-Supervised Learning and AGI Safety","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"Self Fulfilling/Refuting Prophecies/Oracle AI/AI/Machine Learning","date":"2019-08-07T14:21","url":"https://www.lesswrong.com/posts/EMZeJ7vpfeF4GrWwm/self-supervised-learning-and-agi-safety"},{"x":"11.505158","y":"8.422585","title":"Understanding Recent Impact Measures","cluster":"1","author":"['Matthew Barnett']","source":"alignment forum","tags":"Impact Measures/AI","date":"2019-08-07T04:57","url":"https://www.lesswrong.com/posts/pf48kg9xCxJAcHmQc/understanding-recent-impact-measures"},{"x":"8.300725","y":"9.200979","title":"Project Proposal: Considerations for trading off capabilities and safety impacts of AI research","cluster":"3","author":"['capybaralet']","source":"alignment forum","tags":"AI","date":"2019-08-06T22:22","url":"https://www.lesswrong.com/posts/y5fYPAyKjWePCsq3Y/project-proposal-considerations-for-trading-off-capabilities"},{"x":"9.281252","y":"8.827794","title":"New paper: Corrigibility with Utility Preservation","cluster":"4","author":"['Koen.Holtman']","source":"alignment forum","tags":"Academic Papers/Corrigibility","date":"2019-08-06T19:04","url":"https://www.lesswrong.com/posts/3uHgw2uW6BtR74yhQ/new-paper-corrigibility-with-utility-preservation"},{"x":"10.498675","y":"8.34509","title":"A Survey of Early Impact Measures","cluster":"3","author":"['Matthew Barnett']","source":"alignment forum","tags":"Impact Measures/AI","date":"2019-08-06T01:22","url":"https://www.lesswrong.com/posts/TPy4RJvzogqqupDKk/a-survey-of-early-impact-measures"},{"x":"11.044539","y":"9.234321","title":"Preferences as an (instinctive) stance","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-08-06T00:43","url":"https://www.lesswrong.com/posts/f5p7AiDkpkqCyBnBL/preferences-as-an-instinctive-stance"},{"x":"11.02623","y":"8.42127","title":"Practical consequences of impossibility of value learning","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning","date":"2019-08-02T23:06","url":"https://www.lesswrong.com/posts/cnjWN4mzmWzggRnCJ/practical-consequences-of-impossibility-of-value-learning"},{"x":"11.521101","y":"9.20153","title":"Very different, very adequate outcomes","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-08-02T20:31","url":"https://www.lesswrong.com/posts/2HWxzYqo46dve5Ln7/very-different-very-adequate-outcomes"},{"x":"11.126001","y":"9.121129","title":"Why Subagents?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Subagents/Utility Functions","date":"2019-08-01T22:17","url":"https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents"},{"x":"12.852692","y":"9.024946","title":"Contest: $1,000 for good questions to ask to an Oracle AI","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI/AI/Bounties (closed)/AI Boxing (Containment)","date":"2019-07-31T18:48","url":"https://www.lesswrong.com/posts/cSzaxcmeYW6z7cgtc/contest-usd1-000-for-good-questions-to-ask-to-an-oracle-ai"},{"x":"11.342532","y":"9.542075","title":"Toy model piece #1: Partial preferences revisited","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-07-29T16:35","url":"https://www.lesswrong.com/posts/BydQtwfN97pFwEWtW/toy-model-piece-1-partial-preferences-revisited"},{"x":"10.700638","y":"7.800403","title":"Applying Overoptimization to Selection vs. Control (Optimizing and Goodhart Effects—Clarifying Thoughts, Part 3)","cluster":"4","author":"['Davidmanheim']","source":"alignment forum","tags":"Selection vs Control","date":"2019-07-28T09:32","url":"https://www.lesswrong.com/posts/zdeYiQgwYRs2bEmCK/applying-overoptimization-to-selection-vs-control-optimizing"},{"x":"10.911898","y":"7.9419417","title":"What does Optimization Mean, Again?  (Optimizing and Goodhart Effects—Clarifying Thoughts, Part 2)","cluster":"1","author":"['Davidmanheim']","source":"alignment forum","tags":"Goodhart's Law/Selection vs Control","date":"2019-07-28T09:30","url":"https://www.lesswrong.com/posts/BEMvcaeixt3uEqyBk/what-does-optimization-mean-again-optimizing-and-goodhart"},{"x":"10.088462","y":"8.626539","title":"The Artificial Intentional Stance","cluster":"4","author":"['Charlie Steiner']","source":"alignment forum","tags":"AI","date":"2019-07-27T07:00","url":"https://www.lesswrong.com/posts/NvqGmLBCtvQxfMs9m/the-artificial-intentional-stance"},{"x":"10.691711","y":"7.4610944","title":"Just Imitate Humans?","cluster":"4","author":"['michaelcohen']","source":"alignment forum","tags":"","date":"2019-07-27T00:35","url":"https://www.lesswrong.com/posts/LTFaD96D9kWuTibWr/just-imitate-humans"},{"x":"8.574548","y":"7.466469","title":"Ought: why it matters and ways to help","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Ought","date":"2019-07-25T18:00","url":"https://www.lesswrong.com/posts/cpewqG3MjnKJpCr7E/ought-why-it-matters-and-ways-to-help"},{"x":"10.715917","y":"9.480627","title":"On the purposes of decision theory research","cluster":"1","author":"['Wei_Dai']","source":"alignment forum","tags":"Decision Theory/Meta-Philosophy","date":"2019-07-25T07:18","url":"https://www.lesswrong.com/posts/JSjagTDGdz2y6nNE3/on-the-purposes-of-decision-theory-research"},{"x":"9.585873","y":"9.827423","title":"AI Safety Debate and Its Applications","cluster":"4","author":"['VojtaKovarik']","source":"alignment forum","tags":"AI/Debate (AI safety technique)/Open Problems","date":"2019-07-23T22:31","url":"https://www.lesswrong.com/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications"},{"x":"11.417064","y":"9.404672","title":"Normalising utility as willingness to pay","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-07-18T11:44","url":"https://www.lesswrong.com/posts/qudmaMyRuQk2pHxtj/normalising-utility-as-willingness-to-pay"},{"x":"11.431593","y":"9.47302","title":"Intertheoretic utility comparison: examples","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-07-17T12:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753ef/intertheoretic-utility-comparison-examples"},{"x":"10.655898","y":"8.648857","title":"Some Comments on Stuart Armstrong’s \"Research Agenda v0.9\"","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"AI","date":"2019-07-08T19:03","url":"https://www.lesswrong.com/posts/GHNokcgERpLJwJnLW/some-comments-on-stuart-armstrong-s-research-agenda-v0-9"},{"x":"11.363407","y":"8.010577","title":"Indifference: multiple changes, multiple agents","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-07-08T13:36","url":"https://www.lesswrong.com/posts/XkuRKqXKAaMySbXCN/indifference-multiple-changes-multiple-agents"},{"x":"9.527442","y":"8.417204","title":"Musings on Cumulative Cultural Evolution and AI","cluster":"3","author":"['calebo']","source":"alignment forum","tags":"Cultural knowledge/Evolution","date":"2019-07-07T16:46","url":"https://www.lesswrong.com/posts/K686EFdXysfRBdob2/musings-on-cumulative-cultural-evolution-and-ai"},{"x":"11.501899","y":"6.862536","title":"Learning biases and rewards simultaneously","cluster":"0","author":"['Rohin Shah']","source":"alignment forum","tags":"Inverse Reinforcement Learning/Academic Papers/AI","date":"2019-07-06T01:45","url":"https://www.lesswrong.com/posts/xxnPxELC4jLKaFKqG/learning-biases-and-rewards-simultaneously"},{"x":"10.691757","y":"7.97829","title":"Re-introducing Selection vs Control for Optimization (Optimizing and Goodhart Effects—Clarifying Thoughts, Part 1)","cluster":"1","author":"['Davidmanheim']","source":"alignment forum","tags":"Goodhart's Law/Selection vs Control","date":"2019-07-02T15:36","url":"https://www.lesswrong.com/posts/2neeoZ7idRbZf4eNC/re-introducing-selection-vs-control-for-optimization"},{"x":"9.210405","y":"8.453042","title":"An Increasingly Manipulative Newsfeed","cluster":"4","author":"['Michaël Trazzi']","source":"alignment forum","tags":"Deception/Outer Alignment/AI Risk","date":"2019-07-01T15:26","url":"https://www.lesswrong.com/posts/EpdXLNXyL4EYLFwF8/an-increasingly-manipulative-newsfeed"},{"x":"11.769254","y":"8.555165","title":"Conceptual Problems with UDT and Policy Selection","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory","date":"2019-06-28T23:50","url":"https://www.lesswrong.com/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection"},{"x":"9.895499","y":"7.3868566","title":"Aligning a toy model of optimization","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Optimization/AI","date":"2019-06-28T20:23","url":"https://www.lesswrong.com/posts/H5gXpFtg93qDMZ6Xn/aligning-a-toy-model-of-optimization"},{"x":"12.916622","y":"9.087748","title":"Self-confirming prophecies, and simplified Oracle designs","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI/AI/AI Boxing (Containment)","date":"2019-06-28T09:57","url":"https://www.lesswrong.com/posts/wJ3AqNPM7W4nfY5Bk/self-confirming-prophecies-and-simplified-oracle-designs"},{"x":"10.06261","y":"7.0759654","title":"False assumptions and leaky abstractions in machine learning and AI safety","cluster":"4","author":"['capybaralet']","source":"alignment forum","tags":"","date":"2019-06-28T04:54","url":"https://www.lesswrong.com/posts/K4QSzpN4ytZ4iqkze/false-assumptions-and-leaky-abstractions-in-machine-learning"},{"x":"9.538159","y":"9.416935","title":"Research Agenda in reverse: what *would* a solution look like?","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Research Agendas/Goal Factoring","date":"2019-06-25T13:52","url":"https://www.lesswrong.com/posts/TR3eqQ2fnfKWzxxHL/research-agenda-in-reverse-what-would-a-solution-look-like"},{"x":"9.000814","y":"7.012253","title":"Machine Learning Projects on IDA","cluster":"2","author":"['Owain_Evans', 'William_S', 'stuhlmueller']","source":"alignment forum","tags":"Research Agendas/Iterated Amplification /Machine Learning","date":"2019-06-24T18:38","url":"https://www.lesswrong.com/posts/Y9xD78kufNsF7wL6f/machine-learning-projects-on-ida"},{"x":"9.516877","y":"9.603355","title":"Modeling AGI Safety Frameworks with Causal Influence Diagrams","cluster":"3","author":"['Ramana Kumar']","source":"alignment forum","tags":"AI Risk/Causality/AI","date":"2019-06-21T12:50","url":"https://www.lesswrong.com/posts/HE5DL6XeomYxFab74/modeling-agi-safety-frameworks-with-causal-influence-1"},{"x":"8.849979","y":"8.660515","title":"1hr talk: Intro to AGI safety","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI Risk/Transcripts","date":"2019-06-18T21:41","url":"https://www.lesswrong.com/posts/DbZDdupuffc4Xgm7H/1hr-talk-intro-to-agi-safety"},{"x":"10.775223","y":"7.2176633","title":"Research Agenda v0.9: Synthesising a human’s preferences into a utility function","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Research Agendas/Utility Functions","date":"2019-06-17T17:46","url":"https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into"},{"x":"11.511112","y":"9.361359","title":"Preference conditional on circumstances and past preference satisfaction","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-06-17T15:30","url":"https://www.lesswrong.com/posts/uHb2LDW3LGhBMyq74/preference-conditional-on-circumstances-and-past-preference"},{"x":"9.886731","y":"8.072905","title":"Let’s talk about \"Convergent Rationality\"","cluster":"4","author":"['capybaralet']","source":"alignment forum","tags":"Instrumental Convergence/AI Risk","date":"2019-06-12T21:53","url":"https://www.lesswrong.com/posts/pLZ3bdeng4u5W8Yft/let-s-talk-about-convergent-rationality-1"},{"x":"8.103763","y":"8.8314705","title":"AGI will drastically increase economies of scale","cluster":"3","author":"['Wei_Dai']","source":"alignment forum","tags":"AI Takeoff/AI Timelines/Futurism","date":"2019-06-07T23:17","url":"https://www.lesswrong.com/posts/Sn5NiiD5WBi4dLzaB/agi-will-drastically-increase-economies-of-scale"},{"x":"9.700905","y":"6.564045","title":"Risks from Learned Optimization: Conclusion and Related Work","cluster":"0","author":"['evhub', 'Chris van Merwijk', 'vlad_m', 'Joar Skalse', 'Scott Garrabrant']","source":"alignment forum","tags":"Mesa-Optimization/AI/AI Risk","date":"2019-06-07T19:53","url":"https://www.lesswrong.com/posts/4XPa3xa44jAWiCkmy/risks-from-learned-optimization-conclusion-and-related-work"},{"x":"10.989891","y":"9.405927","title":"For the past, in some ways only, we are moral degenerates","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Moral Uncertainty/Ethics & Morality/Progress Studies","date":"2019-06-07T15:57","url":"https://www.lesswrong.com/posts/SDr45pcgJJyvTqmZa/for-the-past-in-some-ways-only-we-are-moral-degenerates"},{"x":"9.922849","y":"6.5997047","title":"Deceptive Alignment","cluster":"0","author":"['evhub', 'Chris van Merwijk', 'vlad_m', 'Joar Skalse', 'Scott Garrabrant']","source":"alignment forum","tags":"Mesa-Optimization/AI/AI Risk","date":"2019-06-05T20:16","url":"https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment"},{"x":"9.611509","y":"6.6546206","title":"The Inner Alignment Problem","cluster":"0","author":"['evhub', 'Chris van Merwijk', 'vlad_m', 'Joar Skalse', 'Scott Garrabrant']","source":"alignment forum","tags":"Inner Alignment/Mesa-Optimization/AI/AI Risk","date":"2019-06-04T01:20","url":"https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem"},{"x":"11.025153","y":"9.495129","title":"To first order, moral realism and moral anti-realism are the same thing","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-06-03T15:04","url":"https://www.lesswrong.com/posts/FQKjY563bJqDeaEDr/to-first-order-moral-realism-and-moral-anti-realism-are-the"},{"x":"11.254372","y":"8.9607315","title":"Conditional meta-preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-06-03T14:09","url":"https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences"},{"x":"11.468085","y":"8.406007","title":"Does Bayes Beat Goodhart?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Goodhart's Law","date":"2019-06-03T02:31","url":"https://www.lesswrong.com/posts/YJq6R9Wgk5Atjx54D/does-bayes-beat-goodhart"},{"x":"10.563808","y":"7.8276744","title":"Selection vs Control","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Optimization/Adaptation Executors/Selection vs Control/Distinctions","date":"2019-06-02T07:01","url":"https://www.lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control"},{"x":"9.485947","y":"6.5972466","title":"Conditions for Mesa-Optimization","cluster":"0","author":"['evhub', 'Chris van Merwijk', 'vlad_m', 'Joar Skalse', 'Scott Garrabrant']","source":"alignment forum","tags":"Mesa-Optimization/AI/AI Risk","date":"2019-06-01T20:52","url":"https://www.lesswrong.com/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization"},{"x":"9.238413","y":"6.673209","title":"Risks from Learned Optimization: Introduction","cluster":"2","author":"['evhub', 'Chris van Merwijk', 'vlad_m', 'Joar Skalse', 'Scott Garrabrant']","source":"alignment forum","tags":"AI/Outer Alignment/Mesa-Optimization/Inner Alignment/Optimization/AI Risk","date":"2019-05-31T23:44","url":"https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction"},{"x":"11.305369","y":"9.402348","title":"Uncertainty versus fuzziness versus extrapolation desiderata","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-05-30T13:52","url":"https://www.lesswrong.com/posts/QJwnPRBBvgaeFeiLR/uncertainty-versus-fuzziness-versus-extrapolation-desiderata"},{"x":"8.618296","y":"9.618578","title":"A shift in arguments for AI risk","cluster":"3","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI Risk","date":"2019-05-28T13:47","url":"https://www.lesswrong.com/posts/hubbRt4DPegiA5gRR/a-shift-in-arguments-for-ai-risk"},{"x":"8.981506","y":"9.959678","title":"And the AI would have got away with it too, if...","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"History/AI Risk","date":"2019-05-22T21:35","url":"https://www.lesswrong.com/posts/92J4zJHkqmXTduxzY/and-the-ai-would-have-got-away-with-it-too-if"},{"x":"11.167554","y":"9.389366","title":"By default, avoid ambiguous distant situations","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Ethics & Morality/World Modeling","date":"2019-05-21T14:48","url":"https://www.lesswrong.com/posts/PX8BB7Rqw7HedrSJd/by-default-avoid-ambiguous-distant-situations"},{"x":"8.638914","y":"9.703436","title":"[Question] Would an option to publish to AF users only be a useful feature?","cluster":"3","author":"['Richard_Ngo']","source":"alignment forum","tags":"","date":"2019-05-20T11:04","url":"https://www.lesswrong.com/posts/DbuntLhCeuPJmGT5M/would-an-option-to-publish-to-af-users-only-be-a-useful"},{"x":"12.198245","y":"9.241521","title":"\"UDT2\" and \"against UD+ASSA\"","cluster":"1","author":"['Wei_Dai']","source":"alignment forum","tags":"Updateless Decision Theory/Decision Theory/AI","date":"2019-05-12T04:18","url":"https://www.lesswrong.com/posts/zd2DrbHApWypJD2Rz/udt2-and-against-ud-assa"},{"x":"10.851644","y":"8.301097","title":"Training human models is an unsolved problem","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Value Learning","date":"2019-05-10T07:17","url":"https://www.lesswrong.com/posts/upP8PYgHfXgvgh3FF/training-human-models-is-an-unsolved-problem"},{"x":"12.659784","y":"9.265769","title":"Not Deceiving the Evaluator","cluster":"1","author":"['michaelcohen']","source":"alignment forum","tags":"","date":"2019-05-08T05:37","url":"https://www.lesswrong.com/posts/kgz6dzFETmpoRiwmm/not-deceiving-the-evaluator"},{"x":"12.863699","y":"9.074517","title":"Oracles, sequence predictors, and self-confirming predictions","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI/AI/AI Boxing (Containment)","date":"2019-05-03T14:09","url":"https://www.lesswrong.com/posts/i2dNFgbjnqZBfeitT/oracles-sequence-predictors-and-self-confirming-predictions"},{"x":"11.251927","y":"8.890113","title":"Self-confirming predictions can be arbitrarily bad","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Newcomb's Problem/AI/Fixed Point Theorems","date":"2019-05-03T11:34","url":"https://www.lesswrong.com/posts/KoEY9CjrKe93ErYhd/self-confirming-predictions-can-be-arbitrarily-bad"},{"x":"12.181484","y":"8.401504","title":"Nash equilibriums can be arbitrarily bad","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Game Theory","date":"2019-05-01T14:58","url":"https://www.lesswrong.com/posts/jz5QoizH8HkQwWZ9Q/nash-equilibriums-can-be-arbitrarily-bad"},{"x":"9.049204","y":"9.265711","title":"Strategic implications of AIs’ ability to coordinate at low cost, for example by merging","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"AI Risk/Futurism","date":"2019-04-25T05:08","url":"https://www.lesswrong.com/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low"},{"x":"10.763335","y":"8.777609","title":"Corrigibility as Constrained Optimisation","cluster":"1","author":"['Henrik Åslund']","source":"alignment forum","tags":"Corrigibility","date":"2019-04-11T20:09","url":"https://www.lesswrong.com/posts/cGLgs3t9md7v7cCm4/corrigibility-as-constrained-optimisation"},{"x":"9.079009","y":"9.638305","title":"[Question] Best reasons for pessimism about impact of impact measures?","cluster":"3","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures","date":"2019-04-10T17:22","url":"https://www.lesswrong.com/posts/kCY9dYGLoThC3aG7w/best-reasons-for-pessimism-about-impact-of-impact-measures"},{"x":"7.720542","y":"7.6654468","title":"Alignment Newsletter One Year Retrospective","cluster":"2","author":"['Rohin Shah']","source":"alignment forum","tags":"Postmortems & Retrospectives/Newsletters/AI","date":"2019-04-10T06:58","url":"https://www.lesswrong.com/posts/3onCb5ph3ywLQZMX2/alignment-newsletter-one-year-retrospective"},{"x":"10.994296","y":"8.435839","title":"Value Learning is only Asymptotically Safe","cluster":"1","author":"['michaelcohen']","source":"alignment forum","tags":"","date":"2019-04-08T09:45","url":"https://www.lesswrong.com/posts/NjhyEej7RK8rmQNP2/value-learning-is-only-asymptotically-safe"},{"x":"11.565253","y":"6.664538","title":"Reinforcement learning with imperceptible rewards","cluster":"0","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Reinforcement Learning","date":"2019-04-07T10:27","url":"https://www.lesswrong.com/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards"},{"x":"11.294372","y":"8.611774","title":"Defeating Goodhart and the \"closest unblocked strategy\" problem","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Goodhart's Law","date":"2019-04-03T14:46","url":"https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"},{"x":"11.6866455","y":"6.024654","title":"Learning \"known\" information when the information is not actually known","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-04-01T17:56","url":"https://www.lesswrong.com/posts/TrudRcZxEjA2RNCxh/learning-known-information-when-the-information-is-not"},{"x":"11.251038","y":"9.414601","title":"Relative exchange rate between preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-29T11:46","url":"https://www.lesswrong.com/posts/NS3Kk9WJBnEauzgCr/relative-exchange-rate-between-preferences"},{"x":"10.764239","y":"9.670062","title":"Being wrong in ethics","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-29T11:28","url":"https://www.lesswrong.com/posts/am5ubSoSe6Hf5tnTF/being-wrong-in-ethics"},{"x":"11.235508","y":"9.359324","title":"Models of preferences in distant situations","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-29T10:42","url":"https://www.lesswrong.com/posts/WAmpz8Z4FZ4FbCNtR/models-of-preferences-in-distant-situations"},{"x":"11.423865","y":"9.27625","title":"The low cost of human preference incoherence","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-27T11:58","url":"https://www.lesswrong.com/posts/FgRRY3vSwDLx2Hk46/the-low-cost-of-human-preference-incoherence"},{"x":"8.6833515","y":"9.966075","title":"Unsolved research problems vs. real-world threat models","cluster":"3","author":"['catherio']","source":"alignment forum","tags":"","date":"2019-03-26T22:10","url":"https://www.lesswrong.com/posts/6shTnnLdZynmRG9rX/unsolved-research-problems-vs-real-world-threat-models"},{"x":"8.97437","y":"7.0426984","title":"A Concrete Proposal for Adversarial IDA","cluster":"2","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2019-03-26T19:50","url":"https://www.lesswrong.com/posts/jYvm4mmjvGHcPXtGL/a-concrete-proposal-for-adversarial-ida"},{"x":"11.134073","y":"9.36571","title":"\"Moral\" as a preference label","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-26T10:30","url":"https://www.lesswrong.com/posts/BQzPyBjahycCN24dR/moral-as-a-preference-label"},{"x":"9.265881","y":"7.6934686","title":"Understanding Iterated Distillation and Amplification: Claims and Oversight","cluster":"4","author":"['William_S']","source":"alignment forum","tags":"Iterated Amplification ","date":"2018-04-17T22:36","url":"https://www.lesswrong.com/posts/yxzrKb2vFXRkwndQ4/understanding-iterated-distillation-and-amplification-claims"},{"x":"11.8967085","y":"7.88887","title":"Computing an exact quantilal policy","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2018-04-12T09:23","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037555f/computing-an-exact-quantilal-policy"},{"x":"11.951803","y":"7.6294518","title":"Quantilal control for finite MDPs","cluster":"0","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"AI/Mild Optimization","date":"2018-04-12T09:21","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375556/quantilal-control-for-finite-mdps"},{"x":"12.718139","y":"10.133335","title":"No Constant Distribution Can be a Logical Inductor","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-04-07T09:09","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037555d/no-constant-distribution-can-be-a-logical-inductor"},{"x":"9.307052","y":"8.520619","title":"Specification gaming examples in AI","cluster":"4","author":"['Vika']","source":"alignment forum","tags":"Goodhart's Law/AI Risk/AI","date":"2018-04-03T12:30","url":"https://www.lesswrong.com/posts/AanbbjYr5zckMKde7/specification-gaming-examples-in-ai-1"},{"x":"12.148172","y":"9.70304","title":"Musings on Exploration","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-04-03T02:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375557/musings-on-exploration"},{"x":"8.22302","y":"7.2963324","title":"Can corrigibility be learned safely?","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"Corrigibility","date":"2018-04-01T23:07","url":"https://www.lesswrong.com/posts/o22kP33tumooBtia3/can-corrigibility-be-learned-safely"},{"x":"10.507623","y":"9.795653","title":"My take on agent foundations: formalizing metaphilosophical competence","cluster":"4","author":"['zhukeepa']","source":"alignment forum","tags":"Meta-Philosophy/AI/Agent Foundations","date":"2018-04-01T06:33","url":"https://www.lesswrong.com/posts/xCpuSfT5Lt6kkR3po/my-take-on-agent-foundations-formalizing-metaphilosophical"},{"x":"9.3710575","y":"8.918085","title":"Evaluating Existing Approaches to AGI Alignment","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"AI","date":"2018-03-27T19:57","url":"https://www.lesswrong.com/posts/RMhs2fXtK5hLAjDQv/evaluating-existing-approaches-to-agi-alignment"},{"x":"9.037886","y":"9.695376","title":"Non-Adversarial Goodhart and AI Risks","cluster":"3","author":"['Davidmanheim']","source":"alignment forum","tags":"Goodhart's Law/AI Risk","date":"2018-03-27T01:39","url":"https://www.lesswrong.com/posts/iK2F9QDZvwWinsBYB/non-adversarial-goodhart-and-ai-risks"},{"x":"7.989375","y":"9.284968","title":"Idea: Open Access AI Safety Journal","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"AI Risk/Community/AI","date":"2018-03-23T18:27","url":"https://www.lesswrong.com/posts/a65sFvymnoLkBnE8n/idea-open-access-ai-safety-journal"},{"x":"12.820384","y":"8.711775","title":"Distributed Cooperation","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-03-18T05:46","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037554e/distributed-cooperation"},{"x":"11.564127","y":"6.3985624","title":"Rigged reward learning","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2018-03-16T15:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703752a8/rigged-reward-learning"},{"x":"8.795758","y":"7.849345","title":"Avoiding AI Races Through Self-Regulation","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2018-03-12T20:53","url":"https://www.lesswrong.com/posts/L9kgWYFYYksiXdTqZ/avoiding-ai-races-through-self-regulation"},{"x":"8.17956","y":"8.316367","title":"Prize for probable problems","cluster":"3","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification /Bounties (closed)/Mesa-Optimization","date":"2018-03-08T16:58","url":"https://www.lesswrong.com/posts/SqcPWvvJJwwgZb6aH/prize-for-probable-problems"},{"x":"8.026193","y":"9.65783","title":"Self-regulation of safety in AI research","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Regulation and AI Risk/AI/AI Governance","date":"2018-02-25T23:17","url":"https://www.lesswrong.com/posts/Z8BWP6CEQuARcbNZu/self-regulation-of-safety-in-ai-research"},{"x":"11.986215","y":"8.795059","title":"Passing Troll Bridge","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-02-25T08:21","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375546/passing-troll-bridge"},{"x":"7.9220705","y":"9.223445","title":"Arguments about fast takeoff","cluster":"3","author":"['paulfchristiano']","source":"alignment forum","tags":"AI Takeoff","date":"2018-02-25T04:53","url":"https://www.lesswrong.com/posts/AfGmsjGPXN97kNp57/arguments-about-fast-takeoff"},{"x":"8.966086","y":"8.334762","title":"Robustness to Scale","cluster":"4","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Robust Agents","date":"2018-02-21T22:55","url":"https://www.lesswrong.com/posts/bBdfbWfWxHN9Chjcq/robustness-to-scale"},{"x":"8.8733015","y":"9.387905","title":"Formally Stating the AI Alignment Problem","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2018-02-19T19:06","url":"https://www.lesswrong.com/posts/7dvDgqvqqziSKweRs/formally-stating-the-ai-alignment-problem-1"},{"x":"12.177612","y":"10.108499","title":"Toward a New Technical Explanation of Technical Explanation","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Logical Induction/Gears-Level/Inside/Outside View/Epistemology/Logical Uncertainty/Problem of Old Evidence","date":"2018-02-16T00:44","url":"https://www.lesswrong.com/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation"},{"x":"12.028559","y":"9.250894","title":"Two Types of Updatelessness","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory","date":"2018-02-15T20:19","url":"https://www.lesswrong.com/posts/pneKTZG9KqnSe2RdQ/two-types-of-updatelessness"},{"x":"12.097327","y":"7.820924","title":"More precise regret bound for DRL","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2018-02-14T11:58","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375528/more-precise-regret-bound-for-drl"},{"x":"10.749056","y":"8.622339","title":"Stable Pointers to Value II: Environmental Goals","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"The Pointers Problem/AI/Value Learning","date":"2018-02-09T06:03","url":"https://www.lesswrong.com/posts/wujPGixayiZSMYfm6/stable-pointers-to-value-ii-environmental-goals"},{"x":"11.866219","y":"9.979511","title":"Knowledge is Freedom","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Bounded Rationality/World Modeling/Decision Theory","date":"2018-02-09T05:24","url":"https://www.lesswrong.com/posts/b3Bt9Cz4hEtR26ANX/knowledge-is-freedom"},{"x":"12.183615","y":"8.513057","title":"UDT as a Nash Equilibrium","cluster":"1","author":"['cousin_it']","source":"alignment forum","tags":"Updateless Decision Theory/Decision Theory/Game Theory","date":"2018-02-06T14:08","url":"https://www.lesswrong.com/posts/6HmaGnXd4EJfpfait/udt-as-a-nash-equilibrium"},{"x":"12.771322","y":"9.199534","title":"Logical counterfactuals and differential privacy","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Counterfactuals","date":"2018-02-04T00:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375532/logical-counterfactuals-and-differential-privacy"},{"x":"12.68334","y":"10.280518","title":"Further Progress on a Bayesian Version of Logical Uncertainty","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-02-01T21:36","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037553d/further-progress-on-a-bayesian-version-of-logical-uncertainty"},{"x":"8.775412","y":"8.216808","title":"Sources of intuitions and data on AGI","cluster":"4","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Intuition","date":"2018-01-31T23:30","url":"https://www.lesswrong.com/posts/BibDWWeo37pzuZCmL/sources-of-intuitions-and-data-on-agi"},{"x":"12.66348","y":"10.283596","title":"All Mathematicians are Trollable: Divergence of Naturalistic Logical Updates","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2018-01-28T14:50","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037518c/all-mathematicians-are-trollable-divergence-of-naturalistic-logical-updates"},{"x":"12.821915","y":"8.738805","title":"Strategy Nonconvexity Induced by a Choice of Potential Oracles","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Oracle AI","date":"2018-01-27T00:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037553c/strategy-nonconvexity-induced-by-a-choice-of-potential-oracles"},{"x":"12.59599","y":"10.298795","title":"An Untrollable Mathematician","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2018-01-23T18:46","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375533/an-untrollable-mathematician"},{"x":"8.798081","y":"7.635585","title":"Beware of black boxes in AI alignment research","cluster":"4","author":"['cousin_it']","source":"alignment forum","tags":"AI Risk/Gears-Level","date":"2018-01-18T15:07","url":"https://www.lesswrong.com/posts/DNKTmmNZr5M2uCZLz/beware-of-black-boxes-in-ai-alignment-research"},{"x":"8.4743185","y":"6.701894","title":"The Three Levels of Goodhart’s Curse","cluster":"2","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Goodhart's Law","date":"2017-12-30T16:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754b2/the-three-levels-of-goodhart-s-curse"},{"x":"11.400651","y":"8.730479","title":"Goodhart Taxonomy","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Goodhart's Law/AI","date":"2017-12-30T16:38","url":"https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy"},{"x":"12.564254","y":"9.847891","title":"Being legible to other agents by committing to using weaker reasoning systems","cluster":"1","author":"['AlexMennen']","source":"alignment forum","tags":"","date":"2017-12-03T07:49","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375523/being-legible-to-other-agents-by-committing-to-using-weaker-reasoning-systems"},{"x":"12.571341","y":"9.270108","title":"Policy Selection Solves Most Problems","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-12-01T00:35","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037550c/policy-selection-solves-most-problems"},{"x":"11.842363","y":"7.586425","title":"Catastrophe Mitigation Using DRL","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2017-11-22T05:54","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375510/catastrophe-mitigation-using-drl"},{"x":"12.332838","y":"8.926201","title":"Where does ADT Go Wrong?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-11-17T23:31","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375512/where-does-adt-go-wrong"},{"x":"12.154892","y":"9.685611","title":"The Happy Dance Problem","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-11-17T00:50","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037550e/the-happy-dance-problem"},{"x":"12.342446","y":"8.99819","title":"XOR Blackmail & Causality","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2017-11-15T04:24","url":"https://www.lesswrong.com/posts/zFX8jFLcREJATshdK/xor-blackmail-and-causality"},{"x":"9.637957","y":"9.438028","title":"Towards an Axiological Approach to AI Alignment","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2017-11-15T02:07","url":"https://www.lesswrong.com/posts/NwxjvegAbLaBJ3TvC/towards-an-axiological-approach-to-ai-alignment-1"},{"x":"12.011","y":"9.297338","title":"Logical Updatelessness as a Robust Delegation Problem","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Embedded Agency/Decision Theory/AI","date":"2017-10-27T21:16","url":"https://www.lesswrong.com/posts/K5Qp7ioupgb7r73Ca/logical-updatelessness-as-a-robust-delegation-problem"},{"x":"11.555962","y":"9.9257965","title":"Bayesian Utility: Representing Preference by Probability Measures","cluster":"1","author":"['Vladimir_Nesov']","source":"alignment forum","tags":"Logic & Mathematics /AI/Bayes' Theorem/Utility Functions","date":"2009-07-27T14:28","url":"https://www.lesswrong.com/posts/kYgWmKJnqq8QkbjFj/bayesian-utility-representing-preference-by-probability"},{"x":"9.106823","y":"8.777277","title":"Cryptographic Boxes for Unfriendly AI","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI Boxing (Containment)/Computer Security & Cryptography","date":"2010-12-18T08:28","url":"https://www.lesswrong.com/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai"},{"x":"12.218713","y":"9.246593","title":"A Rationality Condition for CDT Is That It Equal EDT (Part 2)","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2018-10-09T05:41","url":"https://www.lesswrong.com/posts/tpWfDLZy2tk97MJ3F/a-rationality-condition-for-cdt-is-that-it-equal-edt-part-2"},{"x":"11.986398","y":"9.515587","title":"A Rationality Condition for CDT Is That It Equal EDT (Part 1)","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory","date":"2018-10-04T04:32","url":"https://www.lesswrong.com/posts/XW6Qi2LitMDb2MF8c/a-rationality-condition-for-cdt-is-that-it-equal-edt-part-1"},{"x":"9.310489","y":"8.346987","title":"The Rocket Alignment Problem","cluster":"4","author":"['Eliezer Yudkowsky']","source":"alignment forum","tags":"AI/Machine Intelligence Research Institute (MIRI)/Dialogue (format)/Fiction","date":"2018-10-04T00:38","url":"https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem"},{"x":"12.642436","y":"9.1924715","title":"EDT solves 5 and 10 with conditional oracles","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Decision Theory/AI","date":"2018-09-30T07:57","url":"https://www.lesswrong.com/posts/Rcwv6SPsmhkgzfkDw/edt-solves-5-and-10-with-conditional-oracles"},{"x":"12.323377","y":"8.924083","title":"Asymptotic Decision Theory (Improved Writeup)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Decision Theory/Logical Induction","date":"2018-09-27T05:17","url":"https://www.lesswrong.com/posts/yXCvYqTZCsfN7WRrg/asymptotic-decision-theory-improved-writeup"},{"x":"11.3906555","y":"8.349306","title":"Wireheading as a potential problem with the new impact measure","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Wireheading","date":"2018-09-25T14:15","url":"https://www.lesswrong.com/posts/6EMdmeosYPdn74wuG/wireheading-as-a-potential-problem-with-the-new-impact"},{"x":"10.692139","y":"10.207349","title":"Bridging syntax and semantics with Quine’s Gavagai","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2018-09-24T14:39","url":"https://www.lesswrong.com/posts/XApNuXPckPxwp5ZcW/bridging-syntax-and-semantics-with-quine-s-gavagai"},{"x":"11.988473","y":"9.221556","title":"Reflective AIXI and Anthropics","cluster":"4","author":"['Diffractor']","source":"alignment forum","tags":"AI/AIXI/Solomonoff Induction/Anthropics","date":"2018-09-24T02:15","url":"https://www.lesswrong.com/posts/K8FTuEdAbsHDDw3hR/reflective-aixi-and-anthropics"},{"x":"10.780424","y":"7.9871197","title":"Impact Measure Testing with Honey Pots and Myopia","cluster":"4","author":"['michaelcohen']","source":"alignment forum","tags":"","date":"2018-09-21T15:26","url":"https://www.lesswrong.com/posts/2R8XqCt6f6Ss2TzwM/impact-measure-testing-with-honey-pots-and-myopia"},{"x":"11.89294","y":"9.474768","title":"In Logical Time, All Games are Iterated Games","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Game Theory/Logical Induction/Logical Uncertainty","date":"2018-09-20T02:01","url":"https://www.lesswrong.com/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games"},{"x":"10.871852","y":"10.230904","title":"Bridging syntax and semantics, empirically","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Truth, Semantics, & Meaning","date":"2018-09-19T16:48","url":"https://www.lesswrong.com/posts/EEPdbtvW8ei9Yi2e8/bridging-syntax-and-semantics-empirically"},{"x":"10.979281","y":"9.999343","title":"Web of connotations: Bleggs, Rubes, thermostats and beliefs","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2018-09-19T16:47","url":"https://www.lesswrong.com/posts/ix3KdfJxjo9GQFkCo/web-of-connotations-bleggs-rubes-thermostats-and-beliefs"},{"x":"11.17895","y":"8.258079","title":"Towards a New Impact Measure","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures","date":"2018-09-18T17:21","url":"https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure"},{"x":"10.888005","y":"9.618978","title":"Realism about rationality","cluster":"1","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/Rationality/Law-Thinking","date":"2018-09-16T10:46","url":"https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality"},{"x":"11.03497","y":"9.995123","title":"(A → B) → A","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Decision Theory/AI/Embedded Agency","date":"2018-09-11T22:38","url":"https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a"},{"x":"8.379626","y":"9.40875","title":"Petrov corrigibility","cluster":"3","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Petrov Day/Corrigibility","date":"2018-09-11T13:50","url":"https://www.lesswrong.com/posts/4g29JgtbJ283iJ3Bh/petrov-corrigibility"},{"x":"10.182012","y":"8.010081","title":"Boltzmann brain decision theory","cluster":"2","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2018-09-11T13:24","url":"https://www.lesswrong.com/posts/ZvmicfmGg9LWBvy2D/boltzmann-brain-decision-theory"},{"x":"10.28347","y":"8.808658","title":"Disagreement with Paul: alignment induction","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Iterated Amplification /AI","date":"2018-09-10T13:54","url":"https://www.lesswrong.com/posts/h3Fp3Erddwnm5uthZ/disagreement-with-paul-alignment-induction"},{"x":"10.355982","y":"9.606162","title":"Comment on decision theory","cluster":"1","author":"['Rob Bensinger']","source":"alignment forum","tags":"Decision Theory","date":"2018-09-09T20:13","url":"https://www.lesswrong.com/posts/uKbxi2EJ3KBNRDGpL/comment-on-decision-theory"},{"x":"12.765133","y":"9.197516","title":"Counterfactuals and reflective oracles","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Oracle AI/Counterfactuals","date":"2018-09-05T08:54","url":"https://www.lesswrong.com/posts/pgTioHEzaSddx5csN/counterfactuals-and-reflective-oracles"},{"x":"11.100008","y":"8.222358","title":"Impact Measure Desiderata","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures","date":"2018-09-02T22:21","url":"https://www.lesswrong.com/posts/c2oM7qytRByv6ZFtz/impact-measure-desiderata"},{"x":"11.618955","y":"9.666137","title":"When wishful thinking works","cluster":"1","author":"['AlexMennen']","source":"alignment forum","tags":"Logic & Mathematics /Fixed Point Theorems","date":"2018-09-01T23:43","url":"https://www.lesswrong.com/posts/KbCHcb8yyjAMFAAPJ/when-wishful-thinking-works"},{"x":"12.925386","y":"8.960907","title":"Cooperative Oracles","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Oracle AI","date":"2018-09-01T08:05","url":"https://www.lesswrong.com/posts/SgkaXQn3xqJkGQ2D8/cooperative-oracles"},{"x":"10.44527","y":"7.204104","title":"Bottle Caps Aren’t Optimisers","cluster":"0","author":"['DanielFilan']","source":"alignment forum","tags":"Optimization","date":"2018-08-31T18:30","url":"https://www.lesswrong.com/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers"},{"x":"11.538108","y":"8.972657","title":"VOI is Only Nonnegative When Information is Uncorrelated With Future Action","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Value of Information","date":"2018-08-31T05:13","url":"https://www.lesswrong.com/posts/KER27SxZssfmsusxy/voi-is-only-nonnegative-when-information-is-uncorrelated"},{"x":"10.642793","y":"7.148862","title":"Do what we mean vs. do what we say","cluster":"0","author":"['Rohin Shah']","source":"alignment forum","tags":"Corrigibility/AI","date":"2018-08-30T22:03","url":"https://www.lesswrong.com/posts/8Q5h6hyBXTEgC6EZf/do-what-we-mean-vs-do-what-we-say"},{"x":"11.087593","y":"6.7219305","title":"Computational complexity of RL with traps","cluster":"0","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2018-08-29T09:17","url":"https://www.lesswrong.com/posts/3YYChdX29SMG6kQf6/computational-complexity-of-rl-with-traps"},{"x":"12.446468","y":"10.260997","title":"History of the Development of Logical Induction","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Logical Induction/Postmortems & Retrospectives/AI/Productivity/Logical Uncertainty/History of Rationality","date":"2018-08-29T03:15","url":"https://www.lesswrong.com/posts/iBBK4j6RWC7znEiDv/history-of-the-development-of-logical-induction"},{"x":"11.29514","y":"8.567573","title":"Using expected utility for Good(hart)","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Goodhart's Law/Utility Functions","date":"2018-08-27T03:32","url":"https://www.lesswrong.com/posts/urZzJPwHtjewdKKHc/using-expected-utility-for-good-hart"},{"x":"12.220573","y":"8.289137","title":"Reducing collective rationality to individual optimization in common-payoff games using MCMC","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Game Theory/Logic & Mathematics /Coordination / Cooperation/Group Rationality","date":"2018-08-20T00:51","url":"https://www.lesswrong.com/posts/JKSS8GEu7DGX4YuxN/reducing-collective-rationality-to-individual-optimization"},{"x":"9.628803","y":"9.26053","title":"A developmentally-situated approach to teaching normative behavior to AI","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2018-08-17T18:44","url":"https://www.lesswrong.com/posts/uEAvtbtEBdsQJMdh8/a-developmentally-situated-approach-to-teaching-normative"},{"x":"11.757459","y":"7.2333403","title":"Entropic Regret I: Deterministic MDPs","cluster":"0","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2018-08-16T13:08","url":"https://www.lesswrong.com/posts/zTf946PQwN2AN3X3Y/entropic-regret-i-deterministic-mdps"},{"x":"12.267566","y":"9.354078","title":"Logical Counterfactuals & the Cooperation Game","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Counterfactuals","date":"2018-08-14T14:00","url":"https://www.lesswrong.com/posts/NcA3dMJoWWEN4BQet/logical-counterfactuals-and-the-cooperation-game"},{"x":"12.791286","y":"9.5525875","title":"Probabilistic Tiling (Preliminary Attempt)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-08-07T01:14","url":"https://www.lesswrong.com/posts/nsbKeodxHJFKX2yYp/probabilistic-tiling-preliminary-attempt"},{"x":"9.341221","y":"8.5865","title":"Safely and usefully spectating on AIs optimizing over toy worlds","cluster":"4","author":"['AlexMennen']","source":"alignment forum","tags":"Inner Alignment/AI Boxing (Containment)/AI","date":"2018-07-31T18:30","url":"https://www.lesswrong.com/posts/ikN9qQEkrFuPtYd6Y/safely-and-usefully-spectating-on-ais-optimizing-over-toy"},{"x":"12.020569","y":"9.583177","title":"Counterfactuals, thick and thin","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Counterfactuals","date":"2018-07-31T15:43","url":"https://www.lesswrong.com/posts/YSH3RFSFESzsa5Nrg/counterfactuals-thick-and-thin"},{"x":"10.280973","y":"7.339682","title":"A Gym Gridworld Environment for the  Treacherous Turn","cluster":"0","author":"['Michaël Trazzi']","source":"alignment forum","tags":"Programming/Treacherous Turn/AI Risk/Corrigibility/Instrumental Convergence","date":"2018-07-28T21:27","url":"https://www.lesswrong.com/posts/cKfryXvyJ522iFuNF/a-gym-gridworld-environment-for-the-treacherous-turn"},{"x":"12.263607","y":"9.823231","title":"Exorcizing the Speed Prior?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2018-07-22T06:45","url":"https://www.lesswrong.com/posts/Say4sCQ2g22HGsbRT/exorcizing-the-speed-prior"},{"x":"11.627559","y":"10.080646","title":"Stable Pointers to Value III: Recursive Quantilization","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Utility Functions/Mild Optimization/The Pointers Problem/Value Learning","date":"2018-07-21T08:06","url":"https://www.lesswrong.com/posts/bEa4FuLS4r7hExoty/stable-pointers-to-value-iii-recursive-quantilization"},{"x":"11.759185","y":"9.222033","title":"Conceptual problems with utility functions, second attempt at explaining","cluster":"1","author":"['Dacyn']","source":"alignment forum","tags":"","date":"2018-07-21T02:08","url":"https://www.lesswrong.com/posts/QmeguSp4Pm7gecJCz/conceptual-problems-with-utility-functions-second-attempt-at"},{"x":"11.554494","y":"9.869706","title":"Probability is Real, and Value is Complex","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Logic & Mathematics /Utility Functions","date":"2018-07-20T05:24","url":"https://www.lesswrong.com/posts/oheKfWA7SsvpK7SGp/probability-is-real-and-value-is-complex"},{"x":"12.058979","y":"9.1169195","title":"Generalized Kelly betting","cluster":"1","author":"['Linda Linsefors']","source":"alignment forum","tags":"Kelly Criterion","date":"2018-07-19T01:38","url":"https://www.lesswrong.com/posts/97tCruegbz8GXRFzi/generalized-kelly-betting"},{"x":"12.067292","y":"8.594512","title":"Figuring out what Alice wants, part II","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2018-07-17T13:59","url":"https://www.lesswrong.com/posts/rcXaY3FgoobMkH2jc/figuring-out-what-alice-wants-part-ii"},{"x":"10.887156","y":"9.042956","title":"Figuring out what Alice wants, part I","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2018-07-17T13:59","url":"https://www.lesswrong.com/posts/so78tQvTCaBgw4bfL/figuring-out-what-alice-wants-part-i"},{"x":"11.348103","y":"9.537601","title":"Compact vs. Wide Models","cluster":"1","author":"['Vaniver']","source":"alignment forum","tags":"AI/Definitions/Distinctions","date":"2018-07-16T04:09","url":"https://www.lesswrong.com/posts/JkCPkMxuftohieb8B/compact-vs-wide-models"},{"x":"12.393135","y":"8.541437","title":"Buridan’s ass in coordination games","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Coordination / Cooperation/Game Theory/Utility Functions","date":"2018-07-16T02:51","url":"https://www.lesswrong.com/posts/4xpDnGaKz472qB4LY/buridan-s-ass-in-coordination-games"},{"x":"7.9171844","y":"8.280094","title":"Announcement: AI alignment prize round 3 winners and next round","cluster":"3","author":"['cousin_it']","source":"alignment forum","tags":"AI/Research Agendas","date":"2018-07-15T07:40","url":"https://www.lesswrong.com/posts/juBRTuE3TLti5yB35/announcement-ai-alignment-prize-round-3-winners-and-next"},{"x":"11.221729","y":"9.975093","title":"An Agent is a Worldline in Tegmark V","cluster":"1","author":"['komponisto']","source":"alignment forum","tags":"Definitions/Agency/World Modeling/AI","date":"2018-07-12T05:12","url":"https://www.lesswrong.com/posts/brQYmeX4HFrPbs4XP/an-agent-is-a-worldline-in-tegmark-v"},{"x":"11.811042","y":"9.340526","title":"Monk Treehouse: some problems defining simulation","cluster":"1","author":"['dranorter']","source":"alignment forum","tags":"","date":"2018-07-11T07:35","url":"https://www.lesswrong.com/posts/o7FBQkwgKJPDjDKnh/monk-treehouse-some-problems-defining-simulation"},{"x":"10.247885","y":"9.196388","title":" Mathematical Mindset","cluster":"4","author":"['komponisto']","source":"alignment forum","tags":"Rationality","date":"2018-07-11T03:03","url":"https://www.lesswrong.com/posts/wxBBRzR4FS7nGBjbD/mathematical-mindset"},{"x":"11.113939","y":"8.404401","title":"Agents That Learn From Human Behavior Can’t Learn Human Values That Humans Haven’t Learned Yet","cluster":"1","author":"['steven0461']","source":"alignment forum","tags":"Inverse Reinforcement Learning/Value Learning","date":"2018-07-11T02:59","url":"https://www.lesswrong.com/posts/DfewqowdzDdCD7S9y/agents-that-learn-from-human-behavior-can-t-learn-human"},{"x":"11.940888","y":"8.989155","title":"On the Role of Counterfactuals in Learning","cluster":"1","author":"['Max Kanwal']","source":"alignment forum","tags":"Counterfactuals","date":"2018-07-11T02:45","url":"https://www.lesswrong.com/posts/MeYeLEr4RNGreJZcB/on-the-role-of-counterfactuals-in-learning"},{"x":"12.425384","y":"10.188474","title":"Clarifying Consequentialists in the Solomonoff Prior","cluster":"1","author":"['vlad_m']","source":"alignment forum","tags":"Solomonoff Induction","date":"2018-07-11T02:35","url":"https://www.lesswrong.com/posts/jP3vRbtvDtBtgvkeb/clarifying-consequentialists-in-the-solomonoff-prior"},{"x":"11.655196","y":"9.600674","title":"Complete Class: Consequentialist Foundations","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Futarchy/Decision Theory/Logic & Mathematics /World Modeling","date":"2018-07-11T01:57","url":"https://www.lesswrong.com/posts/sZuw6SGfmZHvcAAEP/complete-class-consequentialist-foundations"},{"x":"10.101554","y":"8.472021","title":"Conditions under which misaligned subagents can (not) arise in classifiers","cluster":"4","author":"['anon1']","source":"alignment forum","tags":"Subagents","date":"2018-07-11T01:52","url":"https://www.lesswrong.com/posts/xmzNAoWcYQfMv3j6J/conditions-under-which-misaligned-subagents-can-not-arise-in"},{"x":"12.235911","y":"9.206279","title":"No, I won’t go there, it feels like you’re trying to Pascal-mug me","cluster":"1","author":"['Rupert']","source":"alignment forum","tags":"Pascal's Mugging","date":"2018-07-11T01:37","url":"https://www.lesswrong.com/posts/o7MXZgx3SGpqSxHYZ/no-i-won-t-go-there-it-feels-like-you-re-trying-to-pascal"},{"x":"11.620748","y":"9.3242035","title":"Conceptual problems with utility functions","cluster":"1","author":"['Dacyn']","source":"alignment forum","tags":"Utility Functions","date":"2018-07-11T01:29","url":"https://www.lesswrong.com/posts/Nx4DsTpMaoTiTp4RQ/conceptual-problems-with-utility-functions"},{"x":"11.930973","y":"10.283615","title":"Dependent Type Theory and Zero-Shot Reasoning","cluster":"1","author":"['evhub']","source":"alignment forum","tags":"","date":"2018-07-11T01:16","url":"https://www.lesswrong.com/posts/Xfw2d5horPunP2MSK/dependent-type-theory-and-zero-shot-reasoning"},{"x":"9.780898","y":"7.872843","title":"A comment on the IDA-AlphaGoZero metaphor; capabilities versus alignment","cluster":"4","author":"['AlexMennen']","source":"alignment forum","tags":"DeepMind/Iterated Amplification ","date":"2018-07-11T01:03","url":"https://www.lesswrong.com/posts/yXFKh2jGysQNfX2NM/a-comment-on-the-ida-alphagozero-metaphor-capabilities"},{"x":"11.941774","y":"7.7141795","title":"Bounding Goodhart’s Law","cluster":"1","author":"['eric_langlois']","source":"alignment forum","tags":"Goodhart's Law","date":"2018-07-11T00:46","url":"https://www.lesswrong.com/posts/nu2KcjGf2EYyY2oRJ/bounding-goodhart-s-law"},{"x":"8.491504","y":"6.560098","title":"Mechanistic Transparency for Machine Learning","cluster":"2","author":"['DanielFilan']","source":"alignment forum","tags":"AI/Transparency / Interpretability (ML & AI)","date":"2018-07-11T00:34","url":"https://www.lesswrong.com/posts/3kwR2dufdJyJamHQq/mechanistic-transparency-for-machine-learning"},{"x":"12.139705","y":"9.494668","title":"An environment for studying counterfactuals","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Counterfactuals","date":"2018-07-11T00:14","url":"https://www.lesswrong.com/posts/hhNH3knNHgdkonAKB/an-environment-for-studying-counterfactuals"},{"x":"11.952641","y":"8.508895","title":"A universal score for optimizers","cluster":"1","author":"['levin']","source":"alignment forum","tags":"","date":"2018-07-10T23:52","url":"https://www.lesswrong.com/posts/vWfFDahpnY3tnCLrp/a-universal-score-for-optimizers"},{"x":"11.508912","y":"10.163949","title":"Bayesian Probability is for things that are Space-like Separated from You","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Bayes' Theorem/Logical Uncertainty/Bayesian Decision Theory","date":"2018-07-10T23:47","url":"https://www.lesswrong.com/posts/FvcyMMaJKhYibtFDD/bayesian-probability-is-for-things-that-are-space-like"},{"x":"10.1706085","y":"7.8773394","title":"A framework for thinking about wireheading","cluster":"4","author":"['theotherotheralex']","source":"alignment forum","tags":"","date":"2018-07-10T23:14","url":"https://www.lesswrong.com/posts/MbWDqFgojfnwhxRxr/a-framework-for-thinking-about-wireheading"},{"x":"11.851945","y":"9.649202","title":"Logical Uncertainty and Functional Decision Theory","cluster":"1","author":"['swordsintoploughshares']","source":"alignment forum","tags":"Logical Uncertainty","date":"2018-07-10T23:08","url":"https://www.lesswrong.com/posts/jyeYdBXAwsc4LPs7m/logical-uncertainty-and-functional-decision-theory"},{"x":"11.740029","y":"8.746105","title":"Repeated (and improved) Sleeping Beauty problem","cluster":"1","author":"['Linda Linsefors']","source":"alignment forum","tags":"Sleeping Beauty Paradox","date":"2018-07-10T22:32","url":"https://www.lesswrong.com/posts/KZh9eKCkBRbevkGnP/repeated-and-improved-sleeping-beauty-problem"},{"x":"11.737072","y":"8.8862095","title":"Probability is fake, frequency is real","cluster":"1","author":"['Linda Linsefors']","source":"alignment forum","tags":"Sleeping Beauty Paradox","date":"2018-07-10T22:32","url":"https://www.lesswrong.com/posts/vmfW2qTac4vF3YS3J/probability-is-fake-frequency-is-real"},{"x":"12.239552","y":"9.450882","title":"Conditioning, Counterfactuals, Exploration, and Gears","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Counterfactuals","date":"2018-07-10T22:11","url":"https://www.lesswrong.com/posts/uQHAJ7TdBbweRR5iS/conditioning-counterfactuals-exploration-and-gears"},{"x":"10.867668","y":"8.994639","title":"Two agents can have the same source code and optimise different utility functions","cluster":"1","author":"['Joar Skalse']","source":"alignment forum","tags":"","date":"2018-07-10T21:51","url":"https://www.lesswrong.com/posts/zMHK9gFY6t48Exqup/two-agents-can-have-the-same-source-code-and-optimise"},{"x":"10.333393","y":"8.378755","title":"The Intentional Agency Experiment","cluster":"4","author":"['Self-Embedded Agent']","source":"alignment forum","tags":"","date":"2018-07-10T20:32","url":"https://www.lesswrong.com/posts/gBeEt7YmaHd8dmif7/the-intentional-agency-experiment"},{"x":"8.008076","y":"8.443464","title":"Announcing AlignmentForum.org Beta","cluster":"3","author":"['Raemon']","source":"alignment forum","tags":"Site Meta/AI","date":"2018-07-10T20:19","url":"https://www.lesswrong.com/posts/JiMAMNAb55Qq24nES/announcing-alignmentforum-org-beta"},{"x":"11.388523","y":"9.412575","title":"Choosing to Choose?","cluster":"1","author":"['Whispermute']","source":"alignment forum","tags":"","date":"2018-07-10T20:15","url":"https://www.lesswrong.com/posts/B6SstsM3cdKTaicbj/choosing-to-choose"},{"x":"9.395065","y":"9.071365","title":"RFC: Mental phenomena in AGI alignment","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2018-07-05T20:52","url":"https://www.lesswrong.com/posts/NtofeMwzDgi8xYqK2/rfc-mental-phenomena-in-agi-alignment"},{"x":"8.874654","y":"7.963336","title":"The Learning-Theoretic AI Alignment Research Agenda","cluster":"4","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"Research Agendas","date":"2018-07-04T09:53","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"},{"x":"11.320329","y":"9.31738","title":"Intertheoretic utility comparison","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Utility Functions","date":"2018-07-03T13:44","url":"https://www.lesswrong.com/posts/hBJCMWELaW6MxinYW/intertheoretic-utility-comparison"},{"x":"8.997735","y":"8.503223","title":"Paul’s research agenda FAQ","cluster":"4","author":"['zhukeepa']","source":"alignment forum","tags":"Research Agendas/Iterated Amplification /Humans Consulting HCH/Q&A (format)","date":"2018-07-01T06:25","url":"https://www.lesswrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq"},{"x":"11.742128","y":"9.799319","title":"Another take on agent foundations: formalizing zero-shot reasoning","cluster":"1","author":"['zhukeepa']","source":"alignment forum","tags":"AI/General Intelligence/Agent Foundations","date":"2018-07-01T06:12","url":"https://www.lesswrong.com/posts/pu3ddLSZjjmiiqQfh/another-take-on-agent-foundations-formalizing-zero-shot"},{"x":"10.496675","y":"7.903913","title":"Overcoming Clinginess in Impact Measures","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures","date":"2018-06-30T22:51","url":"https://www.lesswrong.com/posts/DvmhXysefEyEvXuXS/overcoming-clinginess-in-impact-measures"},{"x":"7.7340155","y":"7.9692297","title":"Meta: IAFF vs LessWrong","cluster":"3","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2018-06-30T21:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375576/meta-iaff-vs-lesswrong"},{"x":"11.137583","y":"8.44536","title":"Policy Alignment","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Decision Theory/Value Learning","date":"2018-06-30T00:24","url":"https://www.lesswrong.com/posts/TeYro2ntqHNyQFx8r/policy-alignment"},{"x":"8.348764","y":"7.813883","title":"Optimization Amplifies","cluster":"4","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Optimization/AI/Goodhart's Law","date":"2018-06-27T01:51","url":"https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies"},{"x":"11.742358","y":"8.801748","title":"UDT can learn anthropic probabilities","cluster":"1","author":"['cousin_it']","source":"alignment forum","tags":"Anthropics/Decision Theory/Updateless Decision Theory","date":"2018-06-24T18:04","url":"https://www.lesswrong.com/posts/ma5Jc4wPT36j3X84P/udt-can-learn-anthropic-probabilities"},{"x":"10.408635","y":"8.19114","title":"Worrying about the Vase: Whitelisting","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Impact Measures","date":"2018-06-16T02:17","url":"https://www.lesswrong.com/posts/H7KB44oKoSjSCkpzL/worrying-about-the-vase-whitelisting"},{"x":"10.058733","y":"9.607723","title":"Aligned AI May Depend on Moral Facts","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2018-06-15T01:33","url":"https://www.lesswrong.com/posts/bDNiZYGuqSwp4GM7u/aligned-ai-may-depend-on-moral-facts"},{"x":"12.673842","y":"9.688688","title":"Logical Inductor Tiling and Why it’s Hard","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-06-14T06:34","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037556d/logical-inductor-tiling-and-why-it-s-hard"},{"x":"12.229189","y":"8.84491","title":"Counterfactual Mugging Poker Game","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Decision Theory/Meta-Honesty/Counterfactual Mugging/Counterfactuals","date":"2018-06-13T23:34","url":"https://www.lesswrong.com/posts/g3PwPgcdcWiP33pYn/counterfactual-mugging-poker-game"},{"x":"8.725416","y":"8.909466","title":"A general model of safety-oriented AI development","cluster":"3","author":"['Wei_Dai']","source":"alignment forum","tags":"AI Risk/Iterated Amplification ","date":"2018-06-11T21:00","url":"https://www.lesswrong.com/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development"},{"x":"12.547737","y":"10.306763","title":"A Loophole for Self-Applicative Soundness","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-06-11T07:57","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037556f/a-loophole-for-self-applicative-soundness"},{"x":"9.721663","y":"9.503537","title":"RFC: Meta-ethical uncertainty in AGI alignment","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Metaethics/Moral Uncertainty/Outer Alignment","date":"2018-06-08T20:56","url":"https://www.lesswrong.com/posts/uR3znuBnaevssYDZY/rfc-meta-ethical-uncertainty-in-agi-alignment"},{"x":"10.885277","y":"9.485106","title":"Beyond Astronomical Waste","cluster":"1","author":"['Wei_Dai']","source":"alignment forum","tags":"Astronomical Waste/Simulation Hypothesis/Decision Theory","date":"2018-06-07T21:04","url":"https://www.lesswrong.com/posts/Qz6w4GYZpgeDp6ATB/beyond-astronomical-waste"},{"x":"13.047985","y":"9.169567","title":"Resource-Limited Reflective Oracles","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Oracle AI/Oracle AI","date":"2018-06-06T02:50","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037555e/resource-limited-reflective-oracles"},{"x":"12.5727215","y":"8.522214","title":"Prisoners’ Dilemma with Costs to Modeling","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Prisoner's Dilemma/Game Theory/Decision Theory/Moloch","date":"2018-06-05T04:51","url":"https://www.lesswrong.com/posts/XjMkPyaPYTf7LrKiT/prisoners-dilemma-with-costs-to-modeling"},{"x":"12.251622","y":"8.341036","title":"Logical Inductors Converge to Correlated Equilibria (Kinda)","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-05-30T20:23","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375569/logical-inductors-converge-to-correlated-equilibria-kinda"},{"x":"10.509684","y":"6.765707","title":"Safety in Machine Learning","cluster":"0","author":"['G Gordon Worley III']","source":"alignment forum","tags":"","date":"2018-05-29T18:54","url":"https://www.lesswrong.com/posts/3iP8P57mNpHBFfYkd/safety-in-machine-learning"},{"x":"9.476127","y":"8.227304","title":"Challenges to Christiano’s capability amplification proposal","cluster":"4","author":"['Eliezer Yudkowsky']","source":"alignment forum","tags":"AI/Iterated Amplification ","date":"2018-05-19T18:18","url":"https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal"},{"x":"9.237142","y":"9.412937","title":"RFC: Philosophical Conservatism in AI Alignment Research","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Conservatism (AI)/AI/Metaethics","date":"2018-05-15T03:29","url":"https://www.lesswrong.com/posts/3r44dhh3uK7s9Pveq/rfc-philosophical-conservatism-in-ai-alignment-research"},{"x":"9.476969","y":"9.993126","title":"Thoughts on \"AI safety via debate\"","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Debate (AI safety technique)","date":"2018-05-10T00:44","url":"https://www.lesswrong.com/posts/WRy6KNnxwQHc5Ktjc/thoughts-on-ai-safety-via-debate"},{"x":"8.754321","y":"7.7883334","title":"Open question: are minimal circuits daemon-free?","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Inner Alignment/AI/Mesa-Optimization/Open Problems","date":"2018-05-05T22:40","url":"https://www.lesswrong.com/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free"},{"x":"12.389178","y":"9.828353","title":"Doubts about Updatelessness","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-05-03T05:44","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375562/doubts-about-updatelessness"},{"x":"12.581666","y":"8.470338","title":"The Game Theory of Blackmail","cluster":"1","author":"['Linda Linsefors']","source":"alignment forum","tags":"Blackmail / Extortion","date":"2019-03-22T17:44","url":"https://www.lesswrong.com/posts/wm2rdS3sDY9M5kpWb/the-game-theory-of-blackmail"},{"x":"9.01836","y":"9.288767","title":"The Main Sources of AI Risk?","cluster":"3","author":"['Daniel Kokotajlo', 'Wei_Dai']","source":"alignment forum","tags":"AI Risk","date":"2019-03-21T18:28","url":"https://www.lesswrong.com/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk"},{"x":"10.104065","y":"9.375015","title":"[Question] What’s wrong with these analogies for understanding Informed Oversight and IDA?","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"Humans Consulting HCH/Iterated Amplification /AI","date":"2019-03-20T09:11","url":"https://www.lesswrong.com/posts/LigbvLH9yKR5Zhd6y/what-s-wrong-with-these-analogies-for-understanding-informed"},{"x":"11.390554","y":"9.7742195","title":"Partial preferences and models","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-19T16:29","url":"https://www.lesswrong.com/posts/CiB3myyeEhFRgmKPL/partial-preferences-and-models"},{"x":"8.904126","y":"8.12464","title":"What failure looks like","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Threat Models/AI Risk/World Optimization/World Modeling","date":"2019-03-17T20:18","url":"https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like"},{"x":"11.846851","y":"9.353144","title":"Comparison of decision theories (with a focus on logical-counterfactual decision theories)","cluster":"1","author":"['riceissa']","source":"alignment forum","tags":"Decision Theory","date":"2019-03-16T21:15","url":"https://www.lesswrong.com/posts/QPhY8Nb7gtT5wvoPH/comparison-of-decision-theories-with-a-focus-on-logical"},{"x":"11.429751","y":"9.338006","title":"Combining individual preference utility functions","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-14T14:14","url":"https://www.lesswrong.com/posts/Abws49L8CNFEorpXg/combining-individual-preference-utility-functions"},{"x":"11.356724","y":"8.870197","title":"Mysteries, identity, and preferences over non-rewards","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-14T13:52","url":"https://www.lesswrong.com/posts/kSiHWuuhfe7rB4wAG/mysteries-identity-and-preferences-over-non-rewards"},{"x":"11.200006","y":"9.240459","title":"A theory of human values","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Metaethics","date":"2019-03-13T15:22","url":"https://www.lesswrong.com/posts/qezBTig6p6p5xtL6G/a-theory-of-human-values"},{"x":"10.56723","y":"8.038969","title":"Designing agent incentives to avoid side effects","cluster":"4","author":"['Vika', 'TurnTrout']","source":"alignment forum","tags":"Impact Measures/AI","date":"2019-03-11T20:55","url":"https://www.lesswrong.com/posts/eax34WBLNmB4Gv6so/designing-agent-incentives-to-avoid-side-effects"},{"x":"11.162568","y":"9.356791","title":"Example population ethics: ordered discounted utility","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Population Ethics","date":"2019-03-11T16:10","url":"https://www.lesswrong.com/posts/Ee29dFnPhaeRmYdMy/example-population-ethics-ordered-discounted-utility"},{"x":"8.732692","y":"8.6466875","title":"Alignment Research Field Guide","cluster":"4","author":"['abramdemski']","source":"alignment forum","tags":"Practical/Practice & Philosophy of Science/Social & Cultural Dynamics/Community/Intellectual Progress (Individual-Level)","date":"2019-03-08T19:57","url":"https://www.lesswrong.com/posts/PqMT9zGrNsGJNfiFR/alignment-research-field-guide"},{"x":"11.325043","y":"9.165683","title":"Smoothmin and personal identity","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Personal Identity","date":"2019-03-08T15:16","url":"https://www.lesswrong.com/posts/MxLK2fvEuijAYgsc2/smoothmin-and-personal-identity"},{"x":"10.966007","y":"9.411019","title":"Preferences in subpieces of hierarchical systems","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-06T15:18","url":"https://www.lesswrong.com/posts/8cedA5q2cBSZiYSPo/preferences-in-subpieces-of-hierarchical-systems"},{"x":"9.910361","y":"8.261769","title":"Asymptotically Unambitious AGI","cluster":"4","author":"['michaelcohen']","source":"alignment forum","tags":"Bounties (closed)/Instrumental Convergence/Impact Measures","date":"2019-03-06T01:15","url":"https://www.lesswrong.com/posts/pZhDWxDmwzuSwLjou/asymptotically-unambitious-agi"},{"x":"9.846275","y":"7.920451","title":"Three ways that \"Sufficiently optimized agents appear coherent\" can be false","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"Utility Functions/General Intelligence/Coherence Arguments","date":"2019-03-05T21:52","url":"https://www.lesswrong.com/posts/4K52SS7fm9mp5rMdX/three-ways-that-sufficiently-optimized-agents-appear"},{"x":"10.7","y":"9.002464","title":"Simplified preferences needed; simplified preferences sufficient","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Impact Measures/Utility Functions","date":"2019-03-05T19:39","url":"https://www.lesswrong.com/posts/sEqu6jMgnHG2fvaoQ/simplified-preferences-needed-simplified-preferences"},{"x":"10.987946","y":"10.259878","title":"Finding the variables","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Truth, Semantics, & Meaning","date":"2019-03-04T19:37","url":"https://www.lesswrong.com/posts/pHHhyZX5zwvwNqDXm/finding-the-variables"},{"x":"10.9245205","y":"10.108233","title":"Syntax vs semantics: alarm better example than thermostat","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-03-04T12:43","url":"https://www.lesswrong.com/posts/bbw6c9as5STvWXAgB/syntax-vs-semantics-alarm-better-example-than-thermostat"},{"x":"10.74151","y":"8.514182","title":"How to get value learning and reference wrong","cluster":"1","author":"['Charlie Steiner']","source":"alignment forum","tags":"Postmortems & Retrospectives/Value Learning","date":"2019-02-26T20:22","url":"https://www.lesswrong.com/posts/NcahJFd5S5RNupxFT/how-to-get-value-learning-and-reference-wrong"},{"x":"11.680964","y":"9.930483","title":"Can HCH epistemically dominate Ramanujan?","cluster":"1","author":"['zhukeepa']","source":"alignment forum","tags":"Humans Consulting HCH","date":"2019-02-23T22:00","url":"https://www.lesswrong.com/posts/4qY9zEHLa2su4PkQ4/can-hch-epistemically-dominate-ramanujan"},{"x":"9.751436","y":"9.236991","title":"Thoughts on Human Models","cluster":"4","author":"['Ramana Kumar', 'Scott Garrabrant']","source":"alignment forum","tags":"AI/Research Agendas/Mind Crime","date":"2019-02-21T09:10","url":"https://www.lesswrong.com/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models"},{"x":"12.326606","y":"8.295638","title":"Pavlov Generalizes","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Prisoner's Dilemma/Game Theory/Decision Theory","date":"2019-02-20T09:03","url":"https://www.lesswrong.com/posts/XTgkhjNTEi97WHMi6/pavlov-generalizes"},{"x":"9.311561","y":"8.523391","title":"How the MtG Color Wheel Explains AI Safety","cluster":"4","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Intuition","date":"2019-02-15T23:42","url":"https://www.lesswrong.com/posts/9CKBtxWtjvminNTmC/how-the-mtg-color-wheel-explains-ai-safety"},{"x":"11.052552","y":"9.255687","title":"Humans interpreting humans","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-02-13T19:03","url":"https://www.lesswrong.com/posts/eXiuk8Yjd7FsunhxM/humans-interpreting-humans"},{"x":"11.337447","y":"8.825284","title":"Anchoring vs Taste: a model","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-02-13T19:03","url":"https://www.lesswrong.com/posts/7gMkkEMQa78h8Bpmr/anchoring-vs-taste-a-model"},{"x":"10.570039","y":"10.020062","title":"Nuances with ascription universality","cluster":"1","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2019-02-12T23:38","url":"https://www.lesswrong.com/posts/R5Euq7gZgobJi5S25/nuances-with-ascription-universality"},{"x":"11.608925","y":"6.7758145","title":"Learning preferences by looking at the world","cluster":"0","author":"['Rohin Shah']","source":"alignment forum","tags":"Impact Measures/Center for Human-Compatible AI (CHAI)/Value Learning/AI/Academic Papers/Summaries","date":"2019-02-12T22:25","url":"https://www.lesswrong.com/posts/7f6DNZhracD7RvxMr/learning-preferences-by-looking-at-the-world"},{"x":"9.097342","y":"8.799742","title":"Would I think for ten thousand years?","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Drift/Complexity of Value/Human Values/Value Learning","date":"2019-02-11T19:37","url":"https://www.lesswrong.com/posts/6zwW9oGaHbuMuvnmX/would-i-think-for-ten-thousand-years"},{"x":"11.682455","y":"8.900828","title":"\"Normative assumptions\" need not be complex","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-02-11T19:03","url":"https://www.lesswrong.com/posts/QzsCrzGd4zkNwk9cd/normative-assumptions-need-not-be-complex"},{"x":"11.352619","y":"9.265862","title":"Coherent behaviour in the real world is an incoherent concept","cluster":"1","author":"['Richard_Ngo']","source":"alignment forum","tags":"Utility Functions/Coherence Arguments","date":"2019-02-11T17:00","url":"https://www.lesswrong.com/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent"},{"x":"10.067616","y":"9.545835","title":"Some Thoughts on Metaphilosophy","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"Meta-Philosophy/Philosophy/AI","date":"2019-02-10T00:28","url":"https://www.lesswrong.com/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy"},{"x":"9.871187","y":"9.680893","title":"The Argument from Philosophical Difficulty","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"AI","date":"2019-02-10T00:28","url":"https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty"},{"x":"10.81427","y":"6.5516","title":"Reinforcement Learning in the Iterated Amplification Framework","cluster":"0","author":"['William_S']","source":"alignment forum","tags":"Iterated Amplification /Reinforcement Learning","date":"2019-02-09T00:56","url":"https://www.lesswrong.com/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification"},{"x":"10.205164","y":"9.573753","title":"HCH is not just Mechanical Turk","cluster":"1","author":"['William_S']","source":"alignment forum","tags":"Humans Consulting HCH","date":"2019-02-09T00:46","url":"https://www.lesswrong.com/posts/4JuKoFguzuMrNn6Qr/hch-is-not-just-mechanical-turk"},{"x":"10.775846","y":"8.341781","title":"Test Cases for Impact Regularisation Methods","cluster":"4","author":"['DanielFilan']","source":"alignment forum","tags":"Impact Measures/AI/Ontology","date":"2019-02-06T21:50","url":"https://www.lesswrong.com/posts/wzPzPmAsG3BwrBrwy/test-cases-for-impact-regularisation-methods"},{"x":"8.908203","y":"8.531457","title":"Security amplification","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2019-02-06T17:28","url":"https://www.lesswrong.com/posts/hjEaZgyQ2iprDhkg8/security-amplification"},{"x":"10.407531","y":"7.015399","title":"When to use quantilization","cluster":"0","author":"['RyanCarey']","source":"alignment forum","tags":"Mild Optimization/Quantilization/Goodhart's Law/Utility Functions/AI","date":"2019-02-05T17:17","url":"https://www.lesswrong.com/posts/Rs6vZCrnQFWQ4p37P/when-to-use-quantilization"},{"x":"10.487259","y":"8.296928","title":"Conclusion to the sequence on value learning","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning","date":"2019-02-03T21:05","url":"https://www.lesswrong.com/posts/TE5nJ882s5dCMkBB8/conclusion-to-the-sequence-on-value-learning"},{"x":"9.115588","y":"7.0352154","title":"[Question] How does Gradient Descent Interact with Goodhart?","cluster":"2","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Goodhart's Law/AI/Gradient Hacking","date":"2019-02-02T00:14","url":"https://www.lesswrong.com/posts/pcomQ4Fwi7FnfBZBR/how-does-gradient-descent-interact-with-goodhart"},{"x":"11.286773","y":"8.3802395","title":"Reliability amplification","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2019-01-31T21:12","url":"https://www.lesswrong.com/posts/6fMvGoyy3kgnonRNM/reliability-amplification"},{"x":"9.918914","y":"8.542494","title":"Wireheading is in the eye of the beholder","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Wireheading","date":"2019-01-30T18:23","url":"https://www.lesswrong.com/posts/BvctuKocyWR4YYea3/wireheading-is-in-the-eye-of-the-beholder"},{"x":"12.103017","y":"9.541468","title":"Deconfusing Logical Counterfactuals","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Logical Uncertainty/Counterfactuals/Functional Decision Theory","date":"2019-01-30T15:13","url":"https://www.lesswrong.com/posts/BRuWm4GxcTNPn4XDX/deconfusing-logical-counterfactuals"},{"x":"10.918509","y":"9.651875","title":"Can there be an indescribable hellworld?","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Debate (AI safety technique)/Complexity of Value","date":"2019-01-29T15:00","url":"https://www.lesswrong.com/posts/rArsypGqq49bk4iRr/can-there-be-an-indescribable-hellworld"},{"x":"10.736029","y":"8.504478","title":"How much can value learning be disentangled?","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning","date":"2019-01-29T14:17","url":"https://www.lesswrong.com/posts/Q7WiHdSSShkNsgDpa/how-much-can-value-learning-be-disentangled"},{"x":"9.13593","y":"6.624278","title":"Techniques for optimizing worst-case performance","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification /Existential Risk","date":"2019-01-28T21:29","url":"https://www.lesswrong.com/posts/E2aZ9Xwdz3i2ghPtn/techniques-for-optimizing-worst-case-performance"},{"x":"10.34331","y":"8.084974","title":"Future directions for narrow value learning","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning","date":"2019-01-26T02:36","url":"https://www.lesswrong.com/posts/MxadmSXHnoCupsWqx/future-directions-for-narrow-value-learning"},{"x":"11.347484","y":"6.6555543","title":"Thoughts on reward engineering","cluster":"0","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification /Reward Functions","date":"2019-01-24T20:15","url":"https://www.lesswrong.com/posts/NtX7LKhCXMW2vjWx6/thoughts-on-reward-engineering"},{"x":"11.993033","y":"8.96826","title":"[Question] Is Agent Simulates Predictor a \"fair\" problem?","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Decision Theory/Newcomb's Problem","date":"2019-01-24T13:18","url":"https://www.lesswrong.com/posts/HvLcGmr2APwqauFzW/is-agent-simulates-predictor-a-fair-problem"},{"x":"9.522726","y":"8.490977","title":"The human side of interaction","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"","date":"2019-01-24T10:14","url":"https://www.lesswrong.com/posts/eD9T4kiwB6MHpySGE/the-human-side-of-interaction"},{"x":"12.520758","y":"10.207239","title":"Allowing a formal proof system to self improve while avoiding Lobian obstacles.","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"Formal Proof","date":"2019-01-23T23:04","url":"https://www.lesswrong.com/posts/hchfRj4qa4hFZxhKM/allowing-a-formal-proof-system-to-self-improve-while"},{"x":"11.333075","y":"9.275056","title":"One-step hypothetical preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-01-23T15:14","url":"https://www.lesswrong.com/posts/i6hWWcKyxBPj7ELT6/one-step-hypothetical-preferences"},{"x":"11.33365","y":"6.6832905","title":"Learning with catastrophes","cluster":"0","author":"['paulfchristiano']","source":"alignment forum","tags":"Machine Learning/AI","date":"2019-01-23T03:01","url":"https://www.lesswrong.com/posts/qALeGJ9nPcs9eC9Af/learning-with-catastrophes"},{"x":"9.005032","y":"8.598015","title":"Disentangling arguments for the importance of AI safety","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/Industrial Revolution/AI Risk/Complexity of Value","date":"2019-01-21T12:41","url":"https://www.lesswrong.com/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety"},{"x":"10.061409","y":"8.533718","title":"Following human norms","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"AI/Value Learning","date":"2019-01-20T23:59","url":"https://www.lesswrong.com/posts/eBd6WvzhuqduCkYv3/following-human-norms"},{"x":"8.980932","y":"9.542973","title":"Announcement: AI alignment prize round 4 winners","cluster":"3","author":"['cousin_it']","source":"alignment forum","tags":"AI Risk/Impact Measures/Corrigibility","date":"2019-01-20T14:46","url":"https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg/announcement-ai-alignment-prize-round-4-winners"},{"x":"9.893123","y":"7.45541","title":"Capability amplification","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2019-01-20T07:03","url":"https://www.lesswrong.com/posts/t3AJW5jP3sk36aGoC/capability-amplification-1"},{"x":"8.882667","y":"8.460581","title":"[Question] Why not tool AI?","cluster":"4","author":"['smithee']","source":"alignment forum","tags":"AI/Tool AI","date":"2019-01-19T22:18","url":"https://www.lesswrong.com/posts/jkxkMTGfZDzBEaaY8/why-not-tool-ai"},{"x":"11.210942","y":"7.2745776","title":"Reward uncertainty","cluster":"0","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning","date":"2019-01-19T02:16","url":"https://www.lesswrong.com/posts/ZiLLxaLB5CCofrzPp/reward-uncertainty"},{"x":"11.323351","y":"9.892558","title":"Anthropics is pretty normal","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Anthropics","date":"2019-01-17T13:26","url":"https://www.lesswrong.com/posts/uAqs5Q3aGEen3nKeX/anthropics-is-pretty-normal"},{"x":"11.20639","y":"6.2585583","title":"The reward engineering problem ","cluster":"0","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Iterated Amplification /Reward Functions","date":"2019-01-16T18:47","url":"https://www.lesswrong.com/posts/4nZRzoGTqg8xy5rr8/the-reward-engineering-problem"},{"x":"9.934588","y":"8.065199","title":"Human-AI Interaction","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning","date":"2019-01-15T01:57","url":"https://www.lesswrong.com/posts/4783ufKpx8xvLMPc6/human-ai-interaction"},{"x":"12.003476","y":"9.5840435","title":"CDT=EDT=UDT","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory","date":"2019-01-13T23:46","url":"https://www.lesswrong.com/posts/WkPf6XCzfJLCm2pbK/cdt-edt-udt"},{"x":"10.480009","y":"10.146866","title":"Towards formalizing universality","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Humans Consulting HCH","date":"2019-01-13T20:39","url":"https://www.lesswrong.com/posts/M8WdeNWacMrmorNdd/towards-formalizing-universality"},{"x":"8.254547","y":"7.9121923","title":"Directions and desiderata for AI alignment","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Iterated Amplification ","date":"2019-01-13T07:47","url":"https://www.lesswrong.com/posts/kphJvksj5TndGapuh/directions-and-desiderata-for-ai-alignment"},{"x":"12.086167","y":"9.153033","title":"Dutch-Booking CDT","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Decision Theory","date":"2019-01-13T00:10","url":"https://www.lesswrong.com/posts/wkNQdYj47HX33noKv/dutch-booking-cdt"},{"x":"8.932133","y":"7.9801126","title":"Comments on CAIS","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI Services (CAIS)/AI","date":"2019-01-12T15:20","url":"https://www.lesswrong.com/posts/HvNAmkXPTSoA4dvzv/comments-on-cais"},{"x":"10.336487","y":"8.1458","title":"Ambitious vs. narrow value learning","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Value Learning","date":"2019-01-12T06:18","url":"https://www.lesswrong.com/posts/SvuLhtREMy8wRBzpC/ambitious-vs-narrow-value-learning"},{"x":"10.770738","y":"9.488567","title":"Hierarchical system preferences and subagent preferences","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2019-01-11T18:47","url":"https://www.lesswrong.com/posts/iutXWSDd56ieAiyTi/hierarchical-system-preferences-and-subagent-preferences"},{"x":"10.837632","y":"8.354504","title":"Non-Consequentialist Cooperation?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"AI/Value Learning/Consequentialism/Deontology","date":"2019-01-11T09:15","url":"https://www.lesswrong.com/posts/F9vcbEMKW48j4Z6h9/non-consequentialist-cooperation"},{"x":"10.510107","y":"7.73298","title":"What is narrow value learning?","cluster":"0","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning","date":"2019-01-10T07:05","url":"https://www.lesswrong.com/posts/vX7KirQwHsBaSEdfK/what-is-narrow-value-learning"},{"x":"11.892869","y":"10.491845","title":"No surjection onto function space for manifold X","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Logic & Mathematics ","date":"2019-01-09T18:07","url":"https://www.lesswrong.com/posts/eqi83c2nNSX7TFSfW/no-surjection-onto-function-space-for-manifold-x"},{"x":"9.791366","y":"7.440893","title":"AlphaGo Zero and capability amplification","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"DeepMind/Iterated Amplification ","date":"2019-01-09T00:40","url":"https://www.lesswrong.com/posts/HA3oArypzNANvXC38/alphago-zero-and-capability-amplification"},{"x":"11.208303","y":"8.047721","title":"Predictors as Agents","cluster":"1","author":"['interstice']","source":"alignment forum","tags":"","date":"2019-01-08T20:50","url":"https://www.lesswrong.com/posts/G2EupNcdtigdyNhL2/predictors-as-agents"},{"x":"9.183608","y":"8.165853","title":"Reframing Superintelligence: Comprehensive AI Services as General Intelligence","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"AI Services (CAIS)/AI/General Intelligence/Narrow AI","date":"2019-01-08T07:12","url":"https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as"},{"x":"10.025148","y":"8.721261","title":"AI safety without goal-directed behavior","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Goal-Directedness/AI","date":"2019-01-07T07:48","url":"https://www.lesswrong.com/posts/tHxXdAn8Yuiy9y2pZ/ai-safety-without-goal-directed-behavior"},{"x":"10.6424","y":"6.459133","title":"Supervising strong learners","cluster":"0","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2019-01-06T07:00","url":"https://www.lesswrong.com/posts/xKvzpodBGcPMq7TqE/supervising-strong-learners-by-amplifying-weak-experts"},{"x":"12.88516","y":"9.356382","title":"Failures of UDT-AIXI, Part 1: Improper Randomizing","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"AIXI","date":"2019-01-06T03:53","url":"https://www.lesswrong.com/posts/mrZp6qC7DDXKQZeeC/failures-of-udt-aixi-part-1-improper-randomizing"},{"x":"9.719591","y":"8.22418","title":"Will humans build goal-directed agents?","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Goal-Directedness/AI","date":"2019-01-05T01:33","url":"https://www.lesswrong.com/posts/9zpT9dikrrebdq3Jf/will-humans-build-goal-directed-agents"},{"x":"10.74111","y":"7.2126145","title":"Optimization Regularization through Time Penalty","cluster":"0","author":"['Linda Linsefors']","source":"alignment forum","tags":"AI/Mild Optimization","date":"2019-01-01T13:05","url":"https://www.lesswrong.com/posts/ehLX2RdbD5ZkeJyuJ/optimization-regularization-through-time-penalty"},{"x":"11.532505","y":"7.646107","title":"Penalizing Impact via Attainable Utility Preservation","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Impact Measures","date":"2018-12-28T21:46","url":"https://www.lesswrong.com/posts/mDTded2Dn7BKRBEPX/penalizing-impact-via-attainable-utility-preservation"},{"x":"11.60362","y":"9.285781","title":"Anthropic probabilities and cost functions","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Anthropics","date":"2018-12-21T17:54","url":"https://www.lesswrong.com/posts/uzb3u3zMTkrSEhCaf/anthropic-probabilities-and-cost-functions"},{"x":"8.139026","y":"6.8873863","title":"Reasons compute may not drive AI capabilities growth","cluster":"2","author":"['Kythe']","source":"alignment forum","tags":"AI/Machine Learning/AI Timelines","date":"2018-12-19T22:13","url":"https://www.lesswrong.com/posts/hSw4MNTc3gAwZWdx9/reasons-compute-may-not-drive-ai-capabilities-growth"},{"x":"11.517447","y":"9.351386","title":"Anthropic paradoxes transposed into Anthropic Decision Theory","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Anthropics","date":"2018-12-19T18:07","url":"https://www.lesswrong.com/posts/PgsxXNSDsyz4DFEuw/anthropic-paradoxes-transposed-into-anthropic-decision"},{"x":"8.037465","y":"9.575164","title":"2018 AI Alignment Literature Review and Charity Comparison","cluster":"3","author":"['Larks']","source":"alignment forum","tags":"Community/AI/Moral Uncertainty/Value Learning/Machine Intelligence Research Institute (MIRI)/DeepMind/Center on Long-Term Risk (CLR)/Future of Humanity Institute (FHI)/Center for Human-Compatible AI (CHAI)/Literature Reviews/Future of Life Institute (FLI)","date":"2018-12-18T04:46","url":"https://www.lesswrong.com/posts/a72owS5hz3acBK5xc/2018-ai-alignment-literature-review-and-charity-comparison"},{"x":"9.126191","y":"9.391231","title":"Two Neglected Problems in Human-AI Safety","cluster":"3","author":"['Wei_Dai']","source":"alignment forum","tags":"AI Risk/Superstimuli/Complexity of Value","date":"2018-12-16T22:13","url":"https://www.lesswrong.com/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety"},{"x":"8.958399","y":"9.085619","title":"Three AI Safety Related Ideas","cluster":"4","author":"['Wei_Dai']","source":"alignment forum","tags":"Mind Uploading/Complexity of Value/AI Risk/Corrigibility/Iterated Amplification ","date":"2018-12-13T21:32","url":"https://www.lesswrong.com/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas"},{"x":"10.531023","y":"8.748608","title":"Multi-agent predictive minds and AI alignment","cluster":"4","author":"['Jan_Kulveit']","source":"alignment forum","tags":"Subagents/Predictive Processing","date":"2018-12-12T23:48","url":"https://www.lesswrong.com/posts/3fkBWpE4f9nYbdf7E/multi-agent-predictive-minds-and-ai-alignment"},{"x":"11.033235","y":"9.019681","title":"Figuring out what Alice wants: non-human Alice","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2018-12-11T19:31","url":"https://www.lesswrong.com/posts/YfQGZderiaGv3kBJ8/figuring-out-what-alice-wants-non-human-alice"},{"x":"9.837949","y":"9.624625","title":"Assuming we’ve solved X, could we do Y...","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2018-12-11T18:13","url":"https://www.lesswrong.com/posts/95i5B78uhqyB3d6Xc/assuming-we-ve-solved-x-could-we-do-y"},{"x":"12.761242","y":"8.75514","title":"COEDT Equilibria in Games","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-12-06T18:00","url":"https://www.lesswrong.com/posts/4MLpRxz7ZoX8YXSY3/coedt-equilibria-in-games"},{"x":"11.255448","y":"9.179251","title":"Why we need a *theory* of human values","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning/Complexity of Value","date":"2018-12-05T16:00","url":"https://www.lesswrong.com/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values"},{"x":"9.476861","y":"7.5358686","title":"Factored Cognition","cluster":"4","author":"['stuhlmueller']","source":"alignment forum","tags":"Ought/Factored Cognition/Iterated Amplification ","date":"2018-12-05T01:01","url":"https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition"},{"x":"10.839959","y":"9.491095","title":"Coherence arguments do not entail goal-directed behavior","cluster":"1","author":"['Rohin Shah']","source":"alignment forum","tags":"Utility Functions/Decision Theory/AI/Goal-Directedness/Coherence Arguments/Value Learning","date":"2018-12-03T03:26","url":"https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior"},{"x":"10.704053","y":"7.164072","title":"Benign model-free RL","cluster":"0","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2018-12-02T04:10","url":"https://www.lesswrong.com/posts/PRaxzmDJdvie46ahL/benign-model-free-rl-1"},{"x":"9.929612","y":"8.377257","title":"Intuitions about goal-directed behavior","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning/AI Risk/Goal-Directedness","date":"2018-12-01T04:25","url":"https://www.lesswrong.com/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior"},{"x":"9.361064","y":"7.483439","title":"Iterated Distillation and Amplification","cluster":"4","author":"['Ajeya Cotra']","source":"alignment forum","tags":"Iterated Amplification ","date":"2018-11-30T04:47","url":"https://www.lesswrong.com/posts/HqLxuZ4LhaFhmAHWk/iterated-distillation-and-amplification-1"},{"x":"12.069808","y":"9.052799","title":"Formal Open Problem in Decision Theory","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Decision Theory/Game Theory","date":"2018-11-29T03:25","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753a9/formal-open-problem-in-decision-theory"},{"x":"13.05233","y":"9.071693","title":"Reflective oracles as a solution to the converse Lawvere problem","cluster":"1","author":"['SamEisenstat']","source":"alignment forum","tags":"Oracle AI","date":"2018-11-29T03:23","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037550d/reflective-oracles-as-a-solution-to-the-converse-lawvere"},{"x":"12.207859","y":"9.227288","title":"The Ubiquitous Converse Lawvere Problem","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"AI/Decision Theory/Game Theory","date":"2018-11-29T03:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703753b9/the-ubiquitous-converse-lawvere-problem"},{"x":"11.6490755","y":"10.262393","title":"Hyperreal Brouwer","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Game Theory","date":"2018-11-29T03:15","url":"https://www.lesswrong.com/posts/5bd75cc58225bf06703754e4/hyperreal-brouwer"},{"x":"13.112492","y":"10.078228","title":"Oracle Induction Proofs","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-11-28T08:12","url":"https://www.lesswrong.com/posts/QjYnuGTFcWCQudLsh/oracle-induction-proofs"},{"x":"12.872069","y":"9.495574","title":"Bounded Oracle Induction","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"Logical Induction/Logical Uncertainty/Oracle AI","date":"2018-11-28T08:11","url":"https://www.lesswrong.com/posts/MgLeAWSeLbzx8mkZ2/bounded-oracle-induction"},{"x":"9.958339","y":"9.12492","title":"Corrigibility","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Corrigibility/Instrumental Convergence/AI/Iterated Amplification ","date":"2018-11-27T21:50","url":"https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility"},{"x":"11.799646","y":"8.57107","title":"Humans Consulting HCH","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"Humans Consulting HCH/Iterated Amplification ","date":"2018-11-25T23:18","url":"https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch"},{"x":"10.185787","y":"8.140154","title":"Approval-directed bootstrapping","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2018-11-25T23:18","url":"https://www.lesswrong.com/posts/6x7oExXi32ot6HjJv/approval-directed-bootstrapping"},{"x":"11.672672","y":"10.377967","title":"Fixed Point Discussion","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Fixed Point Theorems","date":"2018-11-24T20:53","url":"https://www.lesswrong.com/posts/mvqmY9MQ3qf88xRuM/fixed-point-discussion"},{"x":"10.208251","y":"8.526894","title":"Approval-directed agents","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2018-11-22T21:15","url":"https://www.lesswrong.com/posts/7Hr8t6xwuuxBTqADK/approval-directed-agents-1"},{"x":"8.808782","y":"6.3530335","title":"Iteration Fixed Point Exercises","cluster":"2","author":"['Scott Garrabrant', 'SamEisenstat']","source":"alignment forum","tags":"Fixed Point Theorems","date":"2018-11-22T00:35","url":"https://www.lesswrong.com/posts/9a2asxypuNjCmga3p/iteration-fixed-point-exercises"},{"x":"10.341485","y":"6.8665233","title":"New safety research agenda: scalable agent alignment via reward modeling","cluster":"0","author":"['Vika']","source":"alignment forum","tags":"Research Agendas/AI","date":"2018-11-20T17:29","url":"https://www.lesswrong.com/posts/HBGd34LKvXM9TxvNf/new-safety-research-agenda-scalable-agent-alignment-via"},{"x":"8.893908","y":"7.774732","title":"Prosaic AI alignment","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Machine Learning","date":"2018-11-20T13:56","url":"https://www.lesswrong.com/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment"},{"x":"11.720259","y":"10.259115","title":"Diagonalization Fixed Point Exercises","cluster":"1","author":"['Scott Garrabrant', 'SamEisenstat']","source":"alignment forum","tags":"Fixed Point Theorems","date":"2018-11-18T00:31","url":"https://www.lesswrong.com/posts/FZkLa3GRLW97fpknG/diagonalization-fixed-point-exercises"},{"x":"8.880007","y":"7.926333","title":"An unaligned benchmark","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/AI Risk","date":"2018-11-17T15:51","url":"https://www.lesswrong.com/posts/ZHXutm7KpoWEj9G2s/an-unaligned-benchmark"},{"x":"11.724095","y":"10.442939","title":"Topological Fixed Point Exercises","cluster":"1","author":"['Scott Garrabrant', 'SamEisenstat']","source":"alignment forum","tags":"Exercises / Problem-Sets/Logic & Mathematics /Fixed Point Theorems","date":"2018-11-17T01:40","url":"https://www.lesswrong.com/posts/svE3S6NKdPYoGepzq/topological-fixed-point-exercises"},{"x":"11.383377","y":"10.191266","title":"Fixed Point Exercises","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Fixed Point Theorems/Exercises / Problem-Sets","date":"2018-11-17T01:39","url":"https://www.lesswrong.com/posts/mojJ6Hpri8rfzY78b/fixed-point-exercises"},{"x":"11.839329","y":"7.444363","title":"Dimensional regret without resets","cluster":"0","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2018-11-16T19:22","url":"https://www.lesswrong.com/posts/Qa5jG9z9dC6E4s9JH/dimensional-regret-without-resets"},{"x":"10.321382","y":"8.262804","title":"Embedded Agency (full-text version)","cluster":"4","author":"['Scott Garrabrant', 'abramdemski']","source":"alignment forum","tags":"Embedded Agency/AI/Mesa-Optimization/Decision Theory/Research Agendas/Robust Agents/Goodhart's Law/Subagents","date":"2018-11-15T19:49","url":"https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version"},{"x":"8.938644","y":"8.416071","title":"Clarifying \"AI Alignment\"","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Value Learning","date":"2018-11-15T14:41","url":"https://www.lesswrong.com/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment"},{"x":"10.577891","y":"8.642474","title":"Acknowledging Human Preference Types to Support Value Learning","cluster":"4","author":"['Nandi Sabrina Erin']","source":"alignment forum","tags":"Value Learning/Psychology","date":"2018-11-13T18:57","url":"https://www.lesswrong.com/posts/mSPsyEwaymS74unND/acknowledging-human-preference-types-to-support-value"},{"x":"9.538537","y":"7.934488","title":"The Steering Problem","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"AI/Outer Alignment","date":"2018-11-13T17:14","url":"https://www.lesswrong.com/posts/4iPBctHSeHx8AkS6Z/the-steering-problem"},{"x":"12.111281","y":"8.840183","title":"Kelly bettors","cluster":"1","author":"['DanielFilan']","source":"alignment forum","tags":"Betting/Prediction Markets/Bayes' Theorem/Kelly Criterion","date":"2018-11-13T00:40","url":"https://www.lesswrong.com/posts/iWXQgwpksstozSDeA/kelly-bettors"},{"x":"10.95489","y":"8.245973","title":"Future directions for ambitious value learning","cluster":"1","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning","date":"2018-11-11T15:53","url":"https://www.lesswrong.com/posts/EhNCnCkmu7MwrQ7yz/future-directions-for-ambitious-value-learning"},{"x":"8.978428","y":"7.2851605","title":"Preface to the sequence on iterated amplification","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification ","date":"2018-11-10T13:24","url":"https://www.lesswrong.com/posts/HCv2uwgDGf5dyX5y6/preface-to-the-sequence-on-iterated-amplification"},{"x":"11.362995","y":"6.9976897","title":"Model Mis-specification and Inverse Reinforcement Learning","cluster":"0","author":"['Owain_Evans', 'jsteinhardt']","source":"alignment forum","tags":"Value Learning/Inverse Reinforcement Learning/Reinforcement Learning","date":"2018-11-09T15:33","url":"https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning"},{"x":"11.094484","y":"8.7010975","title":"Multi-Agent Overoptimization, and Embedded Agent World Models","cluster":"4","author":"['Davidmanheim']","source":"alignment forum","tags":"","date":"2018-11-08T20:33","url":"https://www.lesswrong.com/posts/dfZLLEfFvkrMwmiMw/multi-agent-overoptimization-and-embedded-agent-world-models"},{"x":"9.742402","y":"8.981422","title":"Embedded Curiosities","cluster":"4","author":"['Scott Garrabrant', 'abramdemski']","source":"alignment forum","tags":"Embedded Agency/AI/Research Agendas","date":"2018-11-08T14:19","url":"https://www.lesswrong.com/posts/j9CbmSsnprxB2uFY9/embedded-curiosities"},{"x":"12.720521","y":"10.172644","title":"What are Universal Inductors, Again?","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-11-07T22:32","url":"https://www.lesswrong.com/posts/FnH2G832sWMySptp5/what-are-universal-inductors-again"},{"x":"10.646158","y":"8.114386","title":"Latent Variables and Model Mis-Specification","cluster":"4","author":"['jsteinhardt']","source":"alignment forum","tags":"Value Learning/Probability & Statistics","date":"2018-11-07T14:48","url":"https://www.lesswrong.com/posts/gnvrixhDfG7S2TpNL/latent-variables-and-model-mis-specification"},{"x":"10.594208","y":"7.599353","title":"Humans can be assigned any values whatsoever…","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Value Learning","date":"2018-11-05T14:26","url":"https://www.lesswrong.com/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever"},{"x":"10.984928","y":"9.727187","title":"When does rationality-as-search have nontrivial implications?","cluster":"1","author":"['nostalgebraist']","source":"alignment forum","tags":"Solomonoff Induction/Embedded Agency","date":"2018-11-04T22:42","url":"https://www.lesswrong.com/posts/BGcXEijZ6HLnASNit/when-does-rationality-as-search-have-nontrivial-implications"},{"x":"10.014227","y":"6.012629","title":"Beliefs at different timescales","cluster":"0","author":"['Nisan']","source":"alignment forum","tags":"Logical Uncertainty/Physics","date":"2018-11-04T20:10","url":"https://www.lesswrong.com/posts/yf4KcTyk2hoXZh9x4/beliefs-at-different-timescales"},{"x":"10.419323","y":"8.212536","title":"The easy goal inference problem is still hard","cluster":"4","author":"['paulfchristiano']","source":"alignment forum","tags":"Value Learning/AI","date":"2018-11-03T14:41","url":"https://www.lesswrong.com/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard"},{"x":"10.454517","y":"9.219695","title":"Meta-execution","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"Iterated Amplification /Humans Consulting HCH","date":"2018-11-01T22:18","url":"https://www.lesswrong.com/posts/4GXqbNvpJ4hJtcoSX/meta-execution"},{"x":"8.625517","y":"8.482955","title":"Discussion on the machine learning approach to AI safety","cluster":"4","author":"['Vika']","source":"alignment forum","tags":"AI/Machine Learning","date":"2018-11-01T20:54","url":"https://www.lesswrong.com/posts/5GFn87cmw7A5hzR89/discussion-on-the-machine-learning-approach-to-ai-safety"},{"x":"10.439524","y":"7.9339705","title":"What is ambitious value learning?","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning/AI","date":"2018-11-01T16:20","url":"https://www.lesswrong.com/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning"},{"x":"10.444533","y":"8.360424","title":"Preface to the sequence on value learning","cluster":"4","author":"['Rohin Shah']","source":"alignment forum","tags":"Value Learning","date":"2018-10-30T22:04","url":"https://www.lesswrong.com/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning"},{"x":"7.8067207","y":"8.200697","title":"Introducing the AI Alignment Forum (FAQ)","cluster":"3","author":"['habryka', 'Ben Pace', 'Raemon', 'jimrandomh']","source":"alignment forum","tags":"Site Meta/AI Risk/Q&A (format)","date":"2018-10-29T21:07","url":"https://www.lesswrong.com/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq"},{"x":"11.714044","y":"8.531808","title":"When EDT=CDT, ADT Does Well","cluster":"1","author":"['Diffractor']","source":"alignment forum","tags":"","date":"2018-10-25T05:03","url":"https://www.lesswrong.com/posts/pgJbaXvYWBx3Mrg5T/when-edt-cdt-adt-does-well"},{"x":"10.676494","y":"8.55539","title":"Possible Dangers of the Unrestricted Value Learners","cluster":"4","author":"['avturchin']","source":"alignment forum","tags":"","date":"2018-10-23T09:15","url":"https://www.lesswrong.com/posts/hzEaasJyQsutYDNfN/possible-dangers-of-the-unrestricted-value-learners"},{"x":"12.272754","y":"9.177609","title":"Addressing three problems with counterfactual corrigibility: bad bets, defending against backstops, and overconfidence.","cluster":"1","author":"['RyanCarey']","source":"alignment forum","tags":"Corrigibility/Counterfactuals","date":"2018-10-21T12:03","url":"https://www.lesswrong.com/posts/owdBiF8pj6Lpwwdup/addressing-three-problems-with-counterfactual-corrigibility"},{"x":"12.497971","y":"8.173443","title":"Standard ML Oracles vs Counterfactual ones","cluster":"2","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI/Counterfactuals","date":"2018-10-10T20:01","url":"https://www.lesswrong.com/posts/hJaJw6LK39zpyCKW6/standard-ml-oracles-vs-counterfactual-ones"},{"x":"10.867061","y":"9.274238","title":"Siren worlds and the perils of over-optimised search","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Futurism/Optimization/AI Risk/AI/Complexity of Value/Risks of Astronomical Suffering (S-risks)","date":"2014-04-07T11:00","url":"https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search"},{"x":"12.284966","y":"8.446774","title":"Single player extensive-form games as a model of UDT","cluster":"1","author":"['cousin_it']","source":"alignment forum","tags":"","date":"2014-02-25T10:43","url":"https://www.lesswrong.com/posts/W4sDWwGZ4puRBXMEZ/single-player-extensive-form-games-as-a-model-of-udt"},{"x":"12.885316","y":"9.8951845","title":"Optimizing arbitrary expressions with a linear number of queries to a Logical Induction Oracle (Cartoon Guide)","cluster":"1","author":"['Donald Hobson']","source":"alignment forum","tags":"Logical Induction/Rationality/AI/Logical Uncertainty/Oracle AI","date":"2020-07-23T21:37","url":"https://www.lesswrong.com/posts/H32NbFcqjTxy2pvaq/optimizing-arbitrary-expressions-with-a-linear-number-of"},{"x":"8.229002","y":"6.384636","title":"Can you get AGI from a Transformer?","cluster":"2","author":"['Steven Byrnes']","source":"alignment forum","tags":"GPT/AI/AI Timelines","date":"2020-07-23T15:27","url":"https://www.lesswrong.com/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer"},{"x":"12.173032","y":"8.568082","title":"Weak HCH accesses EXP","cluster":"1","author":"['evhub']","source":"alignment forum","tags":"AI/Formal Proof","date":"2020-07-22T22:36","url":"https://www.lesswrong.com/posts/CtGH3yEoo4mY2taxe/weak-hch-accesses-exp"},{"x":"8.018003","y":"6.1193","title":"[Preprint] The Computational Limits of Deep Learning","cluster":"2","author":"['G Gordon Worley III']","source":"alignment forum","tags":"AI","date":"2020-07-21T21:25","url":"https://www.lesswrong.com/posts/buhaT2pxsfLrknzxT/preprint-the-computational-limits-of-deep-learning"},{"x":"8.797539","y":"9.625439","title":"Competition: Amplify Rohin’s Prediction on AGI researchers & Safety Concerns","cluster":"3","author":"['stuhlmueller']","source":"alignment forum","tags":"AI/Forecasting & Prediction/Rationality","date":"2020-07-21T20:06","url":"https://www.lesswrong.com/posts/Azqmzp5JoXJihMcr4/competition-amplify-rohin-s-prediction-on-agi-researchers"},{"x":"7.80296","y":"7.1756606","title":"Alignment As A Bottleneck To Usefulness Of GPT-3","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"GPT/AI/Outer Alignment","date":"2020-07-21T20:02","url":"https://www.lesswrong.com/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3"},{"x":"8.2015295","y":"7.469043","title":"$1000 bounty for OpenAI to show whether GPT3 was \"deliberately\" pretending to be stupider than it is","cluster":"4","author":"['jacobjacob']","source":"alignment forum","tags":"GPT/Bounties (active)/AI","date":"2020-07-21T18:42","url":"https://www.lesswrong.com/posts/H9knnv8BWGKj6dZim/usd1000-bounty-for-openai-to-show-whether-gpt3-was"},{"x":"8.791649","y":"9.930687","title":"AI Benefits Post 5: Outstanding Questions on Governing Benefits","cluster":"3","author":"['Cullen_OKeefe']","source":"alignment forum","tags":"AI","date":"2020-07-21T16:46","url":"https://www.lesswrong.com/posts/8e3FmHY4598SJ9PNL/ai-benefits-post-5-outstanding-questions-on-governing"},{"x":"9.596538","y":"10.030382","title":"Parallels Between AI Safety by Debate and Evidence Law","cluster":"3","author":"['Cullen_OKeefe']","source":"alignment forum","tags":"AI/Debate (AI safety technique)","date":"2020-07-20T22:52","url":"https://www.lesswrong.com/posts/SrsH2MyyH8MqH9QSr/parallels-between-ai-safety-by-debate-and-evidence-law"},{"x":"10.177289","y":"9.199275","title":"[Question] To what extent is GPT-3 capable of reasoning?","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"GPT/AI/AI Timelines","date":"2020-07-20T17:10","url":"https://www.lesswrong.com/posts/L5JSMZQvkBAx9MD5A/to-what-extent-is-gpt-3-capable-of-reasoning"},{"x":"9.792402","y":"6.7691245","title":"[Question] Why is pseudo-alignment \"worse\" than other ways ML can fail to generalize?","cluster":"0","author":"['nostalgebraist']","source":"alignment forum","tags":"AI/Academic Papers","date":"2020-07-18T22:54","url":"https://www.lesswrong.com/posts/TSmgTGaLyhL965jX6/why-is-pseudo-alignment-worse-than-other-ways-ml-can-fail-to"},{"x":"8.039929","y":"7.530969","title":"Collection of GPT-3 results","cluster":"2","author":"['Kaj_Sotala']","source":"alignment forum","tags":"GPT/AI/List of Links","date":"2020-07-18T20:04","url":"https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results"},{"x":"9.270032","y":"7.6553125","title":"Environments as a bottleneck in AGI development","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI/AI Timelines","date":"2020-07-17T05:02","url":"https://www.lesswrong.com/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development"},{"x":"12.04432","y":"8.43942","title":"Alignment proposals and complexity classes","cluster":"1","author":"['evhub']","source":"alignment forum","tags":"AI/Research Agendas/Formal Proof","date":"2020-07-16T00:27","url":"https://www.lesswrong.com/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes"},{"x":"9.615419","y":"9.997803","title":"[Question] How should AI debate be judged?","cluster":"4","author":"['abramdemski']","source":"alignment forum","tags":"Debate (AI safety technique)/AI","date":"2020-07-15T22:20","url":"https://www.lesswrong.com/posts/m7oGxvouzzeQKiGJH/how-should-ai-debate-be-judged"},{"x":"9.2462435","y":"8.434218","title":"New paper: AGI Agent Safety by Iteratively Improving the Utility Function","cluster":"4","author":"['Koen.Holtman']","source":"alignment forum","tags":"AI","date":"2020-07-15T14:05","url":"https://www.lesswrong.com/posts/HWRR8YzuM63yZyTPG/new-paper-agi-agent-safety-by-iteratively-improving-the"},{"x":"12.576608","y":"10.40324","title":"The Goldbach conjecture is probably correct; so was Fermat’s last theorem","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Logic & Mathematics /Rationality/World Modeling","date":"2020-07-14T19:30","url":"https://www.lesswrong.com/posts/iNFZG4d9W848zsgch/the-goldbach-conjecture-is-probably-correct-so-was-fermat-s"},{"x":"8.537171","y":"9.817671","title":"AI Benefits Post 4: Outstanding Questions on Selecting Benefits","cluster":"3","author":"['Cullen_OKeefe']","source":"alignment forum","tags":"AI","date":"2020-07-14T17:26","url":"https://www.lesswrong.com/posts/upYKjwjC67ovWKKMo/ai-benefits-post-4-outstanding-questions-on-selecting"},{"x":"12.26091","y":"8.554403","title":"What counts as defection?","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"Game Theory/Rationality/AI/Coordination / Cooperation","date":"2020-07-12T22:03","url":"https://www.lesswrong.com/posts/8LEPDY36jBYpijrSw/what-counts-as-defection"},{"x":"8.575282","y":"8.985513","title":"Talk: Key Issues In Near-Term AI Safety Research","cluster":"4","author":"['Aryeh Englander']","source":"alignment forum","tags":"AI","date":"2020-07-10T18:36","url":"https://www.lesswrong.com/posts/yijG7ptfqFBR8w885/talk-key-issues-in-near-term-ai-safety-research"},{"x":"9.924841","y":"7.7838316","title":"A space of proposals for building safe advanced AI","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-07-10T16:58","url":"https://www.lesswrong.com/posts/S9GxuAEeQomnLkeNt/a-space-of-proposals-for-building-safe-advanced-ai"},{"x":"9.804845","y":"6.9777274","title":"Mesa-Optimizers vs \"Steered Optimizers\"","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Mesa-Optimization/Optimization/Outer Alignment/Inner Alignment/Selection vs Control","date":"2020-07-10T16:49","url":"https://www.lesswrong.com/posts/SJXujr5a2NcoFebr4/mesa-optimizers-vs-steered-optimizers"},{"x":"11.266036","y":"8.206565","title":"Why is the impact penalty time-inconsistent?","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Impact Measures","date":"2020-07-09T17:26","url":"https://www.lesswrong.com/posts/YjqwTepi53MyM4omT/why-is-the-impact-penalty-time-inconsistent"},{"x":"11.183927","y":"6.3731613","title":"Arguments against myopic training","cluster":"0","author":"['Richard_Ngo']","source":"alignment forum","tags":"Myopia/AI","date":"2020-07-09T16:07","url":"https://www.lesswrong.com/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training"},{"x":"10.997746","y":"8.628223","title":"What does it mean to apply decision theory?","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Bounded Rationality/Law-Thinking/Decision Theory/Rationality/AI","date":"2020-07-08T20:31","url":"https://www.lesswrong.com/posts/wgdfBtLmByaKYovYe/what-does-it-mean-to-apply-decision-theory"},{"x":"8.849083","y":"7.4027066","title":"[Question] How \"honest\" is GPT-3?","cluster":"4","author":"['abramdemski']","source":"alignment forum","tags":"GPT/AI/Honesty","date":"2020-07-08T19:38","url":"https://www.lesswrong.com/posts/c3RsLTcxrvH4rXpBL/how-honest-is-gpt-3"},{"x":"11.255204","y":"8.166489","title":"Dynamic inconsistency of the inaction and initial state baseline","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Impact Measures","date":"2020-07-07T12:02","url":"https://www.lesswrong.com/posts/w8QBmgQwb83vDMXoz/dynamic-inconsistency-of-the-inaction-and-initial-state"},{"x":"8.461248","y":"9.76788","title":"AI Benefits Post 3: Direct and Indirect Approaches to AI Benefits","cluster":"3","author":"['Cullen_OKeefe']","source":"alignment forum","tags":"AI","date":"2020-07-06T18:48","url":"https://www.lesswrong.com/posts/q3xFWK3qcR7JGTxsv/ai-benefits-post-3-direct-and-indirect-approaches-to-ai"},{"x":"11.6375475","y":"9.761727","title":"Better priors as a safety problem","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"Rationality/AI","date":"2020-07-05T21:20","url":"https://www.lesswrong.com/posts/roA83jDvq7F2epnHK/better-priors-as-a-safety-problem"},{"x":"8.455299","y":"6.022462","title":"Learning the prior","cluster":"2","author":"['paulfchristiano']","source":"alignment forum","tags":"Priors/Rationality/AI","date":"2020-07-05T21:00","url":"https://www.lesswrong.com/posts/SL9mKhgdmDKXmxwE4/learning-the-prior"},{"x":"9.851707","y":"9.977241","title":"AI Unsafety via Non-Zero-Sum Debate","cluster":"1","author":"['VojtaKovarik']","source":"alignment forum","tags":"AI","date":"2020-07-03T22:03","url":"https://www.lesswrong.com/posts/BRiMQELD5WYyvncTE/ai-unsafety-via-non-zero-sum-debate"},{"x":"10.874742","y":"8.112409","title":"Goals and short descriptions","cluster":"4","author":"['Michele Campolo']","source":"alignment forum","tags":"Goal-Directedness/AI","date":"2020-07-02T17:41","url":"https://www.lesswrong.com/posts/d4NgfKY3cq9yiBLSM/goals-and-short-descriptions"},{"x":"9.530724","y":"9.8705015","title":"The \"AI Debate\" Debate","cluster":"4","author":"['michaelcohen']","source":"alignment forum","tags":"AI","date":"2020-07-02T10:16","url":"https://www.lesswrong.com/posts/L3QDs6of4Rb2TgpRD/the-ai-debate-debate"},{"x":"8.785908","y":"7.547478","title":"Evan Hubinger on Inner Alignment, Outer Alignment, and Proposals for Building Safe Advanced AI","cluster":"4","author":"['Palus Astra']","source":"alignment forum","tags":"AI/Interviews/Inner Alignment/Outer Alignment/Transcripts","date":"2020-07-01T17:30","url":"https://www.lesswrong.com/posts/qZGoHkRgANQpGHWnu/evan-hubinger-on-inner-alignment-outer-alignment-and"},{"x":"9.4233","y":"8.963392","title":"Comparing AI Alignment Approaches to Minimize False Positive Risk","cluster":"4","author":"['G Gordon Worley III']","source":"alignment forum","tags":"AI/Existential Risk/Value Learning/Debate (AI safety technique)","date":"2020-06-30T19:34","url":"https://www.lesswrong.com/posts/eXNy48LxxfgETdtYB/comparing-ai-alignment-approaches-to-minimize-false-positive"},{"x":"8.042379","y":"8.8982935","title":"Web AI discussion Groups","cluster":"3","author":"['Donald Hobson']","source":"alignment forum","tags":"Community/AI","date":"2020-06-30T11:22","url":"https://www.lesswrong.com/posts/omj76gXR67jsG4hxs/web-ai-discussion-groups"},{"x":"8.679179","y":"9.8369","title":"AI Benefits Post 2: How AI Benefits Differs from AI Alignment & AI for Good","cluster":"3","author":"['Cullen_OKeefe']","source":"alignment forum","tags":"AI","date":"2020-06-29T17:00","url":"https://www.lesswrong.com/posts/Z5XXdQDxhpgiXASQW/ai-benefits-post-2-how-ai-benefits-differs-from-ai-alignment"},{"x":"8.361673","y":"6.2319303","title":"Gary Marcus vs Cortical Uniformity","cluster":"2","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience/Neocortex","date":"2020-06-28T18:18","url":"https://www.lesswrong.com/posts/8F8dagB4q4BzR5JNz/gary-marcus-vs-cortical-uniformity"},{"x":"9.974325","y":"8.072154","title":"AI safety via market making","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI","date":"2020-06-26T23:07","url":"https://www.lesswrong.com/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making"},{"x":"12.239895","y":"10.071267","title":"Radical Probabilism [Transcript]","cluster":"1","author":"['abramdemski', 'Ben Pace']","source":"alignment forum","tags":"Rationality/Logical Induction/AI/Bayes' Theorem/Epistemology/Logical Uncertainty/Radical Probabilism/Transcripts","date":"2020-06-26T22:14","url":"https://www.lesswrong.com/posts/ZM63n353vh2ag7z4p/radical-probabilism-transcript"},{"x":"10.661439","y":"10.019628","title":"Abstraction, Evolution and Gears","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/World Modeling/Gears-Level","date":"2020-06-24T17:39","url":"https://www.lesswrong.com/posts/ahZQbxiPPpsTutDy2/abstraction-evolution-and-gears"},{"x":"10.196538","y":"8.033998","title":"Models, myths, dreams, and Cheshire cat grins","cluster":"2","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-06-24T10:50","url":"https://www.lesswrong.com/posts/hxzQoXjtLGRWPoLkE/models-myths-dreams-and-cheshire-cat-grins"},{"x":"7.614027","y":"8.757359","title":"Modelling Continuous Progress","cluster":"3","author":"['Sammy Martin']","source":"alignment forum","tags":"AI Takeoff/AI/World Modeling","date":"2020-06-23T18:06","url":"https://www.lesswrong.com/posts/66FKFkWAugS8diydF/modelling-continuous-progress"},{"x":"10.75522","y":"9.234348","title":"Locality of goals","cluster":"1","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/AI/Rationality","date":"2020-06-22T21:56","url":"https://www.lesswrong.com/posts/HkWB5KCJQ2aLsMzjt/locality-of-goals"},{"x":"11.218856","y":"10.004131","title":"The Indexing Problem","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/AI","date":"2020-06-22T19:11","url":"https://www.lesswrong.com/posts/ABNjLr2H39g2oXqGb/the-indexing-problem"},{"x":"8.27124","y":"8.869798","title":"AI Benefits Post 1: Introducing \"AI Benefits\"","cluster":"3","author":"['Cullen_OKeefe']","source":"alignment forum","tags":"AI","date":"2020-06-22T16:59","url":"https://www.lesswrong.com/posts/EGvtZMvSFELxoRqkZ/ai-benefits-post-1-introducing-ai-benefits"},{"x":"9.936908","y":"9.2525425","title":"Plausible cases for HRAD work, and locating the crux in the \"realism about rationality\" debate","cluster":"4","author":"['riceissa']","source":"alignment forum","tags":"AI","date":"2020-06-22T01:10","url":"https://www.lesswrong.com/posts/BGxTpdBGbwCWrGiCL/plausible-cases-for-hrad-work-and-locating-the-crux-in-the"},{"x":"8.881971","y":"8.810668","title":"Relevant pre-AGI possibilities","cluster":"4","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/Computing Overhang/Narrow AI","date":"2020-06-20T10:52","url":"https://www.lesswrong.com/posts/zjhZpZi76kEBRnjiw/relevant-pre-agi-possibilities"},{"x":"10.433318","y":"7.814721","title":"The ground of optimization","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"Optimization/AI/World Modeling/General Intelligence/Selection vs Control","date":"2020-06-20T00:38","url":"https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1"},{"x":"12.923004","y":"8.994607","title":"Results of $1,000 Oracle contest!","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Oracle AI/Bounties (closed)/AI Boxing (Containment)","date":"2020-06-17T17:44","url":"https://www.lesswrong.com/posts/YbYFeZQWncy9Tzzq9/results-of-usd1-000-oracle-contest"},{"x":"8.975803","y":"7.9022527","title":"Our take on CHAI’s research agenda in under 1500 words","cluster":"4","author":"['Alex Flint']","source":"alignment forum","tags":"Research Agendas/AI/Inverse Reinforcement Learning/Center for Human-Compatible AI (CHAI)","date":"2020-06-17T12:24","url":"https://www.lesswrong.com/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words"},{"x":"12.943454","y":"9.357306","title":"Relating HCH and Logical Induction","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Logical Induction/Humans Consulting HCH/AI/Logical Uncertainty","date":"2020-06-16T22:08","url":"https://www.lesswrong.com/posts/R3HAvMGFNJGXstckQ/relating-hch-and-logical-induction"},{"x":"7.8241234","y":"7.9578032","title":"[Question] What are the high-level approaches to AI alignment?","cluster":"3","author":"['G Gordon Worley III']","source":"alignment forum","tags":"AI","date":"2020-06-16T17:10","url":"https://www.lesswrong.com/posts/H9sxfAZGGAsx5BdYD/what-are-the-high-level-approaches-to-ai-alignment"},{"x":"11.607241","y":"10.355368","title":"Causality Adds Up to Normality","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/Rationality/World Modeling/Adding Up to Normality","date":"2020-06-15T17:19","url":"https://www.lesswrong.com/posts/hgSKz3RkSSgZXrXNp/causality-adds-up-to-normality"},{"x":"8.571557","y":"9.321419","title":"Preparing for \"The Talk\" with AI projects","cluster":"3","author":"['Daniel Kokotajlo']","source":"alignment forum","tags":"AI/World Optimization","date":"2020-06-13T23:01","url":"https://www.lesswrong.com/posts/QSBgGv8byWMjmaGE5/preparing-for-the-talk-with-ai-projects"},{"x":"10.833113","y":"10.272768","title":"Cartesian Boundary as Abstraction Boundary","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/World Modeling","date":"2020-06-11T17:38","url":"https://www.lesswrong.com/posts/JasCkaPtZEJsYDX8H/cartesian-boundary-as-abstraction-boundary"},{"x":"10.968521","y":"10.390654","title":"Public Static: What is Abstraction?","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/World Modeling","date":"2020-06-09T18:36","url":"https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction"},{"x":"7.7631965","y":"8.949595","title":"More on disambiguating \"discontinuity\"","cluster":"3","author":"['Aryeh Englander']","source":"alignment forum","tags":"AI/World Modeling/AI Takeoff","date":"2020-06-09T15:16","url":"https://www.lesswrong.com/posts/C9YMrPAyMXfB8cLPb/more-on-disambiguating-discontinuity"},{"x":"9.975611","y":"9.022092","title":"Goal-directedness is behavioral, not structural","cluster":"4","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/AI/Rationality","date":"2020-06-08T23:05","url":"https://www.lesswrong.com/posts/9pxcekdNjE7oNwvcC/goal-directedness-is-behavioral-not-structural"},{"x":"8.877004","y":"6.142205","title":"Reply to Paul Christiano on Inaccessible Information","cluster":"2","author":"['Alex Flint']","source":"alignment forum","tags":"AI","date":"2020-06-05T09:10","url":"https://www.lesswrong.com/posts/A9vvxguZMytsN3ze9/reply-to-paul-christiano-on-inaccessible-information"},{"x":"10.641188","y":"7.9265203","title":"Focus: you are allowed to be bad at accomplishing your goals","cluster":"0","author":"['adamShimi']","source":"alignment forum","tags":"Goal-Directedness/AI/Rationality","date":"2020-06-03T21:04","url":"https://www.lesswrong.com/posts/X5WTgfX5Ly4ZNHWZD/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals"},{"x":"10.433334","y":"9.467853","title":"Inaccessible information","cluster":"1","author":"['paulfchristiano']","source":"alignment forum","tags":"AI","date":"2020-06-03T05:10","url":"https://www.lesswrong.com/posts/ZyWyAJbedvEgRT2uF/inaccessible-information"},{"x":"9.0098095","y":"7.3484626","title":"Building brain-inspired AGI is infinitely easier than understanding the brain","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/Neuroscience","date":"2020-06-02T14:13","url":"https://www.lesswrong.com/posts/PTkd8nazvH9HQpwP8/building-brain-inspired-agi-is-infinitely-easier-than"},{"x":"8.488199","y":"6.21657","title":"Sparsity and interpretability?","cluster":"2","author":"['spirali', 'RobertKirk', 'Tomáš Gavenčiak']","source":"alignment forum","tags":"World Modeling/AI/Transparency / Interpretability (ML & AI)","date":"2020-06-01T13:25","url":"https://www.lesswrong.com/posts/maBNBgopYxb9YZP8B/sparsity-and-interpretability-1"},{"x":"9.01667","y":"7.7699337","title":"An overview of 11 proposals for building safe advanced AI","cluster":"4","author":"['evhub']","source":"alignment forum","tags":"AI/AI Success Models/Iterated Amplification /Debate (AI safety technique)/Inner Alignment/Outer Alignment/Myopia/AI Risk","date":"2020-05-29T20:38","url":"https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai"},{"x":"8.440905","y":"9.512168","title":"AI Safety Discussion Days","cluster":"3","author":"['Linda Linsefors']","source":"alignment forum","tags":"AI/Community","date":"2020-05-27T16:54","url":"https://www.lesswrong.com/posts/32QD3tRfognNHN9xw/ai-safety-discussion-days"},{"x":"9.408121","y":"9.614612","title":"How can Interpretability help Alignment?","cluster":"3","author":"['RobertKirk', 'Tomáš Gavenčiak', 'flodorner']","source":"alignment forum","tags":"Transparency / Interpretability (ML & AI)/AI/Machine Learning","date":"2020-05-23T16:16","url":"https://www.lesswrong.com/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment"},{"x":"9.1865","y":"8.158068","title":"AGIs as collectives","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-05-22T20:36","url":"https://www.lesswrong.com/posts/HekjhtWesBWTQW5eF/agis-as-collectives"},{"x":"11.658213","y":"6.7588425","title":"Comparing reward learning/​reward tampering formalisms","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-05-21T12:03","url":"https://www.lesswrong.com/posts/MBrhMSZno6qbfGQdZ/comparing-reward-learning-reward-tampering-formalisms"},{"x":"11.657455","y":"7.062213","title":"Probabilities, weights, sums: pretty much the same for reward functions","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Reward Functions","date":"2020-05-20T15:19","url":"https://www.lesswrong.com/posts/pxWEKPHNBzXZWi2rB/probabilities-weights-sums-pretty-much-the-same-for-reward"},{"x":"11.533118","y":"6.790256","title":"Learning and manipulating learning","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-05-19T13:02","url":"https://www.lesswrong.com/posts/LpjjWDBXr88gzcYK2/learning-and-manipulating-learning"},{"x":"10.77714","y":"10.458694","title":"Pointing to a Flower","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/World Modeling/AI","date":"2020-05-18T18:54","url":"https://www.lesswrong.com/posts/3xotPYdAs7GfT9a9r/pointing-to-a-flower"},{"x":"10.395943","y":"9.285276","title":"The Mechanistic and Normative Structure of Agency","cluster":"1","author":"['G Gordon Worley III']","source":"alignment forum","tags":"Rationality/AI","date":"2020-05-18T16:03","url":"https://www.lesswrong.com/posts/QBHxfATzdASQcXwan/the-mechanistic-and-normative-structure-of-agency"},{"x":"11.378819","y":"7.2626143","title":"Reward functions and updating assumptions can hide a multitude of sins","cluster":"0","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI/Reward Functions","date":"2020-05-18T15:18","url":"https://www.lesswrong.com/posts/EYEkYX6vijL7zsKEt/reward-functions-and-updating-assumptions-can-hide-a"},{"x":"12.174535","y":"8.424528","title":"Why you should minimax in two-player zero-sum games","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"World Optimization/Rationality","date":"2020-05-17T20:48","url":"https://www.lesswrong.com/posts/ukZuzb8JpYiFLoord/why-you-should-minimax-in-two-player-zero-sum-games"},{"x":"9.401573","y":"7.8635683","title":"Multi-agent safety","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-05-16T01:59","url":"https://www.lesswrong.com/posts/BXMCgpktdiawT3K5v/multi-agent-safety"},{"x":"10.127366","y":"9.298342","title":"Conjecture Workshop","cluster":"4","author":"['johnswentworth']","source":"alignment forum","tags":"AI/Rationality","date":"2020-05-15T22:41","url":"https://www.lesswrong.com/posts/uDmiEvPtJnRcrbHB6/conjecture-workshop"},{"x":"11.2344","y":"7.5828257","title":"How should AIs update a prior over human preferences?","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-05-15T13:14","url":"https://www.lesswrong.com/posts/gbuwgyYG9WvtsErki/how-should-ais-update-a-prior-over-human-preferences"},{"x":"12.202634","y":"8.452681","title":"Book report: Theory of Games and Economic Behavior (von Neumann & Morgenstern)","cluster":"1","author":"['Nisan']","source":"alignment forum","tags":"Game Theory/World Modeling/Book Reviews","date":"2020-05-11T09:47","url":"https://www.lesswrong.com/posts/qRKyZGcoio9JhdmvX/book-report-theory-of-games-and-economic-behavior-von"},{"x":"10.627236","y":"9.0794525","title":"Corrigibility as outside view","cluster":"1","author":"['TurnTrout']","source":"alignment forum","tags":"AI/Inside/Outside View/Corrigibility","date":"2020-05-08T21:56","url":"https://www.lesswrong.com/posts/BMj6uMuyBidrdZkiD/corrigibility-as-outside-view"},{"x":"10.248949","y":"6.9235854","title":"Specification gaming: the flip side of AI ingenuity","cluster":"0","author":"['Vika', 'vlad_m', 'Matthew Rahtz', 'tom4everitt', 'Zac Kenton', 'janleike']","source":"alignment forum","tags":"AI/Goodhart's Law","date":"2020-05-06T23:51","url":"https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity"},{"x":"8.531666","y":"6.613278","title":"Maths writer/​cowritter needed: how you can’t distinguish early exponential from early sigmoid","cluster":"2","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"World Modeling","date":"2020-05-06T09:41","url":"https://www.lesswrong.com/posts/zCq4ca3tTcfQgrFZM/maths-writer-cowritter-needed-how-you-can-t-distinguish"},{"x":"12.234505","y":"9.493616","title":"Modeling naturalized decision problems in linear logic","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"AI/Decision Theory","date":"2020-05-06T00:15","url":"https://www.lesswrong.com/posts/bcFhPHcDRbWKcAEfk/modeling-naturalized-decision-problems-in-linear-logic"},{"x":"9.305409","y":"7.961332","title":"Competitive safety via gradated curricula","cluster":"4","author":"['Richard_Ngo']","source":"alignment forum","tags":"AI","date":"2020-05-05T18:11","url":"https://www.lesswrong.com/posts/vLepnCxCWW6YTw8eW/competitive-safety-via-gradated-curricula"},{"x":"11.878163","y":"10.1939535","title":"Writing Causal Models Like We Write Programs","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/World Modeling/World Modeling Techniques","date":"2020-05-05T18:05","url":"https://www.lesswrong.com/posts/Xd9FLs4geRAWxkQPE/writing-causal-models-like-we-write-programs"},{"x":"8.34072","y":"6.282489","title":"How uniform is the neocortex?","cluster":"2","author":"['zhukeepa']","source":"alignment forum","tags":"Predictive Processing/Neuroscience/World Modeling/AI/Neocortex","date":"2020-05-04T02:16","url":"https://www.lesswrong.com/posts/WFopenhCXyHX3ukw3/how-uniform-is-the-neocortex"},{"x":"9.547461","y":"7.5164185","title":"[Question] How does iterated amplification exceed human abilities?","cluster":"4","author":"['riceissa']","source":"alignment forum","tags":"AI/Iterated Amplification ","date":"2020-05-02T23:44","url":"https://www.lesswrong.com/posts/ajQzejMYizfX4dMWK/how-does-iterated-amplification-exceed-human-abilities"},{"x":"11.56544","y":"10.40655","title":"Topological metaphysics: relating point-set topology and locale theory","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"World Modeling","date":"2020-05-01T03:57","url":"https://www.lesswrong.com/posts/yTvZFzcgt7rGYMxP5/topological-metaphysics-relating-point-set-topology-and"},{"x":"10.899197","y":"9.696522","title":"Motivating Abstraction-First Decision Theory","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/Rationality/AI","date":"2020-04-29T17:47","url":"https://www.lesswrong.com/posts/oQHoy2tnLsKuEDYtJ/motivating-abstraction-first-decision-theory"},{"x":"11.9399605","y":"9.324711","title":"What makes counterfactuals comparable?","cluster":"1","author":"['Chris_Leong']","source":"alignment forum","tags":"Decision Theory/AI/Counterfactuals","date":"2020-04-24T22:47","url":"https://www.lesswrong.com/posts/6E6D3qLPM3urXDPpK/what-makes-counterfactuals-comparable-1"},{"x":"9.705536","y":"7.563967","title":"Problem relaxation as a tactic","cluster":"4","author":"['TurnTrout']","source":"alignment forum","tags":"Techniques/Rationality/AI","date":"2020-04-22T23:44","url":"https://www.lesswrong.com/posts/JcpwEKbmNHdwhpq5n/problem-relaxation-as-a-tactic"},{"x":"9.3290615","y":"7.524873","title":"Inner alignment in the brain","cluster":"4","author":"['Steven Byrnes']","source":"alignment forum","tags":"AI/World Modeling/Neuroscience/Inner Alignment/Neocortex","date":"2020-04-22T13:14","url":"https://www.lesswrong.com/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain"},{"x":"11.163543","y":"9.263263","title":"Databases of human behaviour and preferences?","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-04-21T18:06","url":"https://www.lesswrong.com/posts/fx8Mdorwmt696Ramm/databases-of-human-behaviour-and-preferences"},{"x":"11.875167","y":"10.024476","title":"Intuitions on Universal Behavior of Information at a Distance","cluster":"1","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/World Modeling/Rationality/Intuition","date":"2020-04-20T21:44","url":"https://www.lesswrong.com/posts/CxEbvETK2WNfHw7v9/intuitions-on-universal-behavior-of-information-at-a"},{"x":"8.813846","y":"8.643914","title":"AI Services as a Research Paradigm","cluster":"3","author":"['VojtaKovarik']","source":"alignment forum","tags":"AI/AI Services (CAIS)","date":"2020-04-20T13:00","url":"https://www.lesswrong.com/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm"},{"x":"11.981217","y":"10.259257","title":"Integrating Hidden Variables Improves Approximation","cluster":"2","author":"['johnswentworth']","source":"alignment forum","tags":"Abstraction/AI/World Modeling","date":"2020-04-16T21:43","url":"https://www.lesswrong.com/posts/4vrL94CqXuyHQMhqo/integrating-hidden-variables-improves-approximation"},{"x":"9.011696","y":"9.353585","title":"AI Alignment Podcast: An Overview of Technical AI Alignment in 2018 and 2019 with Buck Shlegeris and Rohin Shah","cluster":"3","author":"['Palus Astra']","source":"alignment forum","tags":"AI/Interviews/Value Learning/Moral Uncertainty/Regulation and AI Risk/Transcripts","date":"2020-04-16T00:50","url":"https://www.lesswrong.com/posts/6skeZgctugzBBEBw3/ai-alignment-podcast-an-overview-of-technical-ai-alignment"},{"x":"11.415399","y":"8.238478","title":"\"How conservative\" should the partial maximisers be?","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"AI","date":"2020-04-13T15:50","url":"https://www.lesswrong.com/posts/jRHLCyRKsQv5u2Lph/how-conservative-should-the-partial-maximisers-be"},{"x":"11.45959","y":"9.453646","title":"An Orthodox Case Against Utility Functions","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"Utility Functions/Decision Theory/AI/Rationality/Indexical Information","date":"2020-04-07T19:18","url":"https://www.lesswrong.com/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions"},{"x":"8.469873","y":"8.8400545","title":"Resources for AI Alignment Cartography","cluster":"3","author":"['Gyrodiot']","source":"alignment forum","tags":"AI/Research Agendas","date":"2020-04-04T14:20","url":"https://www.lesswrong.com/posts/4az2cFrJp3ya4y6Wx/resources-for-ai-alignment-cartography"},{"x":"8.015617","y":"8.968877","title":"Announcing Web-TAISU, May 13-17","cluster":"3","author":"['Linda Linsefors']","source":"alignment forum","tags":"Events (Community)","date":"2020-04-04T11:48","url":"https://www.lesswrong.com/posts/CMnMaTxNAhXfcEtgm/announcing-web-taisu-may-13-17"},{"x":"11.370583","y":"9.5202875","title":"Harsanyi’s Social Aggregation Theorem and what it means for CEV","cluster":"1","author":"['AlexMennen']","source":"alignment forum","tags":"Utility Functions/Coherent Extrapolated Volition","date":"2013-01-05T21:38","url":"https://www.lesswrong.com/posts/z8afQRsH9wWsB4iMD/harsanyi-s-social-aggregation-theorem-and-what-it-means-for"},{"x":"12.682518","y":"10.3719635","title":"No Good Logical Conditional Probability","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-09-02T03:25","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f7d/no-good-logical-conditional-probability"},{"x":"13.011856","y":"10.204916","title":"Optimal predictor schemes pass a Benford test","cluster":"1","author":"['Vanessa Kosoy']","source":"alignment forum","tags":"","date":"2015-08-30T13:25","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375010/optimal-predictor-schemes-pass-a-benford-test"},{"x":"12.563164","y":"9.487207","title":"Obstacle to modal optimality when you’re being modalized","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-08-29T20:41","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ebd/obstacle-to-modal-optimality-when-you-re-being-modalized"},{"x":"12.885446","y":"10.291724","title":"Asymptotic Logical Uncertainty: Irreducible Patterns","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-28T22:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fb4/asymptotic-logical-uncertainty-irreducible-patterns"},{"x":"12.620661","y":"10.331959","title":"The Two-Update Problem: Monotonicity","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-28T20:28","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375015/the-two-update-problem-monotonicity"},{"x":"12.903921","y":"10.187834","title":"Asymptotic Logical Uncertainty: Iterated Resource Bounded Solomonoff Induction","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-24T04:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037500e/asymptotic-logical-uncertainty-iterated-resource-bounded-solomonoff-induction"},{"x":"12.610984","y":"10.279576","title":"Asymptotic Logical Uncertainty: Introduction","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-24T04:38","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f6b/asymptotic-logical-uncertainty-introduction"},{"x":"12.799864","y":"10.31275","title":"Asymptotic Logical Uncertainty: Self Reference","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-24T03:22","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037500d/asymptotic-logical-uncertainty-self-reference"},{"x":"12.829946","y":"10.234708","title":"Asymptotic Logical Uncertainty: A Benford Learner","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-24T02:14","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fbc/asymptotic-logical-uncertainty-a-benford-learner"},{"x":"12.774809","y":"10.275791","title":"Asymptotic Logical Uncertainty: Uniform Coherence 2","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-23T20:50","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ffc/asymptotic-logical-uncertainty-uniform-coherence-2"},{"x":"12.701627","y":"10.311326","title":"Asymptotic Logical Uncertainty: The Modified Demski Prior is Uniformly Coherent","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-23T20:48","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037500c/asymptotic-logical-uncertainty-the-modified-demski-prior-is-uniformly-coherent"},{"x":"12.804085","y":"10.352787","title":"Asymptotic Logical Uncertainty: A Modification to the Demski Prior","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-23T20:47","url":"https://www.lesswrong.com/posts/5bd75cc58225bf067037500a/asymptotic-logical-uncertainty-a-modification-to-the-demski-prior"},{"x":"12.708534","y":"10.195297","title":"The Two-Update Problem, Part 2","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-08-22T00:47","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670375009/the-two-update-problem-part-2"},{"x":"12.917122","y":"10.300206","title":"Asymptotic Logical Uncertainty: Uniform Coherence","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-14T19:59","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fe5/asymptotic-logical-uncertainty-uniform-coherence"},{"x":"12.629748","y":"8.647329","title":"Modal SAT: Introduction","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-08-08T20:37","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fef/modal-sat-introduction"},{"x":"12.593903","y":"9.104052","title":"Infinite Modal Combat: some observations","cluster":"1","author":"['János Kramár']","source":"alignment forum","tags":"","date":"2015-07-29T04:05","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fdb/infinite-modal-combat-some-observations"},{"x":"12.85064","y":"9.454947","title":"A problem with resource-bounded Solomonoff induction and unpredictable environments","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-07-27T03:03","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fdc/a-problem-with-resource-bounded-solomonoff-induction-and-unpredictable-environments"},{"x":"13.040321","y":"10.1784","title":"Asymptotic Logical Uncertainty: Concrete Failure of the Solomonoff Approach","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Solomonoff Induction/Logical Induction/Logical Uncertainty","date":"2015-07-22T19:27","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fcb/asymptotic-logical-uncertainty-concrete-failure-of-the-solomonoff-approach"},{"x":"9.520255","y":"7.487079","title":"An Idea For Corrigible, Recursively Improving Math Oracles","cluster":"4","author":"['jimrandomh']","source":"alignment forum","tags":"Corrigibility/Oracle AI","date":"2015-07-20T03:35","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fd3/an-idea-for-corrigible-recursively-improving-math-oracles"},{"x":"12.548927","y":"10.083255","title":"A Counterexample to an Informal Conjecture on Proof Length and Logical Counterfactuals","cluster":"1","author":"['SamEisenstat']","source":"alignment forum","tags":"","date":"2015-07-17T17:34","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fce/a-counterexample-to-an-informal-conjecture-on-proof-length-and-logical-counterfactuals"},{"x":"12.610376","y":"10.067942","title":"Waterfall Truth Predicates","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-07-15T03:14","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fc4/waterfall-truth-predicates"},{"x":"9.795247","y":"9.123309","title":"An overall schema for the friendly AI problems: self-referential convergence criteria","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-07-13T15:40","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fc5/an-overall-schema-for-the-friendly-ai-problems-self-referential-convergence-criteria"},{"x":"9.582264","y":"8.562663","title":"The AI, the best human advisor","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-07-13T15:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fc6/the-ai-the-best-human-advisor"},{"x":"12.831725","y":"10.346735","title":"Asymptotic Logical Uncertainty: Connection to Random Logical Extensions.","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-07-06T22:55","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fbf/asymptotic-logical-uncertainty-connection-to-random-logical-extensions"},{"x":"12.887046","y":"10.209528","title":"Asymptotic Logical Uncertainty: Passing the Benford Test","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-07-06T03:34","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fbd/asymptotic-logical-uncertainty-passing-the-benford-test"},{"x":"12.51941","y":"10.116587","title":"Fixed point theorem in the finite and infinite case","cluster":"1","author":"['Vika']","source":"alignment forum","tags":"","date":"2015-07-06T01:42","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fba/fixed-point-theorem-in-the-finite-and-infinite-case"},{"x":"12.040063","y":"9.809507","title":"Vingean Reflection: Open Problems","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-07-03T18:44","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f9d/vingean-reflection-open-problems"},{"x":"12.469295","y":"10.042397","title":"An approach to logical counterfactuals inspired by the Demski prior","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-07-03T17:26","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374fa6/an-approach-to-logical-counterfactuals-inspired-by-the-demski-prior"},{"x":"12.972458","y":"10.22357","title":"Asymptotic Logical Uncertainty: Solomonoff Induction Inspired Approach","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-06-22T18:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f99/asymptotic-logical-uncertainty-solomonoff-induction-inspired-approach"},{"x":"12.196017","y":"9.279828","title":"A simple model of the Löbstacle","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-06-11T16:23","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f8c/a-simple-model-of-the-loebstacle"},{"x":"11.198475","y":"8.3152275","title":"Structural Risk Minimization","cluster":"1","author":"['abramdemski']","source":"alignment forum","tags":"","date":"2015-06-07T03:40","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f81/structural-risk-minimization"},{"x":"12.866409","y":"9.32358","title":"Agent Simulates Predictor using Second-Level Oracles","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-06-06T22:08","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f87/agent-simulates-predictor-using-second-level-oracles"},{"x":"12.798699","y":"10.2778225","title":"A tractable, interpretable formulation of approximate conditioning for pairwise-specified probability distributions over truth values","cluster":"1","author":"['János Kramár']","source":"alignment forum","tags":"","date":"2015-06-03T19:08","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f7f/a-tractable-interpretable-formulation-of-approximate-conditioning-for-pairwise-specified-probability-distributions-over-truth-values"},{"x":"12.981032","y":"9.184428","title":"An Oracle standard trick","cluster":"1","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"Oracle AI","date":"2015-06-03T14:25","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f85/an-oracle-standard-trick"},{"x":"13.007368","y":"10.2105055","title":"Asymptotic Logical Uncertainty: The Benford Test","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-05-31T00:30","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f6c/asymptotic-logical-uncertainty-the-benford-test"},{"x":"12.377078","y":"8.912973","title":"Agents that can predict their Newcomb predictor","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-05-19T10:17","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f4f/agents-that-can-predict-their-newcomb-predictor"},{"x":"12.566516","y":"10.131096","title":"An Informal Conjecture on Proof Length and Logical Counterfactuals","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"","date":"2015-05-15T20:31","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f60/an-informal-conjecture-on-proof-length-and-logical-counterfactuals"},{"x":"12.486237","y":"9.329326","title":"PA+100 cannot always predict modal UDT","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-05-12T20:26","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f5f/pa-100-cannot-always-predict-modal-udt"},{"x":"12.396263","y":"9.927237","title":"Optimal and Causal Counterfactual Worlds","cluster":"1","author":"['Scott Garrabrant']","source":"alignment forum","tags":"Counterfactuals","date":"2015-05-12T03:16","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f4e/optimal-and-causal-counterfactual-worlds"},{"x":"12.699147","y":"8.565767","title":"MIRIx Stanford report","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-05-11T06:11","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f58/mirix-stanford-report"},{"x":"12.489074","y":"10.298141","title":"Reflective probabilistic logic cannot assign positive probability to its own coherence and an inner reflection principle","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-05-07T21:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f47/reflective-probabilistic-logic-cannot-assign-positive-probability-to-its-own-coherence-and-an-inner-reflection-principle"},{"x":"9.522475","y":"8.570886","title":"High impact from low impact, continued","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-04-28T13:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f44/high-impact-from-low-impact-continued"},{"x":"8.395825","y":"5.672139","title":"Learning a concept using only positive examples","cluster":"2","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-04-28T03:57","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f3c/learning-a-concept-using-only-positive-examples"},{"x":"9.616625","y":"8.744469","title":"High impact from low impact","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-04-17T16:01","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f29/high-impact-from-low-impact"},{"x":"12.453661","y":"8.628074","title":"Modal Bargaining Agents","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"Decision Theory/Game Theory","date":"2015-04-16T22:19","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f20/modal-bargaining-agents"},{"x":"10.822746","y":"8.333765","title":"Minimax as an approach to reduced-impact AI","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"","date":"2015-04-02T22:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f17/minimax-as-an-approach-to-reduced-impact-ai"},{"x":"9.66749","y":"8.838566","title":"Utility vs Probability: idea synthesis","cluster":"4","author":"['Stuart_Armstrong']","source":"alignment forum","tags":"","date":"2015-03-27T12:40","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f13/utility-vs-probability-idea-synthesis"},{"x":"12.82822","y":"9.080815","title":"Reflective oracles and the procrastination paradox","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Oracle AI","date":"2015-03-26T22:18","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f11/reflective-oracles-and-the-procrastination-paradox"},{"x":"9.637559","y":"8.704499","title":"Forum Digest: Corrigibility, utility indifference, & related control ideas","cluster":"4","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2015-03-24T17:39","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f04/forum-digest-corrigibility-utility-indifference-and-related-control-ideas"},{"x":"10.372648","y":"8.428584","title":"Welcome, new contributors!","cluster":"4","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-03-23T21:53","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374efa/welcome-new-contributors"},{"x":"12.957558","y":"9.143248","title":"Forum Digest: Reflective Oracles","cluster":"1","author":"['jessicata']","source":"alignment forum","tags":"Oracle AI","date":"2015-03-22T04:02","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374f02/forum-digest-reflective-oracles"},{"x":"12.036129","y":"9.386674","title":"Forum Digest: Updateless Decision Theory","cluster":"1","author":"['orthonormal']","source":"alignment forum","tags":"","date":"2015-03-20T00:22","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374efd/forum-digest-updateless-decision-theory"},{"x":"13.064661","y":"9.329944","title":"Single-bit reflective oracles are enough","cluster":"1","author":"['Benya_Fallenstein']","source":"alignment forum","tags":"","date":"2015-03-17T23:00","url":"https://www.lesswrong.com/posts/5bd75cc58225bf0670374ef8/single-bit-reflective-oracles-are-enough"},{"x":"11.385368","y":"5.764093","title":"Model Primitive Hierarchical Lifelong Reinforcement Learning","cluster":"0","author":"['Bohan Wu', 'Jayesh K. Gupta', 'Mykel J. Kochenderfer']","source":"arxiv","tags":"[]","date":"2019-03-04 22:14:23+00:00","url":"http://arxiv.org/abs/1903.01567v1"},{"x":"9.956619","y":"6.4993134","title":"Toward an AI Physicist for Unsupervised Learning","cluster":"0","author":"['Tailin Wu', 'Max Tegmark']","source":"arxiv","tags":"[]","date":"2018-10-24 17:59:57+00:00","url":"http://arxiv.org/abs/1810.10525v4"},{"x":"8.0921755","y":"9.77415","title":"The Role of Cooperation in Responsible AI Development","cluster":"3","author":"['Amanda Askell', 'Miles Brundage', 'Gillian Hadfield']","source":"arxiv","tags":"[]","date":"2019-07-10 06:51:04+00:00","url":"http://arxiv.org/abs/1907.04534v1"},{"x":"11.554538","y":"6.344413","title":"Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior","cluster":"0","author":"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2018-05-21 12:15:34+00:00","url":"http://arxiv.org/abs/1805.08010v4"},{"x":"11.642416","y":"8.730827","title":"Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings","cluster":"1","author":"['Tom Everitt', 'Pedro A. Ortega', 'Elizabeth Barnes', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2019-02-26 14:54:09+00:00","url":"http://arxiv.org/abs/1902.09980v7"},{"x":"7.8909907","y":"5.5006666","title":"Xception: Deep Learning with Depthwise Separable Convolutions","cluster":"2","author":"['François Chollet']","source":"arxiv","tags":"[]","date":"2016-10-07 17:51:51+00:00","url":"http://arxiv.org/abs/1610.02357v3"},{"x":"8.144841","y":"5.1456585","title":"Adversarial images for the primate brain","cluster":"2","author":"['Li Yuan', 'Will Xiao', 'Gabriel Kreiman', 'Francis E. H. Tay', 'Jiashi Feng', 'Margaret S. Livingstone']","source":"arxiv","tags":"[]","date":"2020-11-11 08:30:54+00:00","url":"http://arxiv.org/abs/2011.05623v2"},{"x":"11.83243","y":"8.358949","title":"Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making","cluster":"1","author":"['Andrew Critch', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2017-10-31 05:09:13+00:00","url":"http://arxiv.org/abs/1711.00363v1"},{"x":"9.344486","y":"6.6277356","title":"Abstraction Learning","cluster":"2","author":"['Fei Deng', 'Jinsheng Ren', 'Feng Chen']","source":"arxiv","tags":"[]","date":"2018-09-11 15:02:24+00:00","url":"http://arxiv.org/abs/1809.03956v1"},{"x":"11.344435","y":"5.3427567","title":"O2A: One-shot Observational learning with Action vectors","cluster":"0","author":"['Leo Pauly', 'Wisdom C. Agboh', 'David C. Hogg', 'Raul Fuentes']","source":"arxiv","tags":"[]","date":"2018-10-17 11:33:11+00:00","url":"http://arxiv.org/abs/1810.07483v3"},{"x":"10.825744","y":"6.1916122","title":"Generalizing from a few environments in safety-critical reinforcement learning","cluster":"0","author":"['Zachary Kenton', 'Angelos Filos', 'Owain Evans', 'Yarin Gal']","source":"arxiv","tags":"[]","date":"2019-07-02 16:12:34+00:00","url":"http://arxiv.org/abs/1907.01475v1"},{"x":"9.542107","y":"9.8989525","title":"AI safety via debate","cluster":"4","author":"['Geoffrey Irving', 'Paul Christiano', 'Dario Amodei']","source":"arxiv","tags":"[]","date":"2018-05-02 16:27:32+00:00","url":"http://arxiv.org/abs/1805.00899v2"},{"x":"8.22171","y":"5.837186","title":"Representer Point Selection for Explaining Deep Neural Networks","cluster":"2","author":"['Chih-Kuan Yeh', 'Joon Sik Kim', 'Ian E. H. Yen', 'Pradeep Ravikumar']","source":"arxiv","tags":"[]","date":"2018-11-23 22:34:17+00:00","url":"http://arxiv.org/abs/1811.09720v1"},{"x":"11.123029","y":"8.061189","title":"Counterfactual Fairness","cluster":"1","author":"['Matt J. Kusner', 'Joshua R. Loftus', 'Chris Russell', 'Ricardo Silva']","source":"arxiv","tags":"[]","date":"2017-03-20 17:18:57+00:00","url":"http://arxiv.org/abs/1703.06856v3"},{"x":"8.810879","y":"6.8171854","title":"Unsolved Problems in ML Safety","cluster":"2","author":"['Dan Hendrycks', 'Nicholas Carlini', 'John Schulman', 'Jacob Steinhardt']","source":"alignment newsletter","tags":"[]","date":"2021-09-28 17:59:36+00:00","url":"http://arxiv.org/abs/2109.13916v3"},{"x":"10.873818","y":"6.2445216","title":"An Optimistic Perspective on Offline Reinforcement Learning","cluster":"0","author":"['Rishabh Agarwal', 'Dale Schuurmans', 'Mohammad Norouzi']","source":"arxiv","tags":"[]","date":"2019-07-10 07:23:27+00:00","url":"http://arxiv.org/abs/1907.04543v4"},{"x":"7.988341","y":"5.553413","title":"Revisiting the Calibration of Modern Neural Networks","cluster":"2","author":"['Matthias Minderer', 'Josip Djolonga', 'Rob Romijnders', 'Frances Hubis', 'Xiaohua Zhai', 'Neil Houlsby', 'Dustin Tran', 'Mario Lucic']","source":"arxiv","tags":"[]","date":"2021-06-15 09:24:43+00:00","url":"http://arxiv.org/abs/2106.07998v2"},{"x":"11.633465","y":"6.0400476","title":"Inverse Reward Design","cluster":"0","author":"['Dylan Hadfield-Menell', 'Smitha Milli', 'Pieter Abbeel', 'Stuart Russell', 'Anca Dragan']","source":"arxiv","tags":"[]","date":"2017-11-08 04:44:32+00:00","url":"http://arxiv.org/abs/1711.02827v2"},{"x":"11.649113","y":"6.120382","title":"Learning-based Model Predictive Control for Safe Exploration","cluster":"0","author":"['Torsten Koller', 'Felix Berkenkamp', 'Matteo Turchetta', 'Andreas Krause']","source":"arxiv","tags":"[]","date":"2018-03-22 09:41:45+00:00","url":"http://arxiv.org/abs/1803.08287v3"},{"x":"8.741729","y":"6.404054","title":"Explaining Explanations: An Overview of Interpretability of Machine Learning","cluster":"2","author":"['Leilani H. Gilpin', 'David Bau', 'Ben Z. Yuan', 'Ayesha Bajwa', 'Michael Specter', 'Lalana Kagal']","source":"arxiv","tags":"[]","date":"2018-05-31 19:48:00+00:00","url":"http://arxiv.org/abs/1806.00069v3"},{"x":"11.6317625","y":"6.41973","title":"Reward-rational (implicit) choice: A unifying formalism for reward learning","cluster":"0","author":"['Hong Jun Jeon', 'Smitha Milli', 'Anca D. Dragan']","source":"alignment newsletter","tags":"[]","date":"2020-02-12 08:07:49+00:00","url":"http://arxiv.org/abs/2002.04833v4"},{"x":"7.8521905","y":"5.9085894","title":"Distributed Representations of Words and Phrases and their Compositionality","cluster":"2","author":"['Tomas Mikolov', 'Ilya Sutskever', 'Kai Chen', 'Greg Corrado', 'Jeffrey Dean']","source":"arxiv","tags":"[]","date":"2013-10-16 23:28:53+00:00","url":"http://arxiv.org/abs/1310.4546v1"},{"x":"7.97251","y":"8.441664","title":"The AI Index 2021 Annual Report","cluster":"3","author":"['Daniel Zhang', 'Saurabh Mishra', 'Erik Brynjolfsson', 'John Etchemendy', 'Deep Ganguli', 'Barbara Grosz', 'Terah Lyons', 'James Manyika', 'Juan Carlos Niebles', 'Michael Sellitto', 'Yoav Shoham', 'Jack Clark', 'Raymond Perrault']","source":"arxiv","tags":"[]","date":"2021-03-09 02:29:44+00:00","url":"http://arxiv.org/abs/2103.06312v1"},{"x":"9.218737","y":"8.041884","title":"Friendly Artificial Intelligence: the Physics Challenge","cluster":"4","author":"['Max Tegmark']","source":"arxiv","tags":"[]","date":"2014-09-02 18:20:28+00:00","url":"http://arxiv.org/abs/1409.0813v2"},{"x":"10.4859705","y":"6.4964366","title":"Multi-Agent Generative Adversarial Imitation Learning","cluster":"0","author":"['Jiaming Song', 'Hongyu Ren', 'Dorsa Sadigh', 'Stefano Ermon']","source":"alignment newsletter","tags":"[]","date":"2018-07-26 03:21:49+00:00","url":"http://arxiv.org/abs/1807.09936v1"},{"x":"9.932607","y":"6.5770655","title":"The MineRL BASALT Competition on Learning from Human Feedback","cluster":"0","author":"['Rohin Shah', 'Cody Wild', 'Steven H. Wang', 'Neel Alex', 'Brandon Houghton', 'William Guss', 'Sharada Mohanty', 'Anssi Kanervisto', 'Stephanie Milani', 'Nicholay Topin', 'Pieter Abbeel', 'Stuart Russell', 'Anca Dragan']","source":"arxiv","tags":"[]","date":"2021-07-05 12:18:17+00:00","url":"http://arxiv.org/abs/2107.01969v1"},{"x":"8.509477","y":"4.906078","title":"Transfer of Adversarial Robustness Between Perturbation Types","cluster":"2","author":"['Daniel Kang', 'Yi Sun', 'Tom Brown', 'Dan Hendrycks', 'Jacob Steinhardt']","source":"arxiv","tags":"[]","date":"2019-05-03 04:51:07+00:00","url":"http://arxiv.org/abs/1905.01034v1"},{"x":"8.589996","y":"5.848916","title":"Underspecification Presents Challenges for Credibility in Modern Machine Learning","cluster":"2","author":"[\"Alexander D'Amour\", 'Katherine Heller', 'Dan Moldovan', 'Ben Adlam', 'Babak Alipanahi', 'Alex Beutel', 'Christina Chen', 'Jonathan Deaton', 'Jacob Eisenstein', 'Matthew D. Hoffman', 'Farhad Hormozdiari', 'Neil Houlsby', 'Shaobo Hou', 'Ghassen Jerfel', 'Alan Karthikesalingam', 'Mario Lucic', 'Yian Ma', 'Cory McLean', 'Diana Mincu', 'Akinori Mitani', 'Andrea Montanari', 'Zachary Nado', 'Vivek Natarajan', 'Christopher Nielson', 'Thomas F. Osborne', 'Rajiv Raman', 'Kim Ramasamy', 'Rory Sayres', 'Jessica Schrouff', 'Martin Seneviratne', 'Shannon Sequeira', 'Harini Suresh', 'Victor Veitch', 'Max Vladymyrov', 'Xuezhi Wang', 'Kellie Webster', 'Steve Yadlowsky', 'Taedong Yun', 'Xiaohua Zhai', 'D. Sculley']","source":"alignment newsletter","tags":"[]","date":"2020-11-06 14:53:13+00:00","url":"http://arxiv.org/abs/2011.03395v2"},{"x":"8.3377695","y":"5.6928697","title":"Leveraging Sparse Linear Layers for Debuggable Deep Networks","cluster":"2","author":"['Eric Wong', 'Shibani Santurkar', 'Aleksander Mądry']","source":"alignment newsletter","tags":"[]","date":"2021-05-11 08:15:25+00:00","url":"http://arxiv.org/abs/2105.04857v1"},{"x":"7.817912","y":"7.8012204","title":"A Roadmap for Robust End-to-End Alignment","cluster":"2","author":"['Lê Nguyên Hoang']","source":"arxiv","tags":"[]","date":"2018-09-04 15:19:44+00:00","url":"http://arxiv.org/abs/1809.01036v4"},{"x":"7.449144","y":"6.9894567","title":"Challenges in Detoxifying Language Models","cluster":"2","author":"['Johannes Welbl', 'Amelia Glaese', 'Jonathan Uesato', 'Sumanth Dathathri', 'John Mellor', 'Lisa Anne Hendricks', 'Kirsty Anderson', 'Pushmeet Kohli', 'Ben Coppin', 'Po-Sen Huang']","source":"arxiv","tags":"[]","date":"2021-09-15 17:27:06+00:00","url":"http://arxiv.org/abs/2109.07445v1"},{"x":"7.6724706","y":"6.325536","title":"Pretrained Transformers as Universal Computation Engines","cluster":"2","author":"['Kevin Lu', 'Aditya Grover', 'Pieter Abbeel', 'Igor Mordatch']","source":"arxiv","tags":"[]","date":"2021-03-09 06:39:56+00:00","url":"http://arxiv.org/abs/2103.05247v2"},{"x":"10.949334","y":"6.64005","title":"Avoiding Side Effects in Complex Environments","cluster":"0","author":"['Alexander Matt Turner', 'Neale Ratzlaff', 'Prasad Tadepalli']","source":"alignment newsletter","tags":"[]","date":"2020-06-11 16:02:30+00:00","url":"http://arxiv.org/abs/2006.06547v2"},{"x":"8.0499935","y":"6.9981637","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning","cluster":"2","author":"['Jian Liu', 'Leyang Cui', 'Hanmeng Liu', 'Dandan Huang', 'Yile Wang', 'Yue Zhang']","source":"arxiv","tags":"[]","date":"2020-07-16 05:52:16+00:00","url":"http://arxiv.org/abs/2007.08124v1"},{"x":"11.2498045","y":"6.449192","title":"DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction","cluster":"0","author":"['Aviral Kumar', 'Abhishek Gupta', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2020-03-16 16:18:52+00:00","url":"http://arxiv.org/abs/2003.07305v1"},{"x":"8.443676","y":"9.380659","title":"Towards a Theory of Justice for Artificial Intelligence","cluster":"3","author":"['Iason Gabriel']","source":"arxiv","tags":"[]","date":"2021-10-27 13:23:38+00:00","url":"http://arxiv.org/abs/2110.14419v1"},{"x":"9.452164","y":"5.8769283","title":"Task-Agnostic Meta-Learning for Few-shot Learning","cluster":"0","author":"['Muhammad Abdullah Jamal', 'Guo-Jun Qi', 'Mubarak Shah']","source":"arxiv","tags":"[]","date":"2018-05-20 07:50:42+00:00","url":"http://arxiv.org/abs/1805.07722v1"},{"x":"7.975577","y":"5.0464807","title":"Pyramid Adversarial Training Improves ViT Performance","cluster":"2","author":"['Charles Herrmann', 'Kyle Sargent', 'Lu Jiang', 'Ramin Zabih', 'Huiwen Chang', 'Ce Liu', 'Dilip Krishnan', 'Deqing Sun']","source":"arxiv","tags":"[]","date":"2021-11-30 04:38:14+00:00","url":"http://arxiv.org/abs/2111.15121v1"},{"x":"10.949797","y":"6.9411035","title":"Avoiding Tampering Incentives in Deep RL via Decoupled Approval","cluster":"0","author":"['Jonathan Uesato', 'Ramana Kumar', 'Victoria Krakovna', 'Tom Everitt', 'Richard Ngo', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2020-11-17 18:48:59+00:00","url":"http://arxiv.org/abs/2011.08827v1"},{"x":"8.187038","y":"5.826555","title":"Generalization Error in Deep Learning","cluster":"2","author":"['Daniel Jakubovitz', 'Raja Giryes', 'Miguel R. D. Rodrigues']","source":"arxiv","tags":"[]","date":"2018-08-03 12:57:12+00:00","url":"http://arxiv.org/abs/1808.01174v3"},{"x":"8.24833","y":"8.580507","title":"Accounting for the Neglected Dimensions of AI Progress","cluster":"3","author":"['Fernando Martínez-Plumed', 'Shahar Avin', 'Miles Brundage', 'Allan Dafoe', 'Sean Ó hÉigeartaigh', 'José Hernández-Orallo']","source":"arxiv","tags":"[]","date":"2018-06-02 09:21:12+00:00","url":"http://arxiv.org/abs/1806.00610v1"},{"x":"11.798711","y":"8.973045","title":"I Don't Want to Think About it Now:Decision Theory With Costly Computation","cluster":"1","author":"['Joseph Y. Halpern', 'Rafael Pass']","source":"arxiv","tags":"[]","date":"2011-06-14 09:52:38+00:00","url":"http://arxiv.org/abs/1106.2657v1"},{"x":"11.059356","y":"8.791896","title":"Designing Recommender Systems to Depolarize","cluster":"3","author":"['Jonathan Stray']","source":"alignment newsletter","tags":"[]","date":"2021-07-11 03:23:42+00:00","url":"http://arxiv.org/abs/2107.04953v1"},{"x":"8.428804","y":"5.45","title":"Distributional Generalization: A New Kind of Generalization","cluster":"2","author":"['Preetum Nakkiran', 'Yamini Bansal']","source":"alignment newsletter","tags":"[]","date":"2020-09-17 06:26:17+00:00","url":"http://arxiv.org/abs/2009.08092v2"},{"x":"8.672903","y":"5.0209117","title":"Evaluating and Understanding the Robustness of Adversarial Logit Pairing","cluster":"2","author":"['Logan Engstrom', 'Andrew Ilyas', 'Anish Athalye']","source":"arxiv","tags":"[]","date":"2018-07-26 17:58:26+00:00","url":"http://arxiv.org/abs/1807.10272v2"},{"x":"8.3423815","y":"6.1754036","title":"Quantifying Local Specialization in Deep Neural Networks","cluster":"2","author":"['Shlomi Hod', 'Daniel Filan', 'Stephen Casper', 'Andrew Critch', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2021-10-13 20:33:30+00:00","url":"http://arxiv.org/abs/2110.08058v2"},{"x":"10.384734","y":"6.2242327","title":"Trial without Error: Towards Safe Reinforcement Learning via Human Intervention","cluster":"0","author":"['William Saunders', 'Girish Sastry', 'Andreas Stuhlmueller', 'Owain Evans']","source":"arxiv","tags":"[]","date":"2017-07-17 14:13:40+00:00","url":"http://arxiv.org/abs/1707.05173v1"},{"x":"8.438256","y":"5.003017","title":"CEB Improves Model Robustness","cluster":"2","author":"['Ian Fischer', 'Alexander A. Alemi']","source":"alignment newsletter","tags":"[]","date":"2020-02-13 07:49:22+00:00","url":"http://arxiv.org/abs/2002.05380v1"},{"x":"11.872352","y":"6.5936923","title":"Bayesian Policy Optimization for Model Uncertainty","cluster":"0","author":"['Gilwoo Lee', 'Brian Hou', 'Aditya Mandalika', 'Jeongseok Lee', 'Sanjiban Choudhury', 'Siddhartha S. Srinivasa']","source":"arxiv","tags":"[]","date":"2018-10-01 23:39:25+00:00","url":"http://arxiv.org/abs/1810.01014v2"},{"x":"7.8724666","y":"5.3257027","title":"Momentum Contrast for Unsupervised Visual Representation Learning","cluster":"2","author":"['Kaiming He', 'Haoqi Fan', 'Yuxin Wu', 'Saining Xie', 'Ross Girshick']","source":"alignment newsletter","tags":"[]","date":"2019-11-13 18:53:26+00:00","url":"http://arxiv.org/abs/1911.05722v3"},{"x":"7.614212","y":"6.2964187","title":"Universal Transformers","cluster":"2","author":"['Mostafa Dehghani', 'Stephan Gouws', 'Oriol Vinyals', 'Jakob Uszkoreit', 'Łukasz Kaiser']","source":"arxiv","tags":"[]","date":"2018-07-10 18:39:15+00:00","url":"http://arxiv.org/abs/1807.03819v3"},{"x":"11.491748","y":"5.654867","title":"Feedback in Imitation Learning: The Three Regimes of Covariate Shift","cluster":"0","author":"['Jonathan Spencer', 'Sanjiban Choudhury', 'Arun Venkatraman', 'Brian Ziebart', 'J. Andrew Bagnell']","source":"arxiv","tags":"[]","date":"2021-02-04 20:18:56+00:00","url":"http://arxiv.org/abs/2102.02872v2"},{"x":"11.408644","y":"6.492536","title":"There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning","cluster":"0","author":"['Nathan Grinsztajn', 'Johan Ferret', 'Olivier Pietquin', 'Philippe Preux', 'Matthieu Geist']","source":"arxiv","tags":"[]","date":"2021-06-08 16:07:10+00:00","url":"http://arxiv.org/abs/2106.04480v3"},{"x":"11.416178","y":"6.8937993","title":"Avoiding Side Effects By Considering Future Tasks","cluster":"0","author":"['Victoria Krakovna', 'Laurent Orseau', 'Richard Ngo', 'Miljan Martic', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2020-10-15 16:55:26+00:00","url":"http://arxiv.org/abs/2010.07877v1"},{"x":"10.903298","y":"5.507844","title":"Imitating Interactive Intelligence","cluster":"0","author":"['Josh Abramson', 'Arun Ahuja', 'Iain Barr', 'Arthur Brussee', 'Federico Carnevale', 'Mary Cassin', 'Rachita Chhaparia', 'Stephen Clark', 'Bogdan Damoc', 'Andrew Dudzik', 'Petko Georgiev', 'Aurelia Guy', 'Tim Harley', 'Felix Hill', 'Alden Hung', 'Zachary Kenton', 'Jessica Landon', 'Timothy Lillicrap', 'Kory Mathewson', 'Soňa Mokrá', 'Alistair Muldal', 'Adam Santoro', 'Nikolay Savinov', 'Vikrant Varma', 'Greg Wayne', 'Duncan Williams', 'Nathaniel Wong', 'Chen Yan', 'Rui Zhu']","source":"alignment newsletter","tags":"[]","date":"2020-12-10 13:55:47+00:00","url":"http://arxiv.org/abs/2012.05672v2"},{"x":"7.838875","y":"5.972652","title":"Scaling Laws for Neural Language Models","cluster":"2","author":"['Jared Kaplan', 'Sam McCandlish', 'Tom Henighan', 'Tom B. Brown', 'Benjamin Chess', 'Rewon Child', 'Scott Gray', 'Alec Radford', 'Jeffrey Wu', 'Dario Amodei']","source":"alignment newsletter","tags":"[]","date":"2020-01-23 03:59:20+00:00","url":"http://arxiv.org/abs/2001.08361v1"},{"x":"11.606336","y":"6.6275544","title":"Avoiding Wireheading with Value Reinforcement Learning","cluster":"0","author":"['Tom Everitt', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2016-05-10 18:28:57+00:00","url":"http://arxiv.org/abs/1605.03143v1"},{"x":"10.691691","y":"6.765716","title":"Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design","cluster":"0","author":"['Michael Dennis', 'Natasha Jaques', 'Eugene Vinitsky', 'Alexandre Bayen', 'Stuart Russell', 'Andrew Critch', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2020-12-03 17:37:01+00:00","url":"http://arxiv.org/abs/2012.02096v2"},{"x":"8.150756","y":"5.7796297","title":"Rethinking Bias-Variance Trade-off for Generalization of Neural Networks","cluster":"2","author":"['Zitong Yang', 'Yaodong Yu', 'Chong You', 'Jacob Steinhardt', 'Yi Ma']","source":"alignment newsletter","tags":"[]","date":"2020-02-26 07:21:54+00:00","url":"http://arxiv.org/abs/2002.11328v3"},{"x":"8.691954","y":"5.452351","title":"Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes","cluster":"2","author":"['Fabian B. Fuchs', 'Oliver Groth', 'Adam R. Kosiorek', 'Alex Bewley', 'Markus Wulfmeier', 'Andrea Vedaldi', 'Ingmar Posner']","source":"arxiv","tags":"[]","date":"2018-06-14 12:35:50+00:00","url":"http://arxiv.org/abs/1806.05502v5"},{"x":"7.9448986","y":"6.209334","title":"Representation Learning with Contrastive Predictive Coding","cluster":"2","author":"['Aaron van den Oord', 'Yazhe Li', 'Oriol Vinyals']","source":"alignment newsletter","tags":"[]","date":"2018-07-10 16:52:11+00:00","url":"http://arxiv.org/abs/1807.03748v2"},{"x":"7.829373","y":"5.497991","title":"Capsules for Object Segmentation","cluster":"2","author":"['Rodney LaLonde', 'Ulas Bagci']","source":"arxiv","tags":"[]","date":"2018-04-11 21:57:57+00:00","url":"http://arxiv.org/abs/1804.04241v1"},{"x":"8.555832","y":"5.810801","title":"Meta-Learning with Hessian-Free Approach in Deep Neural Nets Training","cluster":"2","author":"['Boyu Chen', 'Wenlian Lu', 'Ernest Fokoue']","source":"arxiv","tags":"[]","date":"2018-05-22 09:04:52+00:00","url":"http://arxiv.org/abs/1805.08462v2"},{"x":"8.85599","y":"8.469176","title":"Alignment of Language Agents","cluster":"4","author":"['Zachary Kenton', 'Tom Everitt', 'Laura Weidinger', 'Iason Gabriel', 'Vladimir Mikulik', 'Geoffrey Irving']","source":"arxiv","tags":"[]","date":"2021-03-26 18:01:48+00:00","url":"http://arxiv.org/abs/2103.14659v1"},{"x":"10.956526","y":"6.2096176","title":"Directed Policy Gradient for Safe Reinforcement Learning with Human Advice","cluster":"0","author":"['Hélène Plisnier', 'Denis Steckelmacher', 'Tim Brys', 'Diederik M. Roijers', 'Ann Nowé']","source":"arxiv","tags":"[]","date":"2018-08-13 08:12:22+00:00","url":"http://arxiv.org/abs/1808.04096v1"},{"x":"8.59676","y":"5.7340345","title":"Unsupervised Learning via Meta-Learning","cluster":"2","author":"['Kyle Hsu', 'Sergey Levine', 'Chelsea Finn']","source":"alignment newsletter","tags":"[]","date":"2018-10-04 17:29:17+00:00","url":"http://arxiv.org/abs/1810.02334v6"},{"x":"8.180864","y":"9.300167","title":"Forecasting Transformative AI: An Expert Survey","cluster":"3","author":"['Ross Gruetzemacher', 'David Paradice', 'Kang Bok Lee']","source":"arxiv","tags":"[]","date":"2019-01-24 18:53:07+00:00","url":"http://arxiv.org/abs/1901.08579v2"},{"x":"8.955836","y":"7.6474514","title":"Learning to Complement Humans","cluster":"2","author":"['Bryan Wilder', 'Eric Horvitz', 'Ece Kamar']","source":"alignment newsletter","tags":"[]","date":"2020-05-01 20:00:23+00:00","url":"http://arxiv.org/abs/2005.00582v1"},{"x":"8.069637","y":"6.0170827","title":"Neural Arithmetic Logic Units","cluster":"2","author":"['Andrew Trask', 'Felix Hill', 'Scott Reed', 'Jack Rae', 'Chris Dyer', 'Phil Blunsom']","source":"arxiv","tags":"[]","date":"2018-08-01 18:58:53+00:00","url":"http://arxiv.org/abs/1808.00508v1"},{"x":"11.216445","y":"5.7494407","title":"Deep Imitative Models for Flexible Inference, Planning, and Control","cluster":"0","author":"['Nicholas Rhinehart', 'Rowan McAllister', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2018-10-15 17:51:03+00:00","url":"http://arxiv.org/abs/1810.06544v4"},{"x":"9.172795","y":"7.5760517","title":"AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence","cluster":"4","author":"['Jeff Clune']","source":"alignment newsletter","tags":"[]","date":"2019-05-27 06:05:16+00:00","url":"http://arxiv.org/abs/1905.10985v2"},{"x":"12.173949","y":"7.858443","title":"Learning in two-player games between transparent opponents","cluster":"0","author":"['Adrian Hutter']","source":"arxiv","tags":"[]","date":"2020-12-04 15:41:07+00:00","url":"http://arxiv.org/abs/2012.02671v2"},{"x":"8.051971","y":"5.6918354","title":"Clusterability in Neural Networks","cluster":"2","author":"['Daniel Filan', 'Stephen Casper', 'Shlomi Hod', 'Cody Wild', 'Andrew Critch', 'Stuart Russell']","source":"alignment newsletter","tags":"[]","date":"2021-03-04 23:53:53+00:00","url":"http://arxiv.org/abs/2103.03386v1"},{"x":"9.566412","y":"6.2664638","title":"Towards Mixed Optimization for Reinforcement Learning with Program Synthesis","cluster":"0","author":"['Surya Bhupatiraju', 'Kumar Krishna Agrawal', 'Rishabh Singh']","source":"alignment newsletter","tags":"[]","date":"2018-07-01 21:52:07+00:00","url":"http://arxiv.org/abs/1807.00403v2"},{"x":"9.214346","y":"5.776102","title":"Scaling shared model governance via model splitting","cluster":"0","author":"['Miljan Martic', 'Jan Leike', 'Andrew Trask', 'Matteo Hessel', 'Shane Legg', 'Pushmeet Kohli']","source":"arxiv","tags":"[]","date":"2018-12-14 15:29:21+00:00","url":"http://arxiv.org/abs/1812.05979v1"},{"x":"8.685926","y":"5.8311944","title":"\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier","cluster":"2","author":"['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']","source":"arxiv","tags":"[]","date":"2016-02-16 08:20:14+00:00","url":"http://arxiv.org/abs/1602.04938v3"},{"x":"9.327153","y":"6.7199454","title":"Building Machines That Learn and Think Like People","cluster":"4","author":"['Brenden M. Lake', 'Tomer D. Ullman', 'Joshua B. Tenenbaum', 'Samuel J. Gershman']","source":"alignment newsletter","tags":"[]","date":"2016-04-01 15:37:57+00:00","url":"http://arxiv.org/abs/1604.00289v3"},{"x":"12.350932","y":"8.636831","title":"Game Theory with Translucent Players","cluster":"1","author":"['Joseph Y. Halpern', 'Rafael Pass']","source":"arxiv","tags":"[]","date":"2013-08-17 12:29:53+00:00","url":"http://arxiv.org/abs/1308.3778v1"},{"x":"10.53873","y":"5.895114","title":"Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors","cluster":"0","author":"['Christian Arzate Cruz', 'Takeo Igarashi']","source":"alignment newsletter","tags":"[]","date":"2021-05-27 04:17:48+00:00","url":"http://arxiv.org/abs/2105.12938v1"},{"x":"11.938899","y":"9.281488","title":"The Blessings of Multiple Causes","cluster":"1","author":"['Yixin Wang', 'David M. Blei']","source":"arxiv","tags":"[]","date":"2018-05-17 15:39:17+00:00","url":"http://arxiv.org/abs/1805.06826v3"},{"x":"11.382959","y":"7.2913938","title":"Emergence of Addictive Behaviors in Reinforcement Learning Agents","cluster":"0","author":"['Vahid Behzadan', 'Roman V. Yampolskiy', 'Arslan Munir']","source":"arxiv","tags":"[]","date":"2018-11-14 01:30:00+00:00","url":"http://arxiv.org/abs/1811.05590v1"},{"x":"8.627482","y":"6.1383247","title":"Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations","cluster":"2","author":"['Andrew Slavin Ross', 'Michael C. Hughes', 'Finale Doshi-Velez']","source":"arxiv","tags":"[]","date":"2017-03-10 15:35:32+00:00","url":"http://arxiv.org/abs/1703.03717v2"},{"x":"8.46259","y":"9.18199","title":"Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","cluster":"3","author":"['Miles Brundage', 'Shahar Avin', 'Jasmine Wang', 'Haydn Belfield', 'Gretchen Krueger', 'Gillian Hadfield', 'Heidy Khlaaf', 'Jingying Yang', 'Helen Toner', 'Ruth Fong', 'Tegan Maharaj', 'Pang Wei Koh', 'Sara Hooker', 'Jade Leung', 'Andrew Trask', 'Emma Bluemke', 'Jonathan Lebensold', \"Cullen O'Keefe\", 'Mark Koren', 'Théo Ryffel', 'JB Rubinovitz', 'Tamay Besiroglu', 'Federica Carugati', 'Jack Clark', 'Peter Eckersley', 'Sarah de Haas', 'Maritza Johnson', 'Ben Laurie', 'Alex Ingerman', 'Igor Krawczuk', 'Amanda Askell', 'Rosario Cammarota', 'Andrew Lohn', 'David Krueger', 'Charlotte Stix', 'Peter Henderson', 'Logan Graham', 'Carina Prunkl', 'Bianca Martin', 'Elizabeth Seger', 'Noa Zilberman', 'Seán Ó hÉigeartaigh', 'Frens Kroeger', 'Girish Sastry', 'Rebecca Kagan', 'Adrian Weller', 'Brian Tse', 'Elizabeth Barnes', 'Allan Dafoe', 'Paul Scharre', 'Ariel Herbert-Voss', 'Martijn Rasser', 'Shagun Sodhani', 'Carrick Flynn', 'Thomas Krendl Gilbert', 'Lisa Dyer', 'Saif Khan', 'Yoshua Bengio', 'Markus Anderljung']","source":"arxiv","tags":"[]","date":"2020-04-15 17:15:35+00:00","url":"http://arxiv.org/abs/2004.07213v2"},{"x":"12.341155","y":"8.5948715","title":"Rational Consensus","cluster":"1","author":"['Joseph Y. Halpern', 'Xavier Vilaca']","source":"arxiv","tags":"[]","date":"2020-05-20 15:39:55+00:00","url":"http://arxiv.org/abs/2005.10141v1"},{"x":"11.616311","y":"6.291104","title":"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","cluster":"0","author":"['Sergey Levine', 'Aviral Kumar', 'George Tucker', 'Justin Fu']","source":"alignment newsletter","tags":"[]","date":"2020-05-04 17:00:15+00:00","url":"http://arxiv.org/abs/2005.01643v3"},{"x":"12.64091","y":"9.905978","title":"A Formal Approach to the Problem of Logical Non-Omniscience","cluster":"1","author":"['Scott Garrabrant', 'Tsvi Benson-Tilsen', 'Andrew Critch', 'Nate Soares', 'Jessica Taylor']","source":"arxiv","tags":"[]","date":"2017-07-27 07:49:01+00:00","url":"http://arxiv.org/abs/1707.08747v1"},{"x":"9.148359","y":"5.5000014","title":"Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow","cluster":"2","author":"['Xue Bin Peng', 'Angjoo Kanazawa', 'Sam Toyer', 'Pieter Abbeel', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2018-10-01 17:02:24+00:00","url":"http://arxiv.org/abs/1810.00821v4"},{"x":"10.844188","y":"6.790253","title":"The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models","cluster":"0","author":"['Alexander Pan', 'Kush Bhatia', 'Jacob Steinhardt']","source":"arxiv","tags":"[]","date":"2022-01-10 18:58:52+00:00","url":"http://arxiv.org/abs/2201.03544v2"},{"x":"11.272061","y":"6.787245","title":"Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration","cluster":"0","author":"['Ritesh Noothigattu', 'Djallel Bouneffouf', 'Nicholas Mattei', 'Rachita Chandra', 'Piyush Madan', 'Kush Varshney', 'Murray Campbell', 'Moninder Singh', 'Francesca Rossi']","source":"arxiv","tags":"[]","date":"2018-09-21 23:38:17+00:00","url":"http://arxiv.org/abs/1809.08343v1"},{"x":"8.604431","y":"5.0452003","title":"Certifying Out-of-Domain Generalization for Blackbox Functions","cluster":"2","author":"['Maurice Weber', 'Linyi Li', 'Boxin Wang', 'Zhikuan Zhao', 'Bo Li', 'Ce Zhang']","source":"arxiv","tags":"[]","date":"2022-02-03 16:47:50+00:00","url":"http://arxiv.org/abs/2202.01679v1"},{"x":"12.863036","y":"9.004084","title":"Good and safe uses of AI Oracles","cluster":"4","author":"['Stuart Armstrong', \"Xavier O'Rorke\"]","source":"arxiv","tags":"[]","date":"2017-11-15 12:47:17+00:00","url":"http://arxiv.org/abs/1711.05541v5"},{"x":"7.8351984","y":"5.268617","title":"The Met Dataset: Instance-level Recognition for Artworks","cluster":"2","author":"['Nikolaos-Antonios Ypsilantis', 'Noa Garcia', 'Guangxing Han', 'Sarah Ibrahimi', 'Nanne Van Noord', 'Giorgos Tolias']","source":"arxiv","tags":"[]","date":"2022-02-03 18:13:30+00:00","url":"http://arxiv.org/abs/2202.01747v1"},{"x":"7.4263577","y":"6.596914","title":"Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation","cluster":"2","author":"['Po-Sen Huang', 'Robert Stanforth', 'Johannes Welbl', 'Chris Dyer', 'Dani Yogatama', 'Sven Gowal', 'Krishnamurthy Dvijotham', 'Pushmeet Kohli']","source":"arxiv","tags":"[]","date":"2019-09-03 23:03:10+00:00","url":"http://arxiv.org/abs/1909.01492v2"},{"x":"8.657569","y":"5.4249334","title":"A generic framework for privacy preserving deep learning","cluster":"2","author":"['Theo Ryffel', 'Andrew Trask', 'Morten Dahl', 'Bobby Wagner', 'Jason Mancuso', 'Daniel Rueckert', 'Jonathan Passerat-Palmbach']","source":"arxiv","tags":"[]","date":"2018-11-09 17:10:47+00:00","url":"http://arxiv.org/abs/1811.04017v2"},{"x":"7.6673026","y":"6.7047586","title":"Extracting Training Data from Large Language Models","cluster":"2","author":"['Nicholas Carlini', 'Florian Tramer', 'Eric Wallace', 'Matthew Jagielski', 'Ariel Herbert-Voss', 'Katherine Lee', 'Adam Roberts', 'Tom Brown', 'Dawn Song', 'Ulfar Erlingsson', 'Alina Oprea', 'Colin Raffel']","source":"arxiv","tags":"[]","date":"2020-12-14 18:39:09+00:00","url":"http://arxiv.org/abs/2012.07805v2"},{"x":"11.122395","y":"6.7072916","title":"Safe Option-Critic: Learning Safety in the Option-Critic Architecture","cluster":"0","author":"['Arushi Jain', 'Khimya Khetarpal', 'Doina Precup']","source":"alignment newsletter","tags":"[]","date":"2018-07-21 00:39:23+00:00","url":"http://arxiv.org/abs/1807.08060v2"},{"x":"8.541263","y":"5.776733","title":"Learning Representations by Humans, for Humans","cluster":"2","author":"['Sophie Hilgard', 'Nir Rosenfeld', 'Mahzarin R. Banaji', 'Jack Cao', 'David C. Parkes']","source":"alignment newsletter","tags":"[]","date":"2019-05-29 19:19:09+00:00","url":"http://arxiv.org/abs/1905.12686v4"},{"x":"7.678736","y":"6.8075447","title":"Evaluating Large Language Models Trained on Code","cluster":"2","author":"['Mark Chen', 'Jerry Tworek', 'Heewoo Jun', 'Qiming Yuan', 'Henrique Ponde de Oliveira Pinto', 'Jared Kaplan', 'Harri Edwards', 'Yuri Burda', 'Nicholas Joseph', 'Greg Brockman', 'Alex Ray', 'Raul Puri', 'Gretchen Krueger', 'Michael Petrov', 'Heidy Khlaaf', 'Girish Sastry', 'Pamela Mishkin', 'Brooke Chan', 'Scott Gray', 'Nick Ryder', 'Mikhail Pavlov', 'Alethea Power', 'Lukasz Kaiser', 'Mohammad Bavarian', 'Clemens Winter', 'Philippe Tillet', 'Felipe Petroski Such', 'Dave Cummings', 'Matthias Plappert', 'Fotios Chantzis', 'Elizabeth Barnes', 'Ariel Herbert-Voss', 'William Hebgen Guss', 'Alex Nichol', 'Alex Paino', 'Nikolas Tezak', 'Jie Tang', 'Igor Babuschkin', 'Suchir Balaji', 'Shantanu Jain', 'William Saunders', 'Christopher Hesse', 'Andrew N. Carr', 'Jan Leike', 'Josh Achiam', 'Vedant Misra', 'Evan Morikawa', 'Alec Radford', 'Matthew Knight', 'Miles Brundage', 'Mira Murati', 'Katie Mayer', 'Peter Welinder', 'Bob McGrew', 'Dario Amodei', 'Sam McCandlish', 'Ilya Sutskever', 'Wojciech Zaremba']","source":"alignment newsletter","tags":"[]","date":"2021-07-07 17:41:24+00:00","url":"http://arxiv.org/abs/2107.03374v2"},{"x":"9.393114","y":"7.9634986","title":"On the Measure of Intelligence","cluster":"4","author":"['François Chollet']","source":"arxiv","tags":"[]","date":"2019-11-05 00:31:38+00:00","url":"http://arxiv.org/abs/1911.01547v2"},{"x":"8.457585","y":"7.6713896","title":"Predictability and Surprise in Large Generative Models","cluster":"3","author":"['Deep Ganguli', 'Danny Hernandez', 'Liane Lovitt', 'Nova DasSarma', 'Tom Henighan', 'Andy Jones', 'Nicholas Joseph', 'Jackson Kernion', 'Ben Mann', 'Amanda Askell', 'Yuntao Bai', 'Anna Chen', 'Tom Conerly', 'Dawn Drain', 'Nelson Elhage', 'Sheer El Showk', 'Stanislav Fort', 'Zac Hatfield-Dodds', 'Scott Johnston', 'Shauna Kravec', 'Neel Nanda', 'Kamal Ndousse', 'Catherine Olsson', 'Daniela Amodei', 'Dario Amodei', 'Tom Brown', 'Jared Kaplan', 'Sam McCandlish', 'Chris Olah', 'Jack Clark']","source":"arxiv","tags":"[]","date":"2022-02-15 23:21:23+00:00","url":"http://arxiv.org/abs/2202.07785v1"},{"x":"8.339796","y":"5.0711665","title":"An Alternative Surrogate Loss for PGD-based Adversarial Testing","cluster":"2","author":"['Sven Gowal', 'Jonathan Uesato', 'Chongli Qin', 'Po-Sen Huang', 'Timothy Mann', 'Pushmeet Kohli']","source":"arxiv","tags":"[]","date":"2019-10-21 13:08:54+00:00","url":"http://arxiv.org/abs/1910.09338v1"},{"x":"8.037628","y":"6.171889","title":"Scaling Scaling Laws with Board Games","cluster":"2","author":"['Andy L. Jones']","source":"alignment newsletter","tags":"[]","date":"2021-04-07 13:34:25+00:00","url":"http://arxiv.org/abs/2104.03113v2"},{"x":"10.836837","y":"5.52985","title":"Universal Planning Networks","cluster":"0","author":"['Aravind Srinivas', 'Allan Jabri', 'Pieter Abbeel', 'Sergey Levine', 'Chelsea Finn']","source":"alignment newsletter","tags":"[]","date":"2018-04-02 17:51:53+00:00","url":"http://arxiv.org/abs/1804.00645v2"},{"x":"11.620974","y":"5.6392665","title":"Nonverbal Robot Feedback for Human Teachers","cluster":"0","author":"['Sandy H. Huang', 'Isabella Huang', 'Ravi Pandya', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2019-11-06 11:26:31+00:00","url":"http://arxiv.org/abs/1911.02320v1"},{"x":"11.36298","y":"6.1394563","title":"Feature Expansive Reward Learning: Rethinking Human Input","cluster":"0","author":"['Andreea Bobu', 'Marius Wiggert', 'Claire Tomlin', 'Anca D. Dragan']","source":"alignment newsletter","tags":"[]","date":"2020-06-23 17:59:34+00:00","url":"http://arxiv.org/abs/2006.13208v2"},{"x":"10.927584","y":"6.128636","title":"Safe Reinforcement Learning with Natural Language Constraints","cluster":"0","author":"['Tsung-Yen Yang', 'Michael Hu', 'Yinlam Chow', 'Peter J. Ramadge', 'Karthik Narasimhan']","source":"arxiv","tags":"[]","date":"2020-10-11 03:41:56+00:00","url":"http://arxiv.org/abs/2010.05150v2"},{"x":"12.947092","y":"10.157824","title":"Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm","cluster":"1","author":"['Vanessa Kosoy', 'Alexander Appel']","source":"arxiv","tags":"[]","date":"2016-08-14 15:34:24+00:00","url":"http://arxiv.org/abs/1608.04112v6"},{"x":"8.478121","y":"5.669626","title":"A Theory of Universal Learning","cluster":"2","author":"['Olivier Bousquet', 'Steve Hanneke', 'Shay Moran', 'Ramon van Handel', 'Amir Yehudayoff']","source":"arxiv","tags":"[]","date":"2020-11-09 15:10:32+00:00","url":"http://arxiv.org/abs/2011.04483v1"},{"x":"8.152972","y":"5.702667","title":"Neuron Shapley: Discovering the Responsible Neurons","cluster":"2","author":"['Amirata Ghorbani', 'James Zou']","source":"alignment newsletter","tags":"[]","date":"2020-02-23 03:29:58+00:00","url":"http://arxiv.org/abs/2002.09815v3"},{"x":"11.29413","y":"5.319277","title":"One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks","cluster":"0","author":"['Tianhe Yu', 'Pieter Abbeel', 'Sergey Levine', 'Chelsea Finn']","source":"arxiv","tags":"[]","date":"2018-10-25 18:05:08+00:00","url":"http://arxiv.org/abs/1810.11043v1"},{"x":"10.127278","y":"5.9433594","title":"Learning to Play No-Press Diplomacy with Best Response Policy Iteration","cluster":"0","author":"['Thomas Anthony', 'Tom Eccles', 'Andrea Tacchetti', 'János Kramár', 'Ian Gemp', 'Thomas C. Hudson', 'Nicolas Porcel', 'Marc Lanctot', 'Julien Pérolat', 'Richard Everett', 'Roman Werpachowski', 'Satinder Singh', 'Thore Graepel', 'Yoram Bachrach']","source":"alignment newsletter","tags":"[]","date":"2020-06-08 14:33:31+00:00","url":"http://arxiv.org/abs/2006.04635v4"},{"x":"11.512821","y":"5.530696","title":"Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning","cluster":"0","author":"['Xinlei Pan', 'Eshed Ohn-Bar', 'Nicholas Rhinehart', 'Yan Xu', 'Yilin Shen', 'Kris M. Kitani']","source":"arxiv","tags":"[]","date":"2018-06-22 03:24:00+00:00","url":"http://arxiv.org/abs/1806.08479v1"},{"x":"11.707468","y":"5.619978","title":"Scaled Autonomy: Enabling Human Operators to Control Robot Fleets","cluster":"0","author":"['Gokul Swamy', 'Siddharth Reddy', 'Sergey Levine', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2019-09-22 01:00:49+00:00","url":"http://arxiv.org/abs/1910.02910v2"},{"x":"8.618541","y":"8.617145","title":"AGI Safety Literature Review","cluster":"3","author":"['Tom Everitt', 'Gary Lea', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2018-05-03 04:26:48+00:00","url":"http://arxiv.org/abs/1805.01109v2"},{"x":"10.416721","y":"7.8613896","title":"Categorizing Variants of Goodhart's Law","cluster":"1","author":"['David Manheim', 'Scott Garrabrant']","source":"arxiv","tags":"[]","date":"2018-03-13 01:15:39+00:00","url":"http://arxiv.org/abs/1803.04585v4"},{"x":"8.037794","y":"5.8806214","title":"Pruned Neural Networks are Surprisingly Modular","cluster":"2","author":"['Daniel Filan', 'Shlomi Hod', 'Cody Wild', 'Andrew Critch', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2020-03-10 17:51:33+00:00","url":"http://arxiv.org/abs/2003.04881v4"},{"x":"8.101419","y":"5.4273553","title":"Do Deep Generative Models Know What They Don't Know?","cluster":"2","author":"['Eric Nalisnick', 'Akihiro Matsukawa', 'Yee Whye Teh', 'Dilan Gorur', 'Balaji Lakshminarayanan']","source":"arxiv","tags":"[]","date":"2018-10-22 08:32:02+00:00","url":"http://arxiv.org/abs/1810.09136v3"},{"x":"10.193145","y":"6.3188043","title":"Shielding Atari Games with Bounded Prescience","cluster":"4","author":"['Mirco Giacobbe', 'Mohammadhosein Hasanbeig', 'Daniel Kroening', 'Hjalmar Wijk']","source":"alignment newsletter","tags":"[]","date":"2021-01-20 14:22:04+00:00","url":"http://arxiv.org/abs/2101.08153v2"},{"x":"7.834861","y":"5.229005","title":"From ImageNet to Image Classification: Contextualizing Progress on Benchmarks","cluster":"2","author":"['Dimitris Tsipras', 'Shibani Santurkar', 'Logan Engstrom', 'Andrew Ilyas', 'Aleksander Madry']","source":"arxiv","tags":"[]","date":"2020-05-22 17:39:16+00:00","url":"http://arxiv.org/abs/2005.11295v1"},{"x":"11.287097","y":"5.7614064","title":"Learning to Interactively Learn and Assist","cluster":"0","author":"['Mark Woodward', 'Chelsea Finn', 'Karol Hausman']","source":"arxiv","tags":"[]","date":"2019-06-24 19:23:27+00:00","url":"http://arxiv.org/abs/1906.10187v3"},{"x":"8.232724","y":"5.1295676","title":"Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation","cluster":"2","author":"['Sanghun Jung', 'Jungsoo Lee', 'Daehoon Gwak', 'Sungha Choi', 'Jaegul Choo']","source":"arxiv","tags":"[]","date":"2021-07-23 14:25:02+00:00","url":"http://arxiv.org/abs/2107.11264v4"},{"x":"10.320095","y":"5.94861","title":"Expert-augmented actor-critic for ViZDoom and Montezumas Revenge","cluster":"0","author":"['Michał Garmulewicz', 'Henryk Michalewski', 'Piotr Miłoś']","source":"alignment newsletter","tags":"[]","date":"2018-09-10 16:36:22+00:00","url":"http://arxiv.org/abs/1809.03447v1"},{"x":"9.798159","y":"9.279853","title":"Delphi: Towards Machine Ethics and Norms","cluster":"2","author":"['Liwei Jiang', 'Jena D. Hwang', 'Chandra Bhagavatula', 'Ronan Le Bras', 'Maxwell Forbes', 'Jon Borchardt', 'Jenny Liang', 'Oren Etzioni', 'Maarten Sap', 'Yejin Choi']","source":"arxiv","tags":"[]","date":"2021-10-14 17:38:12+00:00","url":"http://arxiv.org/abs/2110.07574v1"},{"x":"8.158003","y":"5.3880644","title":"Learning Not to Learn: Training Deep Neural Networks with Biased Data","cluster":"2","author":"['Byungju Kim', 'Hyunwoo Kim', 'Kyungsu Kim', 'Sungjin Kim', 'Junmo Kim']","source":"arxiv","tags":"[]","date":"2018-12-26 16:01:29+00:00","url":"http://arxiv.org/abs/1812.10352v2"},{"x":"8.51853","y":"5.61484","title":"Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies","cluster":"2","author":"['Alessandro Achille', 'Tom Eccles', 'Loic Matthey', 'Christopher P. Burgess', 'Nick Watters', 'Alexander Lerchner', 'Irina Higgins']","source":"arxiv","tags":"[]","date":"2018-08-20 15:15:32+00:00","url":"http://arxiv.org/abs/1808.06508v1"},{"x":"7.5028987","y":"7.079203","title":"TuringAdvice: A Generative and Dynamic Evaluation of Language Use","cluster":"2","author":"['Rowan Zellers', 'Ari Holtzman', 'Elizabeth Clark', 'Lianhui Qin', 'Ali Farhadi', 'Yejin Choi']","source":"alignment newsletter","tags":"[]","date":"2020-04-07 18:00:03+00:00","url":"http://arxiv.org/abs/2004.03607v2"},{"x":"11.749607","y":"5.654828","title":"Robot Planning with Mathematical Models of Human State and Action","cluster":"0","author":"['Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2017-05-11 15:02:34+00:00","url":"http://arxiv.org/abs/1705.04226v2"},{"x":"11.588077","y":"6.0685263","title":"An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning","cluster":"0","author":"['Dhruv Malik', 'Malayandi Palaniappan', 'Jaime F. Fisac', 'Dylan Hadfield-Menell', 'Stuart Russell', 'Anca D. Dragan']","source":"alignment newsletter","tags":"[]","date":"2018-06-11 06:06:43+00:00","url":"http://arxiv.org/abs/1806.03820v1"},{"x":"10.96648","y":"6.326424","title":"Non-Adversarial Imitation Learning and its Connections to Adversarial Methods","cluster":"0","author":"['Oleg Arenz', 'Gerhard Neumann']","source":"alignment newsletter","tags":"[]","date":"2020-08-08 13:43:06+00:00","url":"http://arxiv.org/abs/2008.03525v1"},{"x":"10.970132","y":"6.2499714","title":"Fully General Online Imitation Learning","cluster":"0","author":"['Michael K. Cohen', 'Marcus Hutter', 'Neel Nanda']","source":"arxiv","tags":"[]","date":"2021-02-17 10:57:37+00:00","url":"http://arxiv.org/abs/2102.08686v1"},{"x":"8.095841","y":"5.0019937","title":"VOS: Learning What You Don't Know by Virtual Outlier Synthesis","cluster":"2","author":"['Xuefeng Du', 'Zhaoning Wang', 'Mu Cai', 'Yixuan Li']","source":"arxiv","tags":"[]","date":"2022-02-02 18:43:01+00:00","url":"http://arxiv.org/abs/2202.01197v3"},{"x":"9.558163","y":"9.968765","title":"(When) Is Truth-telling Favored in AI Debate?","cluster":"3","author":"['Vojtěch Kovařík', 'Ryan Carey']","source":"arxiv","tags":"[]","date":"2019-11-11 13:49:43+00:00","url":"http://arxiv.org/abs/1911.04266v3"},{"x":"10.536512","y":"8.227992","title":"Value Alignment Verification","cluster":"1","author":"['Daniel S. Brown', 'Jordan Schneider', 'Anca D. Dragan', 'Scott Niekum']","source":"arxiv","tags":"[]","date":"2020-12-02 22:04:01+00:00","url":"http://arxiv.org/abs/2012.01557v2"},{"x":"7.842407","y":"5.7711954","title":"Scaling Laws for Autoregressive Generative Modeling","cluster":"2","author":"['Tom Henighan', 'Jared Kaplan', 'Mor Katz', 'Mark Chen', 'Christopher Hesse', 'Jacob Jackson', 'Heewoo Jun', 'Tom B. Brown', 'Prafulla Dhariwal', 'Scott Gray', 'Chris Hallacy', 'Benjamin Mann', 'Alec Radford', 'Aditya Ramesh', 'Nick Ryder', 'Daniel M. Ziegler', 'John Schulman', 'Dario Amodei', 'Sam McCandlish']","source":"alignment newsletter","tags":"[]","date":"2020-10-28 02:17:24+00:00","url":"http://arxiv.org/abs/2010.14701v2"},{"x":"8.894403","y":"9.035146","title":"TanksWorld: A Multi-Agent Environment for AI Safety Research","cluster":"4","author":"['Corban G. Rivera', 'Olivia Lyons', 'Arielle Summitt', 'Ayman Fatima', 'Ji Pak', 'William Shao', 'Robert Chalmers', 'Aryeh Englander', 'Edward W. Staley', 'I-Jeng Wang', 'Ashley J. Llorens']","source":"alignment newsletter","tags":"[]","date":"2020-02-25 21:00:52+00:00","url":"http://arxiv.org/abs/2002.11174v1"},{"x":"8.6133585","y":"9.019197","title":"Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures","cluster":"3","author":"['Roman V. Yampolskiy', 'M. S. Spellchecker']","source":"arxiv","tags":"[]","date":"2016-10-25 18:14:24+00:00","url":"http://arxiv.org/abs/1610.07997v1"},{"x":"10.083787","y":"6.3149047","title":"Guiding Policies with Language via Meta-Learning","cluster":"0","author":"['John D. Co-Reyes', 'Abhishek Gupta', 'Suvansh Sanjeev', 'Nick Altieri', 'Jacob Andreas', 'John DeNero', 'Pieter Abbeel', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2018-11-19 18:58:42+00:00","url":"http://arxiv.org/abs/1811.07882v2"},{"x":"10.497538","y":"6.216559","title":"Execute Order 66: Targeted Data Poisoning for Reinforcement Learning","cluster":"0","author":"['Harrison Foley', 'Liam Fowl', 'Tom Goldstein', 'Gavin Taylor']","source":"arxiv","tags":"[]","date":"2022-01-03 17:09:32+00:00","url":"http://arxiv.org/abs/2201.00762v1"},{"x":"8.948237","y":"8.322228","title":"Causal Analysis of Agent Behavior for AI Safety","cluster":"4","author":"['Grégoire Déletang', 'Jordi Grau-Moya', 'Miljan Martic', 'Tim Genewein', 'Tom McGrath', 'Vladimir Mikulik', 'Markus Kunesch', 'Shane Legg', 'Pedro A. Ortega']","source":"arxiv","tags":"[]","date":"2021-03-05 20:51:12+00:00","url":"http://arxiv.org/abs/2103.03938v1"},{"x":"8.018461","y":"7.482823","title":"Locating and Editing Factual Knowledge in GPT","cluster":"2","author":"['Kevin Meng', 'David Bau', 'Alex Andonian', 'Yonatan Belinkov']","source":"arxiv","tags":"[]","date":"2022-02-10 18:59:54+00:00","url":"http://arxiv.org/abs/2202.05262v2"},{"x":"11.220514","y":"6.125564","title":"RUDDER: Return Decomposition for Delayed Rewards","cluster":"0","author":"['Jose A. Arjona-Medina', 'Michael Gillhofer', 'Michael Widrich', 'Thomas Unterthiner', 'Johannes Brandstetter', 'Sepp Hochreiter']","source":"arxiv","tags":"[]","date":"2018-06-20 17:34:07+00:00","url":"http://arxiv.org/abs/1806.07857v3"},{"x":"8.006862","y":"7.2863793","title":"Towards a Human-like Open-Domain Chatbot","cluster":"2","author":"['Daniel Adiwardana', 'Minh-Thang Luong', 'David R. So', 'Jamie Hall', 'Noah Fiedel', 'Romal Thoppilan', 'Zi Yang', 'Apoorv Kulshreshtha', 'Gaurav Nemade', 'Yifeng Lu', 'Quoc V. Le']","source":"alignment newsletter","tags":"[]","date":"2020-01-27 18:53:15+00:00","url":"http://arxiv.org/abs/2001.09977v3"},{"x":"12.503072","y":"8.506654","title":"Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents","cluster":"1","author":"['Andrew Critch']","source":"arxiv","tags":"[]","date":"2016-02-12 19:51:54+00:00","url":"http://arxiv.org/abs/1602.04184v5"},{"x":"8.464613","y":"5.0622277","title":"Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations","cluster":"2","author":"['Alex Lamb', 'Jonathan Binas', 'Anirudh Goyal', 'Dmitriy Serdyuk', 'Sandeep Subramanian', 'Ioannis Mitliagkas', 'Yoshua Bengio']","source":"arxiv","tags":"[]","date":"2018-04-07 00:11:05+00:00","url":"http://arxiv.org/abs/1804.02485v1"},{"x":"8.367772","y":"5.326132","title":"GAN Dissection: Visualizing and Understanding Generative Adversarial Networks","cluster":"2","author":"['David Bau', 'Jun-Yan Zhu', 'Hendrik Strobelt', 'Bolei Zhou', 'Joshua B. Tenenbaum', 'William T. Freeman', 'Antonio Torralba']","source":"arxiv","tags":"[]","date":"2018-11-26 18:59:07+00:00","url":"http://arxiv.org/abs/1811.10597v2"},{"x":"7.4470687","y":"6.545155","title":"Gradient-based Adversarial Attacks against Text Transformers","cluster":"2","author":"['Chuan Guo', 'Alexandre Sablayrolles', 'Hervé Jégou', 'Douwe Kiela']","source":"arxiv","tags":"[]","date":"2021-04-15 17:43:43+00:00","url":"http://arxiv.org/abs/2104.13733v1"},{"x":"11.802066","y":"6.017615","title":"Pragmatic-Pedagogic Value Alignment","cluster":"0","author":"['Jaime F. Fisac', 'Monica A. Gates', 'Jessica B. Hamrick', 'Chang Liu', 'Dylan Hadfield-Menell', 'Malayandi Palaniappan', 'Dhruv Malik', 'S. Shankar Sastry', 'Thomas L. Griffiths', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2017-07-20 03:07:19+00:00","url":"http://arxiv.org/abs/1707.06354v2"},{"x":"11.687354","y":"6.2647567","title":"Active Inverse Reward Design","cluster":"0","author":"['Sören Mindermann', 'Rohin Shah', 'Adam Gleave', 'Dylan Hadfield-Menell']","source":"alignment newsletter","tags":"[]","date":"2018-09-09 23:30:59+00:00","url":"http://arxiv.org/abs/1809.03060v3"},{"x":"7.9162393","y":"9.552015","title":"The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?","cluster":"3","author":"['Toby Shevlane', 'Allan Dafoe']","source":"alignment newsletter","tags":"[]","date":"2019-12-27 10:20:44+00:00","url":"http://arxiv.org/abs/2001.00463v2"},{"x":"8.04286","y":"6.269541","title":"Dynamic Control Flow in Large-Scale Machine Learning","cluster":"2","author":"['Yuan Yu', 'Martín Abadi', 'Paul Barham', 'Eugene Brevdo', 'Mike Burrows', 'Andy Davis', 'Jeff Dean', 'Sanjay Ghemawat', 'Tim Harley', 'Peter Hawkins', 'Michael Isard', 'Manjunath Kudlur', 'Rajat Monga', 'Derek Murray', 'Xiaoqiang Zheng']","source":"arxiv","tags":"[]","date":"2018-05-04 13:40:07+00:00","url":"http://arxiv.org/abs/1805.01772v1"},{"x":"7.6415715","y":"6.23769","title":"Sequence to Sequence Learning with Neural Networks","cluster":"2","author":"['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V. Le']","source":"arxiv","tags":"[]","date":"2014-09-10 19:55:35+00:00","url":"http://arxiv.org/abs/1409.3215v3"},{"x":"7.6453757","y":"6.4698157","title":"Attention is not Explanation","cluster":"2","author":"['Sarthak Jain', 'Byron C. Wallace']","source":"alignment newsletter","tags":"[]","date":"2019-02-26 19:59:15+00:00","url":"http://arxiv.org/abs/1902.10186v3"},{"x":"11.496483","y":"6.7540402","title":"Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism","cluster":"0","author":"['Paria Rashidinejad', 'Banghua Zhu', 'Cong Ma', 'Jiantao Jiao', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2021-03-22 17:27:08+00:00","url":"http://arxiv.org/abs/2103.12021v1"},{"x":"7.630959","y":"6.326618","title":"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation","cluster":"2","author":"['Mia Xu Chen', 'Orhan Firat', 'Ankur Bapna', 'Melvin Johnson', 'Wolfgang Macherey', 'George Foster', 'Llion Jones', 'Niki Parmar', 'Mike Schuster', 'Zhifeng Chen', 'Yonghui Wu', 'Macduff Hughes']","source":"arxiv","tags":"[]","date":"2018-04-26 01:24:39+00:00","url":"http://arxiv.org/abs/1804.09849v2"},{"x":"10.061096","y":"6.2461114","title":"An Inductive Synthesis Framework for Verifiable Reinforcement Learning","cluster":"0","author":"['He Zhu', 'Zikang Xiong', 'Stephen Magill', 'Suresh Jagannathan']","source":"arxiv","tags":"[]","date":"2019-07-16 21:57:17+00:00","url":"http://arxiv.org/abs/1907.07273v1"},{"x":"9.782994","y":"5.830299","title":"Dota 2 with Large Scale Deep Reinforcement Learning","cluster":"0","author":"['OpenAI', ':', 'Christopher Berner', 'Greg Brockman', 'Brooke Chan', 'Vicki Cheung', 'Przemysław Dębiak', 'Christy Dennison', 'David Farhi', 'Quirin Fischer', 'Shariq Hashme', 'Chris Hesse', 'Rafal Józefowicz', 'Scott Gray', 'Catherine Olsson', 'Jakub Pachocki', 'Michael Petrov', 'Henrique P. d. O. Pinto', 'Jonathan Raiman', 'Tim Salimans', 'Jeremy Schlatter', 'Jonas Schneider', 'Szymon Sidor', 'Ilya Sutskever', 'Jie Tang', 'Filip Wolski', 'Susan Zhang']","source":"alignment newsletter","tags":"[]","date":"2019-12-13 19:56:40+00:00","url":"http://arxiv.org/abs/1912.06680v1"},{"x":"11.380438","y":"10.451204","title":"Cartesian Frames","cluster":"1","author":"['Scott Garrabrant', 'Daniel A. Herrmann', 'Josiah Lopez-Wild']","source":"arxiv","tags":"[]","date":"2021-09-22 19:27:05+00:00","url":"http://arxiv.org/abs/2109.10996v1"},{"x":"11.425283","y":"7.2024174","title":"Exploration Potential","cluster":"0","author":"['Jan Leike']","source":"arxiv","tags":"[]","date":"2016-09-16 10:55:27+00:00","url":"http://arxiv.org/abs/1609.04994v3"},{"x":"8.718556","y":"6.3101673","title":"Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations","cluster":"2","author":"['Matthew Watson', 'Bashar Awwad Shiekh Hasan', 'Noura Al Moubayed']","source":"arxiv","tags":"[]","date":"2021-05-14 12:16:47+00:00","url":"http://arxiv.org/abs/2105.06791v2"},{"x":"11.615356","y":"6.3681045","title":"Preferences Implicit in the State of the World","cluster":"0","author":"['Rohin Shah', 'Dmitrii Krasheninnikov', 'Jordan Alexander', 'Pieter Abbeel', 'Anca Dragan']","source":"arxiv","tags":"[]","date":"2019-02-12 00:50:56+00:00","url":"http://arxiv.org/abs/1902.04198v2"},{"x":"11.408715","y":"6.408096","title":"A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress","cluster":"0","author":"['Saurabh Arora', 'Prashant Doshi']","source":"alignment newsletter","tags":"[]","date":"2018-06-18 18:26:29+00:00","url":"http://arxiv.org/abs/1806.06877v3"},{"x":"11.7708","y":"7.019724","title":"Detecting Spiky Corruption in Markov Decision Processes","cluster":"0","author":"['Jason Mancuso', 'Tomasz Kisielewski', 'David Lindner', 'Alok Singh']","source":"arxiv","tags":"[]","date":"2019-06-30 20:30:05+00:00","url":"http://arxiv.org/abs/1907.00452v1"},{"x":"10.164536","y":"5.6364565","title":"Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation","cluster":"0","author":"['Niels Justesen', 'Ruben Rodriguez Torrado', 'Philip Bontrager', 'Ahmed Khalifa', 'Julian Togelius', 'Sebastian Risi']","source":"arxiv","tags":"[]","date":"2018-06-28 01:16:11+00:00","url":"http://arxiv.org/abs/1806.10729v5"},{"x":"8.070439","y":"5.1330085","title":"Robust Learning with Frequency Domain Regularization","cluster":"2","author":"['Weiyu Guo', 'Yidong Ouyang']","source":"arxiv","tags":"[]","date":"2020-07-07 07:29:20+00:00","url":"http://arxiv.org/abs/2007.03244v1"},{"x":"11.448785","y":"6.1725016","title":"Deep Reinforcement Learning with Feedback-based Exploration","cluster":"0","author":"['Jan Scholten', 'Daan Wout', 'Carlos Celemin', 'Jens Kober']","source":"arxiv","tags":"[]","date":"2019-03-14 17:52:46+00:00","url":"http://arxiv.org/abs/1903.06151v1"},{"x":"8.644895","y":"5.0819087","title":"The LogBarrier adversarial attack: making effective use of decision boundary information","cluster":"2","author":"['Chris Finlay', 'Aram-Alexandre Pooladian', 'Adam M. Oberman']","source":"alignment newsletter","tags":"[]","date":"2019-03-25 15:21:20+00:00","url":"http://arxiv.org/abs/1903.10396v1"},{"x":"11.469441","y":"6.238773","title":"Safe Exploration in Continuous Action Spaces","cluster":"0","author":"['Gal Dalal', 'Krishnamurthy Dvijotham', 'Matej Vecerik', 'Todd Hester', 'Cosmin Paduraru', 'Yuval Tassa']","source":"arxiv","tags":"[]","date":"2018-01-26 11:11:18+00:00","url":"http://arxiv.org/abs/1801.08757v1"},{"x":"7.7485113","y":"6.926203","title":"Measuring Mathematical Problem Solving With the MATH Dataset","cluster":"2","author":"['Dan Hendrycks', 'Collin Burns', 'Saurav Kadavath', 'Akul Arora', 'Steven Basart', 'Eric Tang', 'Dawn Song', 'Jacob Steinhardt']","source":"alignment newsletter","tags":"[]","date":"2021-03-05 18:59:39+00:00","url":"http://arxiv.org/abs/2103.03874v2"},{"x":"9.952012","y":"6.081379","title":"What Would Jiminy Cricket Do? Towards Agents That Behave Morally","cluster":"4","author":"['Dan Hendrycks', 'Mantas Mazeika', 'Andy Zou', 'Sahil Patel', 'Christine Zhu', 'Jesus Navarro', 'Dawn Song', 'Bo Li', 'Jacob Steinhardt']","source":"arxiv","tags":"[]","date":"2021-10-25 17:59:31+00:00","url":"http://arxiv.org/abs/2110.13136v2"},{"x":"9.175981","y":"9.104785","title":"Open Problems in Cooperative AI","cluster":"4","author":"['Allan Dafoe', 'Edward Hughes', 'Yoram Bachrach', 'Tantum Collins', 'Kevin R. McKee', 'Joel Z. Leibo', 'Kate Larson', 'Thore Graepel']","source":"alignment newsletter","tags":"[]","date":"2020-12-15 21:39:50+00:00","url":"http://arxiv.org/abs/2012.08630v1"},{"x":"11.494316","y":"7.1207805","title":"'Indifference' methods for managing agent rewards","cluster":"0","author":"['Stuart Armstrong', \"Xavier O'Rourke\"]","source":"arxiv","tags":"[]","date":"2017-12-18 12:28:45+00:00","url":"http://arxiv.org/abs/1712.06365v4"},{"x":"8.82681","y":"8.483906","title":"Low Impact Artificial Intelligences","cluster":"4","author":"['Stuart Armstrong', 'Benjamin Levinstein']","source":"arxiv","tags":"[]","date":"2017-05-30 16:15:16+00:00","url":"http://arxiv.org/abs/1705.10720v1"},{"x":"7.935849","y":"5.484071","title":"Sanity Checks for Saliency Maps","cluster":"2","author":"['Julius Adebayo', 'Justin Gilmer', 'Michael Muelly', 'Ian Goodfellow', 'Moritz Hardt', 'Been Kim']","source":"arxiv","tags":"[]","date":"2018-10-08 07:27:11+00:00","url":"http://arxiv.org/abs/1810.03292v3"},{"x":"11.151077","y":"6.524808","title":"Safe Reinforcement Learning via Probabilistic Shields","cluster":"0","author":"['Nils Jansen', 'Bettina Könighofer', 'Sebastian Junges', 'Alexandru C. Serban', 'Roderick Bloem']","source":"alignment newsletter","tags":"[]","date":"2018-07-16 20:29:04+00:00","url":"http://arxiv.org/abs/1807.06096v2"},{"x":"8.49924","y":"5.0341105","title":"A Geometric Perspective on the Transferability of Adversarial Directions","cluster":"2","author":"['Zachary Charles', 'Harrison Rosenberg', 'Dimitris Papailiopoulos']","source":"arxiv","tags":"[]","date":"2018-11-08 16:23:50+00:00","url":"http://arxiv.org/abs/1811.03531v1"},{"x":"11.701618","y":"5.808238","title":"Shared Autonomy via Hindsight Optimization","cluster":"0","author":"['Shervin Javdani', 'Siddhartha S. Srinivasa', 'J. Andrew Bagnell']","source":"alignment newsletter","tags":"[]","date":"2015-03-26 04:50:49+00:00","url":"http://arxiv.org/abs/1503.07619v2"},{"x":"7.5794663","y":"6.6405625","title":"Scaling Laws for Transfer","cluster":"2","author":"['Danny Hernandez', 'Jared Kaplan', 'Tom Henighan', 'Sam McCandlish']","source":"alignment newsletter","tags":"[]","date":"2021-02-02 04:07:38+00:00","url":"http://arxiv.org/abs/2102.01293v1"},{"x":"7.670516","y":"6.489168","title":"RAFT: A Real-World Few-Shot Text Classification Benchmark","cluster":"2","author":"['Neel Alex', 'Eli Lifland', 'Lewis Tunstall', 'Abhishek Thakur', 'Pegah Maham', 'C. Jess Riedel', 'Emmie Hine', 'Carolyn Ashurst', 'Paul Sedille', 'Alexis Carlier', 'Michael Noetel', 'Andreas Stuhlmüller']","source":"arxiv","tags":"[]","date":"2021-09-28 22:35:31+00:00","url":"http://arxiv.org/abs/2109.14076v3"},{"x":"8.325621","y":"5.3782606","title":"Realistic Evaluation of Deep Semi-Supervised Learning Algorithms","cluster":"2","author":"['Avital Oliver', 'Augustus Odena', 'Colin Raffel', 'Ekin D. Cubuk', 'Ian J. Goodfellow']","source":"arxiv","tags":"[]","date":"2018-04-24 17:54:44+00:00","url":"http://arxiv.org/abs/1804.09170v4"},{"x":"9.959855","y":"8.361891","title":"Consequences of Misaligned AI","cluster":"4","author":"['Simon Zhuang', 'Dylan Hadfield-Menell']","source":"arxiv","tags":"[]","date":"2021-02-07 19:34:04+00:00","url":"http://arxiv.org/abs/2102.03896v1"},{"x":"8.081876","y":"4.9677534","title":"ROBIN : A Benchmark for Robustness to Individual Nuisances in Real-World Out-of-Distribution Shifts","cluster":"2","author":"['Bingchen Zhao', 'Shaozuo Yu', 'Wufei Ma', 'Mingxin Yu', 'Shenxiao Mei', 'Angtian Wang', 'Ju He', 'Alan Yuille', 'Adam Kortylewski']","source":"arxiv","tags":"[]","date":"2021-11-29 06:18:46+00:00","url":"http://arxiv.org/abs/2111.14341v2"},{"x":"8.037937","y":"9.544455","title":"The Logic of Strategic Assets: From Oil to Artificial Intelligence","cluster":"3","author":"['Jeffrey Ding', 'Allan Dafoe']","source":"arxiv","tags":"[]","date":"2020-01-09 22:16:05+00:00","url":"http://arxiv.org/abs/2001.03246v2"},{"x":"11.247154","y":"7.234929","title":"Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors","cluster":"0","author":"['Raphael Köster', 'Dylan Hadfield-Menell', 'Gillian K. Hadfield', 'Joel Z. Leibo']","source":"arxiv","tags":"[]","date":"2020-01-25 14:00:33+00:00","url":"http://arxiv.org/abs/2001.09318v1"},{"x":"10.246272","y":"5.6366434","title":"Inverse reinforcement learning for video games","cluster":"0","author":"['Aaron Tucker', 'Adam Gleave', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2018-10-24 20:00:50+00:00","url":"http://arxiv.org/abs/1810.10593v1"},{"x":"11.199484","y":"6.591226","title":"Active Reinforcement Learning with Monte-Carlo Tree Search","cluster":"0","author":"['Sebastian Schulze', 'Owain Evans']","source":"arxiv","tags":"[]","date":"2018-03-13 16:35:25+00:00","url":"http://arxiv.org/abs/1803.04926v3"},{"x":"7.78598","y":"6.436923","title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding","cluster":"2","author":"['Dmitry Lepikhin', 'HyoukJoong Lee', 'Yuanzhong Xu', 'Dehao Chen', 'Orhan Firat', 'Yanping Huang', 'Maxim Krikun', 'Noam Shazeer', 'Zhifeng Chen']","source":"alignment newsletter","tags":"[]","date":"2020-06-30 10:42:02+00:00","url":"http://arxiv.org/abs/2006.16668v1"},{"x":"7.7641807","y":"9.318858","title":"Measurement in AI Policy: Opportunities and Challenges","cluster":"3","author":"['Saurabh Mishra', 'Jack Clark', 'C. Raymond Perrault']","source":"alignment newsletter","tags":"[]","date":"2020-09-10 05:37:40+00:00","url":"http://arxiv.org/abs/2009.09071v1"},{"x":"8.487661","y":"5.1708584","title":"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning","cluster":"2","author":"['Nicolas Papernot', 'Patrick McDaniel']","source":"arxiv","tags":"[]","date":"2018-03-13 13:02:13+00:00","url":"http://arxiv.org/abs/1803.04765v1"},{"x":"11.500002","y":"5.3696246","title":"What Matters in Learning from Offline Human Demonstrations for Robot Manipulation","cluster":"0","author":"['Ajay Mandlekar', 'Danfei Xu', 'Josiah Wong', 'Soroush Nasiriany', 'Chen Wang', 'Rohun Kulkarni', 'Li Fei-Fei', 'Silvio Savarese', 'Yuke Zhu', 'Roberto Martín-Martín']","source":"alignment newsletter","tags":"[]","date":"2021-08-06 20:48:30+00:00","url":"http://arxiv.org/abs/2108.03298v2"},{"x":"8.351323","y":"5.9128976","title":"Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions","cluster":"2","author":"['Matthew MacKay', 'Paul Vicol', 'Jon Lorraine', 'David Duvenaud', 'Roger Grosse']","source":"arxiv","tags":"[]","date":"2019-03-07 18:26:46+00:00","url":"http://arxiv.org/abs/1903.03088v1"},{"x":"11.257424","y":"8.679191","title":"Learning the Preferences of Ignorant, Inconsistent Agents","cluster":"1","author":"['Owain Evans', 'Andreas Stuhlmueller', 'Noah D. Goodman']","source":"arxiv","tags":"[]","date":"2015-12-18 00:24:08+00:00","url":"http://arxiv.org/abs/1512.05832v1"},{"x":"11.001434","y":"6.2501493","title":"World Discovery Models","cluster":"0","author":"['Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Bernardo Avila Pires', 'Jean-Bastien Grill', 'Florent Altché', 'Rémi Munos']","source":"arxiv","tags":"[]","date":"2019-02-20 18:07:18+00:00","url":"http://arxiv.org/abs/1902.07685v3"},{"x":"8.094343","y":"5.7545376","title":"Analyzing Inverse Problems with Invertible Neural Networks","cluster":"2","author":"['Lynton Ardizzone', 'Jakob Kruse', 'Sebastian Wirkert', 'Daniel Rahner', 'Eric W. Pellegrini', 'Ralf S. Klessen', 'Lena Maier-Hein', 'Carsten Rother', 'Ullrich Köthe']","source":"arxiv","tags":"[]","date":"2018-08-14 14:58:59+00:00","url":"http://arxiv.org/abs/1808.04730v3"},{"x":"8.45107","y":"5.608791","title":"Soft Calibration Objectives for Neural Networks","cluster":"2","author":"['Archit Karandikar', 'Nicholas Cain', 'Dustin Tran', 'Balaji Lakshminarayanan', 'Jonathon Shlens', 'Michael C. Mozer', 'Becca Roelofs']","source":"arxiv","tags":"[]","date":"2021-07-30 23:30:20+00:00","url":"http://arxiv.org/abs/2108.00106v2"},{"x":"8.024668","y":"6.2677884","title":"Large scale distributed neural network training through online distillation","cluster":"2","author":"['Rohan Anil', 'Gabriel Pereyra', 'Alexandre Passos', 'Robert Ormandi', 'George E. Dahl', 'Geoffrey E. Hinton']","source":"arxiv","tags":"[]","date":"2018-04-09 20:56:03+00:00","url":"http://arxiv.org/abs/1804.03235v2"},{"x":"9.68746","y":"9.412987","title":"Incomplete Contracting and AI Alignment","cluster":"4","author":"['Dylan Hadfield-Menell', 'Gillian Hadfield']","source":"alignment newsletter","tags":"[]","date":"2018-04-12 01:22:50+00:00","url":"http://arxiv.org/abs/1804.04268v1"},{"x":"10.184996","y":"5.903168","title":"Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization","cluster":"0","author":"['Alexandre Laterre', 'Yunguan Fu', 'Mohamed Khalil Jabri', 'Alain-Sam Cohen', 'David Kas', 'Karl Hajjar', 'Torbjorn S. Dahl', 'Amine Kerkeni', 'Karim Beguir']","source":"arxiv","tags":"[]","date":"2018-07-04 16:40:53+00:00","url":"http://arxiv.org/abs/1807.01672v3"},{"x":"11.335411","y":"7.65325","title":"Uncertain Decisions Facilitate Better Preference Learning","cluster":"0","author":"['Cassidy Laidlaw', 'Stuart Russell']","source":"alignment newsletter","tags":"[]","date":"2021-06-19 00:11:13+00:00","url":"http://arxiv.org/abs/2106.10394v2"},{"x":"11.535684","y":"5.439689","title":"Few-Shot Goal Inference for Visuomotor Learning and Planning","cluster":"0","author":"['Annie Xie', 'Avi Singh', 'Sergey Levine', 'Chelsea Finn']","source":"arxiv","tags":"[]","date":"2018-09-30 22:57:58+00:00","url":"http://arxiv.org/abs/1810.00482v1"},{"x":"9.716562","y":"8.935716","title":"Mammalian Value Systems","cluster":"4","author":"['Gopal P. Sarma', 'Nick J. Hay']","source":"arxiv","tags":"[]","date":"2016-07-28 01:22:26+00:00","url":"http://arxiv.org/abs/1607.08289v4"},{"x":"8.159093","y":"4.971548","title":"Theoretically Principled Trade-off between Robustness and Accuracy","cluster":"2","author":"['Hongyang Zhang', 'Yaodong Yu', 'Jiantao Jiao', 'Eric P. Xing', 'Laurent El Ghaoui', 'Michael I. Jordan']","source":"alignment newsletter","tags":"[]","date":"2019-01-24 18:43:57+00:00","url":"http://arxiv.org/abs/1901.08573v3"},{"x":"11.287064","y":"8.6390705","title":"The Social Cost of Strategic Classification","cluster":"3","author":"['Smitha Milli', 'John Miller', 'Anca D. Dragan', 'Moritz Hardt']","source":"arxiv","tags":"[]","date":"2018-08-25 18:31:52+00:00","url":"http://arxiv.org/abs/1808.08460v2"},{"x":"8.417066","y":"6.501923","title":"Deep Learning for Symbolic Mathematics","cluster":"2","author":"['Guillaume Lample', 'François Charton']","source":"alignment newsletter","tags":"[]","date":"2019-12-02 15:05:24+00:00","url":"http://arxiv.org/abs/1912.01412v1"},{"x":"7.6021214","y":"6.6966863","title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections","cluster":"2","author":"['Ruiqi Zhong', 'Kristy Lee', 'Zheng Zhang', 'Dan Klein']","source":"alignment newsletter","tags":"[]","date":"2021-04-10 02:57:22+00:00","url":"http://arxiv.org/abs/2104.04670v5"},{"x":"8.355778","y":"5.7135534","title":"Deep Ensembles: A Loss Landscape Perspective","cluster":"2","author":"['Stanislav Fort', 'Huiyi Hu', 'Balaji Lakshminarayanan']","source":"arxiv","tags":"[]","date":"2019-12-05 17:48:18+00:00","url":"http://arxiv.org/abs/1912.02757v2"},{"x":"10.174648","y":"5.672181","title":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model","cluster":"0","author":"['Julian Schrittwieser', 'Ioannis Antonoglou', 'Thomas Hubert', 'Karen Simonyan', 'Laurent Sifre', 'Simon Schmitt', 'Arthur Guez', 'Edward Lockhart', 'Demis Hassabis', 'Thore Graepel', 'Timothy Lillicrap', 'David Silver']","source":"alignment newsletter","tags":"[]","date":"2019-11-19 13:58:52+00:00","url":"http://arxiv.org/abs/1911.08265v2"},{"x":"9.173293","y":"7.913015","title":"Energetics of the brain and AI","cluster":"4","author":"['Anders Sandberg']","source":"arxiv","tags":"[]","date":"2016-02-12 11:32:59+00:00","url":"http://arxiv.org/abs/1602.04019v1"},{"x":"11.4755125","y":"5.4349046","title":"Unsupervised Visuomotor Control through Distributional Planning Networks","cluster":"0","author":"['Tianhe Yu', 'Gleb Shevchuk', 'Dorsa Sadigh', 'Chelsea Finn']","source":"arxiv","tags":"[]","date":"2019-02-14 18:54:54+00:00","url":"http://arxiv.org/abs/1902.05542v1"},{"x":"11.266899","y":"5.289739","title":"Zero-Shot Visual Imitation","cluster":"0","author":"['Deepak Pathak', 'Parsa Mahmoudieh', 'Guanghao Luo', 'Pulkit Agrawal', 'Dian Chen', 'Yide Shentu', 'Evan Shelhamer', 'Jitendra Malik', 'Alexei A. Efros', 'Trevor Darrell']","source":"arxiv","tags":"[]","date":"2018-04-23 17:58:26+00:00","url":"http://arxiv.org/abs/1804.08606v1"},{"x":"10.696385","y":"6.5587626","title":"SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards","cluster":"0","author":"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2019-05-27 10:29:31+00:00","url":"http://arxiv.org/abs/1905.11108v3"},{"x":"11.375575","y":"6.7847457","title":"Multi-Principal Assistance Games","cluster":"0","author":"['Arnaud Fickinger', 'Simon Zhuang', 'Dylan Hadfield-Menell', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2020-07-19 00:23:25+00:00","url":"http://arxiv.org/abs/2007.09540v1"},{"x":"9.504693","y":"9.258555","title":"Modeling AGI Safety Frameworks with Causal Influence Diagrams","cluster":"3","author":"['Tom Everitt', 'Ramana Kumar', 'Victoria Krakovna', 'Shane Legg']","source":"alignment newsletter","tags":"[]","date":"2019-06-20 14:35:03+00:00","url":"http://arxiv.org/abs/1906.08663v1"},{"x":"10.876031","y":"5.687307","title":"Planning to Explore via Self-Supervised World Models","cluster":"0","author":"['Ramanan Sekar', 'Oleh Rybkin', 'Kostas Daniilidis', 'Pieter Abbeel', 'Danijar Hafner', 'Deepak Pathak']","source":"alignment newsletter","tags":"[]","date":"2020-05-12 17:59:45+00:00","url":"http://arxiv.org/abs/2005.05960v2"},{"x":"12.744142","y":"10.259133","title":"Logical Induction","cluster":"1","author":"['Scott Garrabrant', 'Tsvi Benson-Tilsen', 'Andrew Critch', 'Nate Soares', 'Jessica Taylor']","source":"arxiv","tags":"[]","date":"2016-09-12 19:30:56+00:00","url":"http://arxiv.org/abs/1609.03543v5"},{"x":"11.744229","y":"7.215914","title":"Pessimism About Unknown Unknowns Inspires Conservatism","cluster":"0","author":"['Michael K. Cohen', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2020-06-15 20:46:33+00:00","url":"http://arxiv.org/abs/2006.08753v1"},{"x":"9.058221","y":"7.9610047","title":"Can Intelligence Explode?","cluster":"4","author":"['Marcus Hutter']","source":"arxiv","tags":"[]","date":"2012-02-28 10:46:29+00:00","url":"http://arxiv.org/abs/1202.6177v1"},{"x":"10.563778","y":"6.35681","title":"Objective Robustness in Deep Reinforcement Learning","cluster":"0","author":"['Jack Koch', 'Lauro Langosco', 'Jacob Pfau', 'James Le', 'Lee Sharkey']","source":"arxiv","tags":"[]","date":"2021-05-28 21:13:34+00:00","url":"http://arxiv.org/abs/2105.14111v2"},{"x":"7.7998405","y":"6.24475","title":"Neural Architecture Search with Reinforcement Learning","cluster":"2","author":"['Barret Zoph', 'Quoc V. Le']","source":"arxiv","tags":"[]","date":"2016-11-05 00:41:37+00:00","url":"http://arxiv.org/abs/1611.01578v2"},{"x":"10.786633","y":"5.624634","title":"Neural Modular Control for Embodied Question Answering","cluster":"0","author":"['Abhishek Das', 'Georgia Gkioxari', 'Stefan Lee', 'Devi Parikh', 'Dhruv Batra']","source":"arxiv","tags":"[]","date":"2018-10-26 03:58:26+00:00","url":"http://arxiv.org/abs/1810.11181v2"},{"x":"7.570903","y":"6.1906285","title":"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin","cluster":"2","author":"['Dario Amodei', 'Rishita Anubhai', 'Eric Battenberg', 'Carl Case', 'Jared Casper', 'Bryan Catanzaro', 'Jingdong Chen', 'Mike Chrzanowski', 'Adam Coates', 'Greg Diamos', 'Erich Elsen', 'Jesse Engel', 'Linxi Fan', 'Christopher Fougner', 'Tony Han', 'Awni Hannun', 'Billy Jun', 'Patrick LeGresley', 'Libby Lin', 'Sharan Narang', 'Andrew Ng', 'Sherjil Ozair', 'Ryan Prenger', 'Jonathan Raiman', 'Sanjeev Satheesh', 'David Seetapun', 'Shubho Sengupta', 'Yi Wang', 'Zhiqian Wang', 'Chong Wang', 'Bo Xiao', 'Dani Yogatama', 'Jun Zhan', 'Zhenyao Zhu']","source":"arxiv","tags":"[]","date":"2015-12-08 19:13:50+00:00","url":"http://arxiv.org/abs/1512.02595v1"},{"x":"11.466768","y":"5.400646","title":"Learning Latent Plans from Play","cluster":"0","author":"['Corey Lynch', 'Mohi Khansari', 'Ted Xiao', 'Vikash Kumar', 'Jonathan Tompson', 'Sergey Levine', 'Pierre Sermanet']","source":"arxiv","tags":"[]","date":"2019-03-05 18:36:42+00:00","url":"http://arxiv.org/abs/1903.01973v2"},{"x":"7.4570947","y":"6.955679","title":"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models","cluster":"2","author":"['Boxin Wang', 'Chejian Xu', 'Shuohang Wang', 'Zhe Gan', 'Yu Cheng', 'Jianfeng Gao', 'Ahmed Hassan Awadallah', 'Bo Li']","source":"arxiv","tags":"[]","date":"2021-11-04 12:59:55+00:00","url":"http://arxiv.org/abs/2111.02840v2"},{"x":"10.476954","y":"5.9442554","title":"Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning","cluster":"0","author":"['Akanksha Atrey', 'Kaleigh Clary', 'David Jensen']","source":"arxiv","tags":"[]","date":"2019-12-09 12:42:07+00:00","url":"http://arxiv.org/abs/1912.05743v2"},{"x":"11.41536","y":"9.399076","title":"Toward Idealized Decision Theory","cluster":"1","author":"['Nate Soares', 'Benja Fallenstein']","source":"arxiv","tags":"[]","date":"2015-07-07 23:06:59+00:00","url":"http://arxiv.org/abs/1507.01986v1"},{"x":"10.333899","y":"5.7512217","title":"The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach","cluster":"0","author":"['Iulian Vlad Serban', 'Chinnadhurai Sankar', 'Michael Pieper', 'Joelle Pineau', 'Yoshua Bengio']","source":"arxiv","tags":"[]","date":"2018-07-12 16:59:28+00:00","url":"http://arxiv.org/abs/1807.04723v1"},{"x":"10.6669235","y":"6.3940372","title":"Shaking the foundations: delusions in sequence models for interaction and control","cluster":"0","author":"['Pedro A. Ortega', 'Markus Kunesch', 'Grégoire Delétang', 'Tim Genewein', 'Jordi Grau-Moya', 'Joel Veness', 'Jonas Buchli', 'Jonas Degrave', 'Bilal Piot', 'Julien Perolat', 'Tom Everitt', 'Corentin Tallec', 'Emilio Parisotto', 'Tom Erez', 'Yutian Chen', 'Scott Reed', 'Marcus Hutter', 'Nando de Freitas', 'Shane Legg']","source":"alignment newsletter","tags":"[]","date":"2021-10-20 23:31:05+00:00","url":"http://arxiv.org/abs/2110.10819v1"},{"x":"8.708691","y":"6.4842916","title":"Regularizing Black-box Models for Improved Interpretability","cluster":"2","author":"['Gregory Plumb', 'Maruan Al-Shedivat', 'Angel Alexander Cabrera', 'Adam Perer', 'Eric Xing', 'Ameet Talwalkar']","source":"arxiv","tags":"[]","date":"2019-02-18 20:23:12+00:00","url":"http://arxiv.org/abs/1902.06787v6"},{"x":"11.33604","y":"5.5993886","title":"Learning from Observations Using a Single Video Demonstration and Human Feedback","cluster":"0","author":"['Sunil Gandhi', 'Tim Oates', 'Tinoosh Mohsenin', 'Nicholas Waytowich']","source":"alignment newsletter","tags":"[]","date":"2019-09-29 22:44:59+00:00","url":"http://arxiv.org/abs/1909.13392v1"},{"x":"11.325955","y":"9.006637","title":"Augmenting Decision Making via Interactive What-If Analysis","cluster":"1","author":"['Sneha Gathani', 'Madelon Hulsebos', 'James Gale', 'Peter J. Haas', 'Çağatay Demiralp']","source":"arxiv","tags":"[]","date":"2021-09-13 17:54:30+00:00","url":"http://arxiv.org/abs/2109.06160v4"},{"x":"10.245988","y":"5.651119","title":"Playing hard exploration games by watching YouTube","cluster":"0","author":"['Yusuf Aytar', 'Tobias Pfaff', 'David Budden', 'Tom Le Paine', 'Ziyu Wang', 'Nando de Freitas']","source":"alignment newsletter","tags":"[]","date":"2018-05-29 17:19:36+00:00","url":"http://arxiv.org/abs/1805.11592v2"},{"x":"11.692708","y":"5.7675185","title":"Learning under Misspecified Objective Spaces","cluster":"0","author":"['Andreea Bobu', 'Andrea Bajcsy', 'Jaime F. Fisac', 'Anca D. Dragan']","source":"alignment newsletter","tags":"[]","date":"2018-10-11 17:58:27+00:00","url":"http://arxiv.org/abs/1810.05157v4"},{"x":"8.15955","y":"5.6594887","title":"Is SGD a Bayesian sampler? Well, almost","cluster":"2","author":"['Chris Mingard', 'Guillermo Valle-Pérez', 'Joar Skalse', 'Ard A. Louis']","source":"alignment newsletter","tags":"[]","date":"2020-06-26 19:45:36+00:00","url":"http://arxiv.org/abs/2006.15191v2"},{"x":"11.299571","y":"6.1447716","title":"B-Pref: Benchmarking Preference-Based Reinforcement Learning","cluster":"0","author":"['Kimin Lee', 'Laura Smith', 'Anca Dragan', 'Pieter Abbeel']","source":"arxiv","tags":"[]","date":"2021-11-04 17:32:06+00:00","url":"http://arxiv.org/abs/2111.03026v1"},{"x":"7.520596","y":"7.067447","title":"Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts","cluster":"2","author":"['Hanmeng Liu', 'Leyang Cui', 'Jian Liu', 'Yue Zhang']","source":"arxiv","tags":"[]","date":"2020-11-10 02:31:31+00:00","url":"http://arxiv.org/abs/2011.04864v1"},{"x":"8.082361","y":"4.996711","title":"3D Common Corruptions and Data Augmentation","cluster":"2","author":"['Oğuzhan Fatih Kar', 'Teresa Yeo', 'Andrei Atanov', 'Amir Zamir']","source":"arxiv","tags":"[]","date":"2022-03-02 22:31:16+00:00","url":"http://arxiv.org/abs/2203.01441v1"},{"x":"10.572715","y":"6.2466574","title":"Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot","cluster":"0","author":"['Joel Z. Leibo', 'Edgar Duéñez-Guzmán', 'Alexander Sasha Vezhnevets', 'John P. Agapiou', 'Peter Sunehag', 'Raphael Koster', 'Jayd Matyas', 'Charles Beattie', 'Igor Mordatch', 'Thore Graepel']","source":"alignment newsletter","tags":"[]","date":"2021-07-14 17:22:14+00:00","url":"http://arxiv.org/abs/2107.06857v1"},{"x":"10.903161","y":"5.7807636","title":"Learning Efficient Representation for Intrinsic Motivation","cluster":"0","author":"['Ruihan Zhao', 'Stas Tiomkin', 'Pieter Abbeel']","source":"arxiv","tags":"[]","date":"2019-12-04 07:48:40+00:00","url":"http://arxiv.org/abs/1912.02624v3"},{"x":"11.31984","y":"5.8214564","title":"Learning What To Do by Simulating the Past","cluster":"0","author":"['David Lindner', 'Rohin Shah', 'Pieter Abbeel', 'Anca Dragan']","source":"arxiv","tags":"[]","date":"2021-04-08 17:43:29+00:00","url":"http://arxiv.org/abs/2104.03946v2"},{"x":"11.137392","y":"8.844059","title":"Cognitive Model Priors for Predicting Human Decisions","cluster":"1","author":"['David D. Bourgin', 'Joshua C. Peterson', 'Daniel Reichman', 'Thomas L. Griffiths', 'Stuart J. Russell']","source":"arxiv","tags":"[]","date":"2019-05-22 23:05:53+00:00","url":"http://arxiv.org/abs/1905.09397v1"},{"x":"10.06525","y":"8.495227","title":"Machine Theory of Mind","cluster":"4","author":"['Neil C. Rabinowitz', 'Frank Perbet', 'H. Francis Song', 'Chiyuan Zhang', 'S. M. Ali Eslami', 'Matthew Botvinick']","source":"arxiv","tags":"[]","date":"2018-02-21 19:00:10+00:00","url":"http://arxiv.org/abs/1802.07740v2"},{"x":"11.346198","y":"8.212283","title":"From Optimizing Engagement to Measuring Value","cluster":"1","author":"['Smitha Milli', 'Luca Belli', 'Moritz Hardt']","source":"alignment newsletter","tags":"[]","date":"2020-08-21 03:10:45+00:00","url":"http://arxiv.org/abs/2008.12623v2"},{"x":"11.12935","y":"6.8711715","title":"Preventing Imitation Learning with Adversarial Policy Ensembles","cluster":"0","author":"['Albert Zhan', 'Stas Tiomkin', 'Pieter Abbeel']","source":"arxiv","tags":"[]","date":"2020-01-31 01:57:16+00:00","url":"http://arxiv.org/abs/2002.01059v2"},{"x":"8.245373","y":"5.536011","title":"Hybrid Models with Deep and Invertible Features","cluster":"2","author":"['Eric Nalisnick', 'Akihiro Matsukawa', 'Yee Whye Teh', 'Dilan Gorur', 'Balaji Lakshminarayanan']","source":"arxiv","tags":"[]","date":"2019-02-07 18:49:47+00:00","url":"http://arxiv.org/abs/1902.02767v2"},{"x":"11.274427","y":"5.846036","title":"Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning","cluster":"0","author":"['Benjamin Eysenbach', 'Shixiang Gu', 'Julian Ibarz', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2017-11-18 00:53:20+00:00","url":"http://arxiv.org/abs/1711.06782v1"},{"x":"7.8303633","y":"6.853333","title":"Program Synthesis with Large Language Models","cluster":"2","author":"['Jacob Austin', 'Augustus Odena', 'Maxwell Nye', 'Maarten Bosma', 'Henryk Michalewski', 'David Dohan', 'Ellen Jiang', 'Carrie Cai', 'Michael Terry', 'Quoc Le', 'Charles Sutton']","source":"alignment newsletter","tags":"[]","date":"2021-08-16 03:57:30+00:00","url":"http://arxiv.org/abs/2108.07732v1"},{"x":"10.740748","y":"5.7370076","title":"Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation","cluster":"0","author":"['Lin Guan', 'Mudit Verma', 'Sihang Guo', 'Ruohan Zhang', 'Subbarao Kambhampati']","source":"arxiv","tags":"[]","date":"2020-06-26 05:40:05+00:00","url":"http://arxiv.org/abs/2006.14804v5"},{"x":"8.410984","y":"5.980115","title":"Weight Agnostic Neural Networks","cluster":"2","author":"['Adam Gaier', 'David Ha']","source":"arxiv","tags":"[]","date":"2019-06-11 02:40:11+00:00","url":"http://arxiv.org/abs/1906.04358v2"},{"x":"11.371841","y":"5.272421","title":"Interpretable Latent Spaces for Learning from Demonstration","cluster":"0","author":"['Yordan Hristov', 'Alex Lascarides', 'Subramanian Ramamoorthy']","source":"arxiv","tags":"[]","date":"2018-07-17 17:56:09+00:00","url":"http://arxiv.org/abs/1807.06583v2"},{"x":"7.905546","y":"5.160532","title":"Robust fine-tuning of zero-shot models","cluster":"2","author":"['Mitchell Wortsman', 'Gabriel Ilharco', 'Jong Wook Kim', 'Mike Li', 'Simon Kornblith', 'Rebecca Roelofs', 'Raphael Gontijo Lopes', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Hongseok Namkoong', 'Ludwig Schmidt']","source":"arxiv","tags":"[]","date":"2021-09-04 17:11:28+00:00","url":"http://arxiv.org/abs/2109.01903v2"},{"x":"8.465327","y":"5.618832","title":"Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift","cluster":"2","author":"['Yaniv Ovadia', 'Emily Fertig', 'Jie Ren', 'Zachary Nado', 'D Sculley', 'Sebastian Nowozin', 'Joshua V. Dillon', 'Balaji Lakshminarayanan', 'Jasper Snoek']","source":"arxiv","tags":"[]","date":"2019-06-06 11:42:53+00:00","url":"http://arxiv.org/abs/1906.02530v2"},{"x":"7.4666696","y":"6.951064","title":"Release Strategies and the Social Impacts of Language Models","cluster":"2","author":"['Irene Solaiman', 'Miles Brundage', 'Jack Clark', 'Amanda Askell', 'Ariel Herbert-Voss', 'Jeff Wu', 'Alec Radford', 'Gretchen Krueger', 'Jong Wook Kim', 'Sarah Kreps', 'Miles McCain', 'Alex Newhouse', 'Jason Blazakis', 'Kris McGuffie', 'Jasmine Wang']","source":"arxiv","tags":"[]","date":"2019-08-24 20:41:40+00:00","url":"http://arxiv.org/abs/1908.09203v2"},{"x":"8.645647","y":"5.2773423","title":"Security and Privacy Issues in Deep Learning","cluster":"2","author":"['Ho Bae', 'Jaehee Jang', 'Dahuin Jung', 'Hyemi Jang', 'Heonseok Ha', 'Hyungyu Lee', 'Sungroh Yoon']","source":"arxiv","tags":"[]","date":"2018-07-31 04:18:26+00:00","url":"http://arxiv.org/abs/1807.11655v4"},{"x":"8.38941","y":"5.592227","title":"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles","cluster":"2","author":"['Balaji Lakshminarayanan', 'Alexander Pritzel', 'Charles Blundell']","source":"arxiv","tags":"[]","date":"2016-12-05 18:54:43+00:00","url":"http://arxiv.org/abs/1612.01474v3"},{"x":"8.330114","y":"5.528168","title":"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples","cluster":"2","author":"['Eleni Triantafillou', 'Tyler Zhu', 'Vincent Dumoulin', 'Pascal Lamblin', 'Utku Evci', 'Kelvin Xu', 'Ross Goroshin', 'Carles Gelada', 'Kevin Swersky', 'Pierre-Antoine Manzagol', 'Hugo Larochelle']","source":"arxiv","tags":"[]","date":"2019-03-07 18:48:55+00:00","url":"http://arxiv.org/abs/1903.03096v4"},{"x":"9.350029","y":"5.9450064","title":"Meta-learners' learning dynamics are unlike learners'","cluster":"0","author":"['Neil C. Rabinowitz']","source":"alignment newsletter","tags":"[]","date":"2019-05-03 18:00:26+00:00","url":"http://arxiv.org/abs/1905.01320v1"},{"x":"10.838738","y":"6.4477654","title":"CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning","cluster":"0","author":"['Jiachen Yang', 'Alireza Nakhaei', 'David Isele', 'Kikuo Fujimura', 'Hongyuan Zha']","source":"arxiv","tags":"[]","date":"2018-09-13 21:46:54+00:00","url":"http://arxiv.org/abs/1809.05188v3"},{"x":"8.042762","y":"9.296358","title":"Engines of Power: Electricity, AI, and General-Purpose Military Transformations","cluster":"3","author":"['Jeffrey Ding', 'Allan Dafoe']","source":"arxiv","tags":"[]","date":"2021-06-08 13:55:19+00:00","url":"http://arxiv.org/abs/2106.04338v1"},{"x":"8.124382","y":"5.648424","title":"Improving neural networks by preventing co-adaptation of feature detectors","cluster":"2","author":"['Geoffrey E. Hinton', 'Nitish Srivastava', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan R. Salakhutdinov']","source":"arxiv","tags":"[]","date":"2012-07-03 06:35:15+00:00","url":"http://arxiv.org/abs/1207.0580v1"},{"x":"7.6249614","y":"6.3099627","title":"Attention Is All You Need","cluster":"2","author":"['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'Illia Polosukhin']","source":"arxiv","tags":"[]","date":"2017-06-12 17:57:34+00:00","url":"http://arxiv.org/abs/1706.03762v5"},{"x":"8.225508","y":"5.3471828","title":"Large Scale Adversarial Representation Learning","cluster":"2","author":"['Jeff Donahue', 'Karen Simonyan']","source":"alignment newsletter","tags":"[]","date":"2019-07-04 18:00:17+00:00","url":"http://arxiv.org/abs/1907.02544v2"},{"x":"11.387455","y":"7.3748207","title":"Occam's razor is insufficient to infer the preferences of irrational agents","cluster":"0","author":"['Stuart Armstrong', 'Sören Mindermann']","source":"arxiv","tags":"[]","date":"2017-12-15 19:05:01+00:00","url":"http://arxiv.org/abs/1712.05812v6"},{"x":"10.788423","y":"9.8429165","title":"Dissolving the Fermi Paradox","cluster":"1","author":"['Anders Sandberg', 'Eric Drexler', 'Toby Ord']","source":"arxiv","tags":"[]","date":"2018-06-06 19:51:21+00:00","url":"http://arxiv.org/abs/1806.02404v1"},{"x":"11.048939","y":"6.2690477","title":"Exploring Restart Distributions","cluster":"0","author":"['Arash Tavakoli', 'Vitaly Levdik', 'Riashat Islam', 'Christopher M. Smith', 'Petar Kormushev']","source":"arxiv","tags":"[]","date":"2018-11-27 22:40:01+00:00","url":"http://arxiv.org/abs/1811.11298v3"},{"x":"10.540503","y":"5.96909","title":"Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning","cluster":"0","author":"['Ilya Kostrikov', 'Kumar Krishna Agrawal', 'Debidatta Dwibedi', 'Sergey Levine', 'Jonathan Tompson']","source":"alignment newsletter","tags":"[]","date":"2018-09-09 05:37:25+00:00","url":"http://arxiv.org/abs/1809.02925v2"},{"x":"8.428594","y":"4.9030905","title":"Quantifying Perceptual Distortion of Adversarial Examples","cluster":"2","author":"['Matt Jordan', 'Naren Manoj', 'Surbhi Goel', 'Alexandros G. Dimakis']","source":"alignment newsletter","tags":"[]","date":"2019-02-21 21:02:58+00:00","url":"http://arxiv.org/abs/1902.08265v1"},{"x":"10.330848","y":"6.102966","title":"Adversarial Policies: Attacking Deep Reinforcement Learning","cluster":"0","author":"['Adam Gleave', 'Michael Dennis', 'Cody Wild', 'Neel Kant', 'Sergey Levine', 'Stuart Russell']","source":"alignment newsletter","tags":"[]","date":"2019-05-25 15:23:19+00:00","url":"http://arxiv.org/abs/1905.10615v3"},{"x":"8.141939","y":"4.969512","title":"Certified Patch Robustness via Smoothed Vision Transformers","cluster":"2","author":"['Hadi Salman', 'Saachi Jain', 'Eric Wong', 'Aleksander Mądry']","source":"arxiv","tags":"[]","date":"2021-10-11 17:44:05+00:00","url":"http://arxiv.org/abs/2110.07719v1"},{"x":"11.127548","y":"6.1449857","title":"Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives","cluster":"0","author":"['Anirudh Goyal', 'Shagun Sodhani', 'Jonathan Binas', 'Xue Bin Peng', 'Sergey Levine', 'Yoshua Bengio']","source":"alignment newsletter","tags":"[]","date":"2019-06-25 17:04:48+00:00","url":"http://arxiv.org/abs/1906.10667v1"},{"x":"8.025939","y":"5.0334377","title":"MEMO: Test Time Robustness via Adaptation and Augmentation","cluster":"2","author":"['Marvin Zhang', 'Sergey Levine', 'Chelsea Finn']","source":"arxiv","tags":"[]","date":"2021-10-18 17:55:11+00:00","url":"http://arxiv.org/abs/2110.09506v2"},{"x":"11.536646","y":"5.8409576","title":"A Framework and Method for Online Inverse Reinforcement Learning","cluster":"0","author":"['Saurabh Arora', 'Prashant Doshi', 'Bikramjit Banerjee']","source":"alignment newsletter","tags":"[]","date":"2018-05-21 02:27:58+00:00","url":"http://arxiv.org/abs/1805.07871v1"},{"x":"11.544712","y":"6.162212","title":"The Assistive Multi-Armed Bandit","cluster":"0","author":"['Lawrence Chan', 'Dylan Hadfield-Menell', 'Siddhartha Srinivasa', 'Anca Dragan']","source":"arxiv","tags":"[]","date":"2019-01-24 21:52:01+00:00","url":"http://arxiv.org/abs/1901.08654v1"},{"x":"7.734061","y":"5.780152","title":"Robust Self-Supervised Audio-Visual Speech Recognition","cluster":"2","author":"['Bowen Shi', 'Wei-Ning Hsu', 'Abdelrahman Mohamed']","source":"arxiv","tags":"[]","date":"2022-01-05 18:50:50+00:00","url":"http://arxiv.org/abs/2201.01763v1"},{"x":"11.554949","y":"6.0642138","title":"Cooperative Inverse Reinforcement Learning","cluster":"0","author":"['Dylan Hadfield-Menell', 'Anca Dragan', 'Pieter Abbeel', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2016-06-09 22:39:54+00:00","url":"http://arxiv.org/abs/1606.03137v3"},{"x":"11.713432","y":"5.896414","title":"I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set","cluster":"0","author":"['Ananth Jonnavittula', 'Dylan P. Losey']","source":"alignment newsletter","tags":"[]","date":"2020-11-11 23:32:13+00:00","url":"http://arxiv.org/abs/2011.06118v2"},{"x":"11.540577","y":"5.4598103","title":"Using Causal Analysis to Learn Specifications from Task Demonstrations","cluster":"0","author":"['Daniel Angelov', 'Yordan Hristov', 'Subramanian Ramamoorthy']","source":"arxiv","tags":"[]","date":"2019-03-04 14:26:13+00:00","url":"http://arxiv.org/abs/1903.01267v1"},{"x":"7.9277816","y":"5.7156253","title":"Explaining Neural Scaling Laws","cluster":"2","author":"['Yasaman Bahri', 'Ethan Dyer', 'Jared Kaplan', 'Jaehoon Lee', 'Utkarsh Sharma']","source":"arxiv","tags":"[]","date":"2021-02-12 18:57:46+00:00","url":"http://arxiv.org/abs/2102.06701v1"},{"x":"11.049066","y":"6.6005354","title":"Learning Rewards from Linguistic Feedback","cluster":"0","author":"['Theodore R. Sumers', 'Mark K. Ho', 'Robert D. Hawkins', 'Karthik Narasimhan', 'Thomas L. Griffiths']","source":"alignment newsletter","tags":"[]","date":"2020-09-30 14:51:00+00:00","url":"http://arxiv.org/abs/2009.14715v3"},{"x":"11.626919","y":"7.738223","title":"Hidden Incentives for Auto-Induced Distributional Shift","cluster":"1","author":"['David Krueger', 'Tegan Maharaj', 'Jan Leike']","source":"arxiv","tags":"[]","date":"2020-09-19 03:31:27+00:00","url":"http://arxiv.org/abs/2009.09153v1"},{"x":"8.275537","y":"4.9169226","title":"Benchmarking Neural Network Robustness to Common Corruptions and Perturbations","cluster":"2","author":"['Dan Hendrycks', 'Thomas Dietterich']","source":"arxiv","tags":"[]","date":"2019-03-28 20:56:37+00:00","url":"http://arxiv.org/abs/1903.12261v1"},{"x":"10.90128","y":"8.339426","title":"Reinforcement Learning Under Moral Uncertainty","cluster":"4","author":"['Adrien Ecoffet', 'Joel Lehman']","source":"alignment newsletter","tags":"[]","date":"2020-06-08 16:40:12+00:00","url":"http://arxiv.org/abs/2006.04734v3"},{"x":"8.278113","y":"4.962577","title":"Adversarial Logit Pairing","cluster":"2","author":"['Harini Kannan', 'Alexey Kurakin', 'Ian Goodfellow']","source":"arxiv","tags":"[]","date":"2018-03-16 19:03:45+00:00","url":"http://arxiv.org/abs/1803.06373v1"},{"x":"7.8175254","y":"7.005264","title":"Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning","cluster":"2","author":"['Aviv Ovadya', 'Jess Whittlestone']","source":"arxiv","tags":"[]","date":"2019-07-25 18:51:45+00:00","url":"http://arxiv.org/abs/1907.11274v2"},{"x":"11.418325","y":"6.3935423","title":"Pitfalls of learning a reward function online","cluster":"0","author":"['Stuart Armstrong', 'Jan Leike', 'Laurent Orseau', 'Shane Legg']","source":"alignment newsletter","tags":"[]","date":"2020-04-28 16:58:58+00:00","url":"http://arxiv.org/abs/2004.13654v1"},{"x":"11.452907","y":"6.0954323","title":"Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations","cluster":"0","author":"['Daniel S. Brown', 'Wonjoon Goo', 'Scott Niekum']","source":"alignment newsletter","tags":"[]","date":"2019-07-09 04:11:53+00:00","url":"http://arxiv.org/abs/1907.03976v3"},{"x":"10.8442545","y":"5.7046576","title":"An Extensible Interactive Interface for Agent Design","cluster":"0","author":"['Matthew Rahtz', 'James Fang', 'Anca D. Dragan', 'Dylan Hadfield-Menell']","source":"arxiv","tags":"[]","date":"2019-06-06 15:18:40+00:00","url":"http://arxiv.org/abs/1906.02641v3"},{"x":"8.317243","y":"8.409742","title":"When Will AI Exceed Human Performance? Evidence from AI Experts","cluster":"3","author":"['Katja Grace', 'John Salvatier', 'Allan Dafoe', 'Baobao Zhang', 'Owain Evans']","source":"arxiv","tags":"[]","date":"2017-05-24 15:00:20+00:00","url":"http://arxiv.org/abs/1705.08807v3"},{"x":"8.455137","y":"4.977271","title":"Testing Robustness Against Unforeseen Adversaries","cluster":"2","author":"['Daniel Kang', 'Yi Sun', 'Dan Hendrycks', 'Tom Brown', 'Jacob Steinhardt']","source":"arxiv","tags":"[]","date":"2019-08-21 17:36:48+00:00","url":"http://arxiv.org/abs/1908.08016v2"},{"x":"7.498761","y":"6.7189794","title":"Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment","cluster":"2","author":"['Di Jin', 'Zhijing Jin', 'Joey Tianyi Zhou', 'Peter Szolovits']","source":"alignment newsletter","tags":"[]","date":"2019-07-27 15:07:04+00:00","url":"http://arxiv.org/abs/1907.11932v6"},{"x":"9.860396","y":"6.033077","title":"Aligning Superhuman AI with Human Behavior: Chess as a Model System","cluster":"4","author":"['Reid McIlroy-Young', 'Siddhartha Sen', 'Jon Kleinberg', 'Ashton Anderson']","source":"alignment newsletter","tags":"[]","date":"2020-06-02 18:12:52+00:00","url":"http://arxiv.org/abs/2006.01855v3"},{"x":"11.505589","y":"7.400944","title":"Optimal Policies Tend to Seek Power","cluster":"0","author":"['Alexander Matt Turner', 'Logan Smith', 'Rohin Shah', 'Andrew Critch', 'Prasad Tadepalli']","source":"arxiv","tags":"[]","date":"2019-12-03 20:45:49+00:00","url":"http://arxiv.org/abs/1912.01683v9"},{"x":"8.333471","y":"5.6729755","title":"Meta-Learning with Latent Embedding Optimization","cluster":"2","author":"['Andrei A. Rusu', 'Dushyant Rao', 'Jakub Sygnowski', 'Oriol Vinyals', 'Razvan Pascanu', 'Simon Osindero', 'Raia Hadsell']","source":"arxiv","tags":"[]","date":"2018-07-16 16:35:29+00:00","url":"http://arxiv.org/abs/1807.05960v3"},{"x":"11.66376","y":"5.8896065","title":"Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization","cluster":"0","author":"['Sreejith Balakrishnan', 'Quoc Phong Nguyen', 'Bryan Kian Hsiang Low', 'Harold Soh']","source":"arxiv","tags":"[]","date":"2020-11-17 10:17:45+00:00","url":"http://arxiv.org/abs/2011.08541v1"},{"x":"11.403547","y":"5.8044066","title":"PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training","cluster":"0","author":"['Kimin Lee', 'Laura Smith', 'Pieter Abbeel']","source":"arxiv","tags":"[]","date":"2021-06-09 14:10:50+00:00","url":"http://arxiv.org/abs/2106.05091v1"},{"x":"11.449242","y":"5.533877","title":"Shared Autonomy via Deep Reinforcement Learning","cluster":"0","author":"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2018-02-06 00:45:12+00:00","url":"http://arxiv.org/abs/1802.01744v2"},{"x":"9.546524","y":"9.250998","title":"Legible Normativity for AI Alignment: The Value of Silly Rules","cluster":"4","author":"['Dylan Hadfield-Menell', 'McKane Andrus', 'Gillian K. Hadfield']","source":"alignment newsletter","tags":"[]","date":"2018-11-03 19:09:18+00:00","url":"http://arxiv.org/abs/1811.01267v1"},{"x":"8.43019","y":"5.0068717","title":"Adversarial Examples Are Not Bugs, They Are Features","cluster":"2","author":"['Andrew Ilyas', 'Shibani Santurkar', 'Dimitris Tsipras', 'Logan Engstrom', 'Brandon Tran', 'Aleksander Madry']","source":"alignment newsletter","tags":"[]","date":"2019-05-06 17:45:05+00:00","url":"http://arxiv.org/abs/1905.02175v4"},{"x":"10.781464","y":"6.291599","title":"Supervising strong learners by amplifying weak experts","cluster":"0","author":"['Paul Christiano', 'Buck Shlegeris', 'Dario Amodei']","source":"arxiv","tags":"[]","date":"2018-10-19 16:30:48+00:00","url":"http://arxiv.org/abs/1810.08575v1"},{"x":"11.222093","y":"6.009174","title":"Decision Transformer: Reinforcement Learning via Sequence Modeling","cluster":"0","author":"['Lili Chen', 'Kevin Lu', 'Aravind Rajeswaran', 'Kimin Lee', 'Aditya Grover', 'Michael Laskin', 'Pieter Abbeel', 'Aravind Srinivas', 'Igor Mordatch']","source":"alignment newsletter","tags":"[]","date":"2021-06-02 17:53:39+00:00","url":"http://arxiv.org/abs/2106.01345v2"},{"x":"11.393767","y":"5.7611384","title":"Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning","cluster":"0","author":"['Smitha Milli', 'Anca D. Dragan']","source":"alignment newsletter","tags":"[]","date":"2019-03-09 21:58:46+00:00","url":"http://arxiv.org/abs/1903.03877v2"},{"x":"11.891315","y":"10.6065235","title":"Probabilistic Dependency Graphs","cluster":"1","author":"['Oliver Richardson', 'Joseph Y Halpern']","source":"arxiv","tags":"[]","date":"2020-12-19 22:29:49+00:00","url":"http://arxiv.org/abs/2012.10800v1"},{"x":"10.651018","y":"9.634135","title":"Embedded Agency","cluster":"1","author":"['Abram Demski', 'Scott Garrabrant']","source":"arxiv","tags":"[]","date":"2019-02-25 17:38:48+00:00","url":"http://arxiv.org/abs/1902.09469v3"},{"x":"8.076341","y":"5.134511","title":"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty","cluster":"2","author":"['Dan Hendrycks', 'Norman Mu', 'Ekin D. Cubuk', 'Barret Zoph', 'Justin Gilmer', 'Balaji Lakshminarayanan']","source":"arxiv","tags":"[]","date":"2019-12-05 18:18:10+00:00","url":"http://arxiv.org/abs/1912.02781v2"},{"x":"8.279896","y":"5.510134","title":"Estimating Training Data Influence by Tracing Gradient Descent","cluster":"2","author":"['Garima Pruthi', 'Frederick Liu', 'Mukund Sundararajan', 'Satyen Kale']","source":"alignment newsletter","tags":"[]","date":"2020-02-19 22:40:32+00:00","url":"http://arxiv.org/abs/2002.08484v3"},{"x":"8.359066","y":"4.985478","title":"Certified Adversarial Robustness via Randomized Smoothing","cluster":"2","author":"['Jeremy M Cohen', 'Elan Rosenfeld', 'J. Zico Kolter']","source":"arxiv","tags":"[]","date":"2019-02-08 02:08:19+00:00","url":"http://arxiv.org/abs/1902.02918v2"},{"x":"11.270931","y":"6.884075","title":"Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent","cluster":"0","author":"['Michael K. Cohen', 'Elliot Catt', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2020-06-05 10:42:29+00:00","url":"http://arxiv.org/abs/2006.03357v2"},{"x":"11.036462","y":"6.1964474","title":"Towards Safe Reinforcement Learning with a Safety Editor Policy","cluster":"0","author":"['Haonan Yu', 'Wei Xu', 'Haichao Zhang']","source":"arxiv","tags":"[]","date":"2022-01-28 21:32:59+00:00","url":"http://arxiv.org/abs/2201.12427v1"},{"x":"10.200818","y":"6.410884","title":"Open-Ended Learning Leads to Generally Capable Agents","cluster":"0","author":"['Open Ended Learning Team', 'Adam Stooke', 'Anuj Mahajan', 'Catarina Barros', 'Charlie Deck', 'Jakob Bauer', 'Jakub Sygnowski', 'Maja Trebacz', 'Max Jaderberg', 'Michael Mathieu', 'Nat McAleese', 'Nathalie Bradley-Schmieg', 'Nathaniel Wong', 'Nicolas Porcel', 'Roberta Raileanu', 'Steph Hughes-Fitt', 'Valentin Dalibard', 'Wojciech Marian Czarnecki']","source":"arxiv","tags":"[]","date":"2021-07-27 13:30:07+00:00","url":"http://arxiv.org/abs/2107.12808v2"},{"x":"11.759443","y":"6.986987","title":"Online Bayesian Goal Inference for Boundedly-Rational Planning Agents","cluster":"0","author":"['Tan Zhi-Xuan', 'Jordyn L. Mann', 'Tom Silver', 'Joshua B. Tenenbaum', 'Vikash K. Mansinghka']","source":"alignment newsletter","tags":"[]","date":"2020-06-13 01:48:10+00:00","url":"http://arxiv.org/abs/2006.07532v2"},{"x":"8.906287","y":"7.355615","title":"Rissanen Data Analysis: Examining Dataset Characteristics via Description Length","cluster":"2","author":"['Ethan Perez', 'Douwe Kiela', 'Kyunghyun Cho']","source":"alignment newsletter","tags":"[]","date":"2021-03-05 18:58:32+00:00","url":"http://arxiv.org/abs/2103.03872v1"},{"x":"7.7043614","y":"6.9678307","title":"On the Opportunities and Risks of Foundation Models","cluster":"2","author":"['Rishi Bommasani', 'Drew A. Hudson', 'Ehsan Adeli', 'Russ Altman', 'Simran Arora', 'Sydney von Arx', 'Michael S. Bernstein', 'Jeannette Bohg', 'Antoine Bosselut', 'Emma Brunskill', 'Erik Brynjolfsson', 'Shyamal Buch', 'Dallas Card', 'Rodrigo Castellon', 'Niladri Chatterji', 'Annie Chen', 'Kathleen Creel', 'Jared Quincy Davis', 'Dora Demszky', 'Chris Donahue', 'Moussa Doumbouya', 'Esin Durmus', 'Stefano Ermon', 'John Etchemendy', 'Kawin Ethayarajh', 'Li Fei-Fei', 'Chelsea Finn', 'Trevor Gale', 'Lauren Gillespie', 'Karan Goel', 'Noah Goodman', 'Shelby Grossman', 'Neel Guha', 'Tatsunori Hashimoto', 'Peter Henderson', 'John Hewitt', 'Daniel E. Ho', 'Jenny Hong', 'Kyle Hsu', 'Jing Huang', 'Thomas Icard', 'Saahil Jain', 'Dan Jurafsky', 'Pratyusha Kalluri', 'Siddharth Karamcheti', 'Geoff Keeling', 'Fereshte Khani', 'Omar Khattab', 'Pang Wei Koh', 'Mark Krass', 'Ranjay Krishna', 'Rohith Kuditipudi', 'Ananya Kumar', 'Faisal Ladhak', 'Mina Lee', 'Tony Lee', 'Jure Leskovec', 'Isabelle Levent', 'Xiang Lisa Li', 'Xuechen Li', 'Tengyu Ma', 'Ali Malik', 'Christopher D. Manning', 'Suvir Mirchandani', 'Eric Mitchell', 'Zanele Munyikwa', 'Suraj Nair', 'Avanika Narayan', 'Deepak Narayanan', 'Ben Newman', 'Allen Nie', 'Juan Carlos Niebles', 'Hamed Nilforoshan', 'Julian Nyarko', 'Giray Ogut', 'Laurel Orr', 'Isabel Papadimitriou', 'Joon Sung Park', 'Chris Piech', 'Eva Portelance', 'Christopher Potts', 'Aditi Raghunathan', 'Rob Reich', 'Hongyu Ren', 'Frieda Rong', 'Yusuf Roohani', 'Camilo Ruiz', 'Jack Ryan', 'Christopher Ré', 'Dorsa Sadigh', 'Shiori Sagawa', 'Keshav Santhanam', 'Andy Shih', 'Krishnan Srinivasan', 'Alex Tamkin', 'Rohan Taori', 'Armin W. Thomas', 'Florian Tramèr', 'Rose E. Wang', 'William Wang', 'Bohan Wu', 'Jiajun Wu', 'Yuhuai Wu', 'Sang Michael Xie', 'Michihiro Yasunaga', 'Jiaxuan You', 'Matei Zaharia', 'Michael Zhang', 'Tianyi Zhang', 'Xikun Zhang', 'Yuhui Zhang', 'Lucia Zheng', 'Kaitlyn Zhou', 'Percy Liang']","source":"alignment newsletter","tags":"[]","date":"2021-08-16 17:50:08+00:00","url":"http://arxiv.org/abs/2108.07258v2"},{"x":"7.842765","y":"5.179828","title":"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness","cluster":"2","author":"['Robert Geirhos', 'Patricia Rubisch', 'Claudio Michaelis', 'Matthias Bethge', 'Felix A. Wichmann', 'Wieland Brendel']","source":"alignment newsletter","tags":"[]","date":"2018-11-29 15:04:05+00:00","url":"http://arxiv.org/abs/1811.12231v2"},{"x":"8.230407","y":"9.239093","title":"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","cluster":"3","author":"['Miles Brundage', 'Shahar Avin', 'Jack Clark', 'Helen Toner', 'Peter Eckersley', 'Ben Garfinkel', 'Allan Dafoe', 'Paul Scharre', 'Thomas Zeitzoff', 'Bobby Filar', 'Hyrum Anderson', 'Heather Roff', 'Gregory C. Allen', 'Jacob Steinhardt', 'Carrick Flynn', 'Seán Ó hÉigeartaigh', 'Simon Beard', 'Haydn Belfield', 'Sebastian Farquhar', 'Clare Lyle', 'Rebecca Crootof', 'Owain Evans', 'Michael Page', 'Joanna Bryson', 'Roman Yampolskiy', 'Dario Amodei']","source":"arxiv","tags":"[]","date":"2018-02-20 18:07:50+00:00","url":"http://arxiv.org/abs/1802.07228v1"},{"x":"8.190665","y":"6.20397","title":"A Comprehensive Survey on Graph Neural Networks","cluster":"2","author":"['Zonghan Wu', 'Shirui Pan', 'Fengwen Chen', 'Guodong Long', 'Chengqi Zhang', 'Philip S. Yu']","source":"arxiv","tags":"[]","date":"2019-01-03 03:20:55+00:00","url":"http://arxiv.org/abs/1901.00596v4"},{"x":"8.226056","y":"9.178167","title":"Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers","cluster":"3","author":"['Baobao Zhang', 'Markus Anderljung', 'Lauren Kahn', 'Noemi Dreksler', 'Michael C. Horowitz', 'Allan Dafoe']","source":"arxiv","tags":"[]","date":"2021-05-05 15:23:12+00:00","url":"http://arxiv.org/abs/2105.02117v1"},{"x":"7.784011","y":"6.9550996","title":"Measuring Coding Challenge Competence With APPS","cluster":"2","author":"['Dan Hendrycks', 'Steven Basart', 'Saurav Kadavath', 'Mantas Mazeika', 'Akul Arora', 'Ethan Guo', 'Collin Burns', 'Samir Puranik', 'Horace He', 'Dawn Song', 'Jacob Steinhardt']","source":"alignment newsletter","tags":"[]","date":"2021-05-20 17:58:42+00:00","url":"http://arxiv.org/abs/2105.09938v3"},{"x":"7.750063","y":"6.329819","title":"DARTS: Differentiable Architecture Search","cluster":"2","author":"['Hanxiao Liu', 'Karen Simonyan', 'Yiming Yang']","source":"arxiv","tags":"[]","date":"2018-06-24 00:06:13+00:00","url":"http://arxiv.org/abs/1806.09055v2"},{"x":"10.72219","y":"5.988473","title":"Amplifying the Imitation Effect for Reinforcement Learning of UCAV's Mission Execution","cluster":"0","author":"['Gyeong Taek Lee', 'Chang Ouk Kim']","source":"arxiv","tags":"[]","date":"2019-01-17 15:47:12+00:00","url":"http://arxiv.org/abs/1901.05856v1"},{"x":"10.678083","y":"7.6315103","title":"Performance of Bounded-Rational Agents With the Ability to Self-Modify","cluster":"4","author":"['Jakub Tětek', 'Marek Sklenka', 'Tomáš Gavenčiak']","source":"arxiv","tags":"[]","date":"2020-11-12 09:25:08+00:00","url":"http://arxiv.org/abs/2011.06275v2"},{"x":"8.145218","y":"4.9665413","title":"Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation","cluster":"2","author":"['Victor Besnier', 'Andrei Bursuc', 'David Picard', 'Alexandre Briot']","source":"arxiv","tags":"[]","date":"2021-08-03 17:09:56+00:00","url":"http://arxiv.org/abs/2108.01634v1"},{"x":"10.065327","y":"6.5244865","title":"Solving the Rubik's Cube Without Human Knowledge","cluster":"0","author":"['Stephen McAleer', 'Forest Agostinelli', 'Alexander Shmakov', 'Pierre Baldi']","source":"alignment newsletter","tags":"[]","date":"2018-05-18 23:07:31+00:00","url":"http://arxiv.org/abs/1805.07470v1"},{"x":"12.725307","y":"10.600843","title":"Verification of deep probabilistic models","cluster":"2","author":"['Krishnamurthy Dvijotham', 'Marta Garnelo', 'Alhussein Fawzi', 'Pushmeet Kohli']","source":"arxiv","tags":"[]","date":"2018-12-06 20:38:19+00:00","url":"http://arxiv.org/abs/1812.02795v1"},{"x":"7.837333","y":"5.26703","title":"Discrete Representations Strengthen Vision Transformer Robustness","cluster":"2","author":"['Chengzhi Mao', 'Lu Jiang', 'Mostafa Dehghani', 'Carl Vondrick', 'Rahul Sukthankar', 'Irfan Essa']","source":"arxiv","tags":"[]","date":"2021-11-20 01:49:56+00:00","url":"http://arxiv.org/abs/2111.10493v1"},{"x":"8.172834","y":"9.268992","title":"The Transformative Potential of Artificial Intelligence","cluster":"3","author":"['Ross Gruetzemacher', 'Jess Whittlestone']","source":"alignment newsletter","tags":"[]","date":"2019-11-27 09:37:58+00:00","url":"http://arxiv.org/abs/1912.00747v3"},{"x":"8.5797205","y":"9.022313","title":"Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance","cluster":"3","author":"['Andrew J. Lohn']","source":"alignment newsletter","tags":"[]","date":"2020-09-02 03:33:40+00:00","url":"http://arxiv.org/abs/2009.00802v1"},{"x":"8.113984","y":"9.575331","title":"Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society","cluster":"3","author":"['Carina Prunkl', 'Jess Whittlestone']","source":"arxiv","tags":"[]","date":"2020-01-13 15:22:42+00:00","url":"http://arxiv.org/abs/2001.04335v2"},{"x":"10.334185","y":"6.9114466","title":"On the Utility of Learning about Humans for Human-AI Coordination","cluster":"4","author":"['Micah Carroll', 'Rohin Shah', 'Mark K. Ho', 'Thomas L. Griffiths', 'Sanjit A. Seshia', 'Pieter Abbeel', 'Anca Dragan']","source":"arxiv","tags":"[]","date":"2019-10-13 17:17:52+00:00","url":"http://arxiv.org/abs/1910.05789v2"},{"x":"11.404758","y":"6.2012477","title":"Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification","cluster":"0","author":"['Benjamin Eysenbach', 'Sergey Levine', 'Ruslan Salakhutdinov']","source":"arxiv","tags":"[]","date":"2021-03-23 16:19:55+00:00","url":"http://arxiv.org/abs/2103.12656v2"},{"x":"10.981155","y":"5.530341","title":"Imitating Latent Policies from Observation","cluster":"0","author":"['Ashley D. Edwards', 'Himanshu Sahni', 'Yannick Schroecker', 'Charles L. Isbell']","source":"alignment newsletter","tags":"[]","date":"2018-05-21 06:49:57+00:00","url":"http://arxiv.org/abs/1805.07914v3"},{"x":"8.5651455","y":"5.2475457","title":"Adversarial Attacks Against Medical Deep Learning Systems","cluster":"2","author":"['Samuel G. Finlayson', 'Hyung Won Chung', 'Isaac S. Kohane', 'Andrew L. Beam']","source":"arxiv","tags":"[]","date":"2018-04-15 02:33:08+00:00","url":"http://arxiv.org/abs/1804.05296v3"},{"x":"11.638435","y":"5.8404803","title":"Learning What Information to Give in Partially Observed Domains","cluster":"0","author":"['Rohan Chitnis', 'Leslie Pack Kaelbling', 'Tomás Lozano-Pérez']","source":"arxiv","tags":"[]","date":"2018-05-21 19:16:02+00:00","url":"http://arxiv.org/abs/1805.08263v4"},{"x":"11.6561775","y":"5.601866","title":"Simplifying Reward Design through Divide-and-Conquer","cluster":"0","author":"['Ellis Ratner', 'Dylan Hadfield-Menell', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2018-06-07 03:49:05+00:00","url":"http://arxiv.org/abs/1806.02501v1"},{"x":"7.7236333","y":"7.15098","title":"Red Teaming Language Models with Language Models","cluster":"2","author":"['Ethan Perez', 'Saffron Huang', 'Francis Song', 'Trevor Cai', 'Roman Ring', 'John Aslanides', 'Amelia Glaese', 'Nat McAleese', 'Geoffrey Irving']","source":"arxiv","tags":"[]","date":"2022-02-07 15:22:17+00:00","url":"http://arxiv.org/abs/2202.03286v1"},{"x":"11.323841","y":"6.488088","title":"An empirical investigation of the challenges of real-world reinforcement learning","cluster":"0","author":"['Gabriel Dulac-Arnold', 'Nir Levine', 'Daniel J. Mankowitz', 'Jerry Li', 'Cosmin Paduraru', 'Sven Gowal', 'Todd Hester']","source":"arxiv","tags":"[]","date":"2020-03-24 11:05:41+00:00","url":"http://arxiv.org/abs/2003.11881v2"},{"x":"11.125087","y":"5.7764263","title":"DropoutDAgger: A Bayesian Approach to Safe Imitation Learning","cluster":"0","author":"['Kunal Menda', 'Katherine Driggs-Campbell', 'Mykel J. Kochenderfer']","source":"arxiv","tags":"[]","date":"2017-09-18 20:51:53+00:00","url":"http://arxiv.org/abs/1709.06166v1"},{"x":"11.678986","y":"6.666583","title":"Heuristic Approaches for Goal Recognition in Incomplete Domain Models","cluster":"0","author":"['Ramon Fraga Pereira', 'Felipe Meneguzzi']","source":"alignment newsletter","tags":"[]","date":"2018-04-16 20:00:41+00:00","url":"http://arxiv.org/abs/1804.05917v1"},{"x":"11.608771","y":"5.6723514","title":"Enabling Robots to Communicate their Objectives","cluster":"0","author":"['Sandy H. Huang', 'David Held', 'Pieter Abbeel', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2017-02-11 22:39:39+00:00","url":"http://arxiv.org/abs/1702.03465v2"},{"x":"10.750234","y":"5.577734","title":"Learning a Behavioral Repertoire from Demonstrations","cluster":"0","author":"['Niels Justesen', 'Miguel Gonzalez Duque', 'Daniel Cabarcas Jaramillo', 'Jean-Baptiste Mouret', 'Sebastian Risi']","source":"arxiv","tags":"[]","date":"2019-07-05 23:08:08+00:00","url":"http://arxiv.org/abs/1907.03046v1"},{"x":"7.8407655","y":"5.6272883","title":"Stand-Alone Self-Attention in Vision Models","cluster":"2","author":"['Prajit Ramachandran', 'Niki Parmar', 'Ashish Vaswani', 'Irwan Bello', 'Anselm Levskaya', 'Jonathon Shlens']","source":"alignment newsletter","tags":"[]","date":"2019-06-13 19:43:01+00:00","url":"http://arxiv.org/abs/1906.05909v1"},{"x":"8.603556","y":"8.980148","title":"Stovepiping and Malicious Software: A Critical Review of AGI Containment","cluster":"3","author":"['Jason M. Pittman', 'Jesus P. Espinoza', 'Courtney Crosby']","source":"arxiv","tags":"[]","date":"2018-11-08 19:19:53+00:00","url":"http://arxiv.org/abs/1811.03653v2"},{"x":"7.474806","y":"7.2019753","title":"Linguistic Cues of Deception in a Multilingual April Fools' Day Context","cluster":"2","author":"['Katerina Papantoniou', 'Panagiotis Papadakos', 'Giorgos Flouris', 'Dimitris Plexousakis']","source":"arxiv","tags":"[]","date":"2021-11-06 16:28:12+00:00","url":"http://arxiv.org/abs/2111.03913v3"},{"x":"11.81332","y":"5.98236","title":"Safely Probabilistically Complete Real-Time Planning and Exploration in Unknown Environments","cluster":"0","author":"['David Fridovich-Keil', 'Jaime F. Fisac', 'Claire J. Tomlin']","source":"arxiv","tags":"[]","date":"2018-11-19 17:47:01+00:00","url":"http://arxiv.org/abs/1811.07834v3"},{"x":"11.112672","y":"5.7191043","title":"Deep reinforcement learning from human preferences","cluster":"0","author":"['Paul Christiano', 'Jan Leike', 'Tom B. Brown', 'Miljan Martic', 'Shane Legg', 'Dario Amodei']","source":"arxiv","tags":"[]","date":"2017-06-12 17:23:59+00:00","url":"http://arxiv.org/abs/1706.03741v3"},{"x":"11.120425","y":"7.6957335","title":"Should Robots be Obedient?","cluster":"1","author":"['Smitha Milli', 'Dylan Hadfield-Menell', 'Anca Dragan', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2017-05-28 20:51:19+00:00","url":"http://arxiv.org/abs/1705.09990v1"},{"x":"10.423506","y":"5.6895576","title":"Thinking Fast and Slow with Deep Learning and Tree Search","cluster":"0","author":"['Thomas Anthony', 'Zheng Tian', 'David Barber']","source":"arxiv","tags":"[]","date":"2017-05-23 17:48:51+00:00","url":"http://arxiv.org/abs/1705.08439v4"},{"x":"8.405472","y":"5.796293","title":"Model Reconstruction from Model Explanations","cluster":"2","author":"['Smitha Milli', 'Ludwig Schmidt', 'Anca D. Dragan', 'Moritz Hardt']","source":"alignment newsletter","tags":"[]","date":"2018-07-13 17:15:00+00:00","url":"http://arxiv.org/abs/1807.05185v1"},{"x":"11.260498","y":"9.016316","title":"Manipulating and Measuring Model Interpretability","cluster":"1","author":"['Forough Poursabzi-Sangdeh', 'Daniel G. Goldstein', 'Jake M. Hofman', 'Jennifer Wortman Vaughan', 'Hanna Wallach']","source":"alignment newsletter","tags":"[]","date":"2018-02-21 21:11:36+00:00","url":"http://arxiv.org/abs/1802.07810v5"},{"x":"9.640115","y":"9.308847","title":"Aligning AI With Shared Human Values","cluster":"4","author":"['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andrew Critch', 'Jerry Li', 'Dawn Song', 'Jacob Steinhardt']","source":"alignment newsletter","tags":"[]","date":"2020-08-05 17:59:16+00:00","url":"http://arxiv.org/abs/2008.02275v5"},{"x":"8.468899","y":"5.8915434","title":"Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data","cluster":"2","author":"['Felipe Petroski Such', 'Aditya Rawal', 'Joel Lehman', 'Kenneth O. Stanley', 'Jeff Clune']","source":"alignment newsletter","tags":"[]","date":"2019-12-17 00:57:50+00:00","url":"http://arxiv.org/abs/1912.07768v1"},{"x":"11.400289","y":"7.236414","title":"Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games","cluster":"0","author":"['Xiaomin Lin', 'Stephen C. Adams', 'Peter A. Beling']","source":"arxiv","tags":"[]","date":"2018-06-26 05:14:13+00:00","url":"http://arxiv.org/abs/1806.09795v3"},{"x":"9.163732","y":"5.677474","title":"Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming","cluster":"2","author":"['Sumanth Dathathri', 'Krishnamurthy Dvijotham', 'Alexey Kurakin', 'Aditi Raghunathan', 'Jonathan Uesato', 'Rudy Bunel', 'Shreya Shankar', 'Jacob Steinhardt', 'Ian Goodfellow', 'Percy Liang', 'Pushmeet Kohli']","source":"alignment newsletter","tags":"[]","date":"2020-10-22 12:32:29+00:00","url":"http://arxiv.org/abs/2010.11645v2"},{"x":"10.41008","y":"5.692762","title":"Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents","cluster":"0","author":"['Christian Rupprecht', 'Cyril Ibrahim', 'Christopher J. Pal']","source":"arxiv","tags":"[]","date":"2019-04-02 10:21:23+00:00","url":"http://arxiv.org/abs/1904.01318v1"},{"x":"8.698862","y":"5.082404","title":"Motivating the Rules of the Game for Adversarial Example Research","cluster":"2","author":"['Justin Gilmer', 'Ryan P. Adams', 'Ian Goodfellow', 'David Andersen', 'George E. Dahl']","source":"arxiv","tags":"[]","date":"2018-07-18 01:17:27+00:00","url":"http://arxiv.org/abs/1807.06732v2"},{"x":"10.401902","y":"5.831216","title":"Using Natural Language for Reward Shaping in Reinforcement Learning","cluster":"0","author":"['Prasoon Goyal', 'Scott Niekum', 'Raymond J. Mooney']","source":"alignment newsletter","tags":"[]","date":"2019-03-05 19:20:35+00:00","url":"http://arxiv.org/abs/1903.02020v2"},{"x":"11.109799","y":"6.9354234","title":"Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making","cluster":"0","author":"['Andrew Critch']","source":"arxiv","tags":"[]","date":"2017-01-05 13:00:05+00:00","url":"http://arxiv.org/abs/1701.01302v3"},{"x":"11.614602","y":"6.63702","title":"Quantifying Differences in Reward Functions","cluster":"0","author":"['Adam Gleave', 'Michael Dennis', 'Shane Legg', 'Stuart Russell', 'Jan Leike']","source":"alignment newsletter","tags":"[]","date":"2020-06-24 17:35:15+00:00","url":"http://arxiv.org/abs/2006.13900v3"},{"x":"9.441268","y":"6.8531857","title":"SmartChoices: Hybridizing Programming and Machine Learning","cluster":"0","author":"['Victor Carbune', 'Thierry Coppey', 'Alexander Daryin', 'Thomas Deselaers', 'Nikhil Sarda', 'Jay Yagnik']","source":"arxiv","tags":"[]","date":"2018-10-01 11:14:22+00:00","url":"http://arxiv.org/abs/1810.00619v3"},{"x":"11.129775","y":"7.2891545","title":"Emergent Communication through Negotiation","cluster":"0","author":"['Kris Cao', 'Angeliki Lazaridou', 'Marc Lanctot', 'Joel Z Leibo', 'Karl Tuyls', 'Stephen Clark']","source":"arxiv","tags":"[]","date":"2018-04-11 13:48:08+00:00","url":"http://arxiv.org/abs/1804.03980v1"},{"x":"10.5467","y":"5.6870875","title":"Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces","cluster":"0","author":"['Garrett Warnell', 'Nicholas Waytowich', 'Vernon Lawhern', 'Peter Stone']","source":"arxiv","tags":"[]","date":"2017-09-28 20:43:40+00:00","url":"http://arxiv.org/abs/1709.10163v2"},{"x":"11.137366","y":"5.9665937","title":"Challenges of Real-World Reinforcement Learning","cluster":"0","author":"['Gabriel Dulac-Arnold', 'Daniel Mankowitz', 'Todd Hester']","source":"alignment newsletter","tags":"[]","date":"2019-04-29 18:40:15+00:00","url":"http://arxiv.org/abs/1904.12901v1"},{"x":"10.876162","y":"6.3707557","title":"Learning Invariances for Policy Generalization","cluster":"0","author":"['Remi Tachet', 'Philip Bachman', 'Harm van Seijen']","source":"arxiv","tags":"[]","date":"2018-09-07 17:32:19+00:00","url":"http://arxiv.org/abs/1809.02591v2"},{"x":"11.488505","y":"5.3247275","title":"Adversarial Active Exploration for Inverse Dynamics Model Learning","cluster":"0","author":"['Zhang-Wei Hong', 'Tsu-Jui Fu', 'Tzu-Yun Shann', 'Yi-Hsiang Chang', 'Chun-Yi Lee']","source":"arxiv","tags":"[]","date":"2018-06-26 14:33:22+00:00","url":"http://arxiv.org/abs/1806.10019v2"},{"x":"10.76153","y":"6.940337","title":"Evaluating the Robustness of Collaborative Agents","cluster":"0","author":"['Paul Knott', 'Micah Carroll', 'Sam Devlin', 'Kamil Ciosek', 'Katja Hofmann', 'A. D. Dragan', 'Rohin Shah']","source":"alignment newsletter","tags":"[]","date":"2021-01-14 09:02:45+00:00","url":"http://arxiv.org/abs/2101.05507v1"},{"x":"11.351439","y":"5.9258604","title":"EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning","cluster":"0","author":"['Kunal Menda', 'Katherine Driggs-Campbell', 'Mykel J. Kochenderfer']","source":"arxiv","tags":"[]","date":"2018-07-22 20:52:56+00:00","url":"http://arxiv.org/abs/1807.08364v3"},{"x":"10.963587","y":"6.1582384","title":"Learning to be Safe: Deep RL with a Safety Critic","cluster":"0","author":"['Krishnan Srinivasan', 'Benjamin Eysenbach', 'Sehoon Ha', 'Jie Tan', 'Chelsea Finn']","source":"alignment newsletter","tags":"[]","date":"2020-10-27 20:53:20+00:00","url":"http://arxiv.org/abs/2010.14603v1"},{"x":"9.588873","y":"9.294381","title":"Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem","cluster":"4","author":"['Pedro Fernandes', 'Francisco C. Santos', 'Manuel Lopes']","source":"arxiv","tags":"[]","date":"2019-06-26 10:18:19+00:00","url":"http://arxiv.org/abs/1907.03843v2"},{"x":"9.233416","y":"9.3598995","title":"Truthful AI: Developing and governing AI that does not lie","cluster":"3","author":"['Owain Evans', 'Owen Cotton-Barratt', 'Lukas Finnveden', 'Adam Bales', 'Avital Balwit', 'Peter Wills', 'Luca Righetti', 'William Saunders']","source":"alignment newsletter","tags":"[]","date":"2021-10-13 12:18:09+00:00","url":"http://arxiv.org/abs/2110.06674v1"},{"x":"7.7970004","y":"5.832249","title":"A Neural Scaling Law from the Dimension of the Data Manifold","cluster":"2","author":"['Utkarsh Sharma', 'Jared Kaplan']","source":"arxiv","tags":"[]","date":"2020-04-22 19:16:06+00:00","url":"http://arxiv.org/abs/2004.10802v1"},{"x":"12.290817","y":"8.23984","title":"Normative Disagreement as a Challenge for Cooperative AI","cluster":"1","author":"['Julian Stastny', 'Maxime Riché', 'Alexander Lyzhov', 'Johannes Treutlein', 'Allan Dafoe', 'Jesse Clifton']","source":"arxiv","tags":"[]","date":"2021-11-27 11:37:42+00:00","url":"http://arxiv.org/abs/2111.13872v1"},{"x":"10.31709","y":"6.9298077","title":"Learning to Understand Goal Specifications by Modelling Reward","cluster":"0","author":"['Dzmitry Bahdanau', 'Felix Hill', 'Jan Leike', 'Edward Hughes', 'Arian Hosseini', 'Pushmeet Kohli', 'Edward Grefenstette']","source":"alignment newsletter","tags":"[]","date":"2018-06-05 22:01:51+00:00","url":"http://arxiv.org/abs/1806.01946v4"},{"x":"8.578478","y":"9.264986","title":"Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments","cluster":"3","author":"['Roel Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']","source":"alignment newsletter","tags":"[]","date":"2019-11-20 16:21:12+00:00","url":"http://arxiv.org/abs/1911.09005v1"},{"x":"11.50406","y":"9.28093","title":"Functional Decision Theory: A New Theory of Instrumental Rationality","cluster":"1","author":"['Eliezer Yudkowsky', 'Nate Soares']","source":"arxiv","tags":"[]","date":"2017-10-13 19:51:38+00:00","url":"http://arxiv.org/abs/1710.05060v2"},{"x":"7.580598","y":"6.677922","title":"True Few-Shot Learning with Language Models","cluster":"2","author":"['Ethan Perez', 'Douwe Kiela', 'Kyunghyun Cho']","source":"alignment newsletter","tags":"[]","date":"2021-05-24 17:55:51+00:00","url":"http://arxiv.org/abs/2105.11447v1"},{"x":"11.415951","y":"5.6240897","title":"Multi-task Maximum Entropy Inverse Reinforcement Learning","cluster":"0","author":"['Adam Gleave', 'Oliver Habryka']","source":"arxiv","tags":"[]","date":"2018-05-22 21:57:34+00:00","url":"http://arxiv.org/abs/1805.08882v2"},{"x":"12.780675","y":"10.415472","title":"Inductive Coherence","cluster":"1","author":"['Scott Garrabrant', 'Benya Fallenstein', 'Abram Demski', 'Nate Soares']","source":"arxiv","tags":"[]","date":"2016-04-18 19:37:46+00:00","url":"http://arxiv.org/abs/1604.05288v3"},{"x":"10.827864","y":"9.844663","title":"That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi's paradox","cluster":"1","author":"['Anders Sandberg', 'Stuart Armstrong', 'Milan M. Cirkovic']","source":"arxiv","tags":"[]","date":"2017-04-27 15:41:00+00:00","url":"http://arxiv.org/abs/1705.03394v1"},{"x":"8.369537","y":"6.7013116","title":"HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning","cluster":"2","author":"['Oguzhan Gencoglu', 'Mark van Gils', 'Esin Guldogan', 'Chamin Morikawa', 'Mehmet Süzen', 'Mathias Gruber', 'Jussi Leinonen', 'Heikki Huttunen']","source":"alignment newsletter","tags":"[]","date":"2019-04-16 13:02:01+00:00","url":"http://arxiv.org/abs/1904.07633v1"},{"x":"8.574165","y":"6.103596","title":"Training Machine Learning Models by Regularizing their Explanations","cluster":"2","author":"['Andrew Slavin Ross']","source":"arxiv","tags":"[]","date":"2018-09-29 17:43:21+00:00","url":"http://arxiv.org/abs/1810.00869v1"},{"x":"7.588742","y":"6.3500185","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation","cluster":"2","author":"['Yonghui Wu', 'Mike Schuster', 'Zhifeng Chen', 'Quoc V. Le', 'Mohammad Norouzi', 'Wolfgang Macherey', 'Maxim Krikun', 'Yuan Cao', 'Qin Gao', 'Klaus Macherey', 'Jeff Klingner', 'Apurva Shah', 'Melvin Johnson', 'Xiaobing Liu', 'Łukasz Kaiser', 'Stephan Gouws', 'Yoshikiyo Kato', 'Taku Kudo', 'Hideto Kazawa', 'Keith Stevens', 'George Kurian', 'Nishant Patil', 'Wei Wang', 'Cliff Young', 'Jason Smith', 'Jason Riesa', 'Alex Rudnick', 'Oriol Vinyals', 'Greg Corrado', 'Macduff Hughes', 'Jeffrey Dean']","source":"arxiv","tags":"[]","date":"2016-09-26 19:59:55+00:00","url":"http://arxiv.org/abs/1609.08144v2"},{"x":"9.934443","y":"6.0873456","title":"Fractal AI: A fragile theory of intelligence","cluster":"4","author":"['Sergio Hernandez Cerezo', 'Guillem Duran Ballester']","source":"arxiv","tags":"[]","date":"2018-03-13 21:17:26+00:00","url":"http://arxiv.org/abs/1803.05049v5"},{"x":"10.92677","y":"5.637327","title":"Improving Deep Reinforcement Learning in Minecraft with Action Advice","cluster":"0","author":"['Spencer Frazier', 'Mark Riedl']","source":"alignment newsletter","tags":"[]","date":"2019-08-02 18:36:44+00:00","url":"http://arxiv.org/abs/1908.01007v1"},{"x":"9.451031","y":"5.9878125","title":"Fast Context Adaptation via Meta-Learning","cluster":"0","author":"['Luisa M Zintgraf', 'Kyriacos Shiarlis', 'Vitaly Kurin', 'Katja Hofmann', 'Shimon Whiteson']","source":"arxiv","tags":"[]","date":"2018-10-08 18:11:01+00:00","url":"http://arxiv.org/abs/1810.03642v4"},{"x":"11.505395","y":"6.4475055","title":"Understanding Learned Reward Functions","cluster":"0","author":"['Eric J. Michaud', 'Adam Gleave', 'Stuart Russell']","source":"alignment newsletter","tags":"[]","date":"2020-12-10 18:19:48+00:00","url":"http://arxiv.org/abs/2012.05862v1"},{"x":"8.244997","y":"5.1376314","title":"Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks","cluster":"2","author":"['Yarin Gal', 'Lewis Smith']","source":"arxiv","tags":"[]","date":"2018-06-02 16:43:17+00:00","url":"http://arxiv.org/abs/1806.00667v3"},{"x":"11.361662","y":"5.578784","title":"Efficiently Combining Human Demonstrations and Interventions for Safe Training of Autonomous Systems in Real-Time","cluster":"0","author":"['Vinicius G. Goecks', 'Gregory M. Gremillion', 'Vernon J. Lawhern', 'John Valasek', 'Nicholas R. Waytowich']","source":"arxiv","tags":"[]","date":"2018-10-26 22:23:27+00:00","url":"http://arxiv.org/abs/1810.11545v2"},{"x":"10.642058","y":"6.3966312","title":"Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures","cluster":"0","author":"['Jonathan Uesato', 'Ananya Kumar', 'Csaba Szepesvari', 'Tom Erez', 'Avraham Ruderman', 'Keith Anderson', 'Krishmamurthy', 'Dvijotham', 'Nicolas Heess', 'Pushmeet Kohli']","source":"arxiv","tags":"[]","date":"2018-12-04 19:39:53+00:00","url":"http://arxiv.org/abs/1812.01647v1"},{"x":"8.506287","y":"5.16163","title":"Adversarial Robustness as a Prior for Learned Representations","cluster":"2","author":"['Logan Engstrom', 'Andrew Ilyas', 'Shibani Santurkar', 'Dimitris Tsipras', 'Brandon Tran', 'Aleksander Madry']","source":"alignment newsletter","tags":"[]","date":"2019-06-03 17:55:20+00:00","url":"http://arxiv.org/abs/1906.00945v2"},{"x":"11.214668","y":"5.732135","title":"Learning a Prior over Intent via Meta-Inverse Reinforcement Learning","cluster":"0","author":"['Kelvin Xu', 'Ellis Ratner', 'Anca Dragan', 'Sergey Levine', 'Chelsea Finn']","source":"arxiv","tags":"[]","date":"2018-05-31 17:29:25+00:00","url":"http://arxiv.org/abs/1805.12573v5"},{"x":"7.9341297","y":"6.205128","title":"Resource-Efficient Neural Architect","cluster":"2","author":"['Yanqi Zhou', 'Siavash Ebrahimi', 'Sercan Ö. Arık', 'Haonan Yu', 'Hairong Liu', 'Greg Diamos']","source":"arxiv","tags":"[]","date":"2018-06-12 20:41:32+00:00","url":"http://arxiv.org/abs/1806.07912v1"},{"x":"8.939349","y":"7.780858","title":"An overview of 11 proposals for building safe advanced AI","cluster":"0","author":"['Evan Hubinger']","source":"arxiv","tags":"[]","date":"2020-12-04 22:53:18+00:00","url":"http://arxiv.org/abs/2012.07532v1"},{"x":"9.422733","y":"6.029935","title":"Learning to Continually Learn","cluster":"0","author":"['Shawn Beaulieu', 'Lapo Frati', 'Thomas Miconi', 'Joel Lehman', 'Kenneth O. Stanley', 'Jeff Clune', 'Nick Cheney']","source":"alignment newsletter","tags":"[]","date":"2020-02-21 22:52:00+00:00","url":"http://arxiv.org/abs/2002.09571v2"},{"x":"10.925436","y":"5.915637","title":"Towards Governing Agent's Efficacy: Action-Conditional $β$-VAE for Deep Transparent Reinforcement Learning","cluster":"0","author":"['John Yang', 'Gyujeong Lee', 'Minsung Hyun', 'Simyung Chang', 'Nojun Kwak']","source":"arxiv","tags":"[]","date":"2018-11-11 04:48:15+00:00","url":"http://arxiv.org/abs/1811.04350v1"},{"x":"11.344469","y":"5.6671877","title":"Learning Exploration Policies for Navigation","cluster":"0","author":"['Tao Chen', 'Saurabh Gupta', 'Abhinav Gupta']","source":"arxiv","tags":"[]","date":"2019-03-05 18:03:47+00:00","url":"http://arxiv.org/abs/1903.01959v1"},{"x":"8.112076","y":"5.3951187","title":"Learning Robust Representations by Projecting Superficial Statistics Out","cluster":"2","author":"['Haohan Wang', 'Zexue He', 'Zachary C. Lipton', 'Eric P. Xing']","source":"arxiv","tags":"[]","date":"2019-03-02 00:42:03+00:00","url":"http://arxiv.org/abs/1903.06256v1"},{"x":"8.117778","y":"5.188732","title":"ReAct: Out-of-distribution Detection With Rectified Activations","cluster":"2","author":"['Yiyou Sun', 'Chuan Guo', 'Yixuan Li']","source":"arxiv","tags":"[]","date":"2021-11-24 21:02:07+00:00","url":"http://arxiv.org/abs/2111.12797v1"},{"x":"10.945946","y":"6.260053","title":"Imitation Learning as $f$-Divergence Minimization","cluster":"0","author":"['Liyiming Ke', 'Sanjiban Choudhury', 'Matt Barnes', 'Wen Sun', 'Gilwoo Lee', 'Siddhartha Srinivasa']","source":"alignment newsletter","tags":"[]","date":"2019-05-30 07:19:13+00:00","url":"http://arxiv.org/abs/1905.12888v2"},{"x":"7.840951","y":"5.505063","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition","cluster":"2","author":"['Karen Simonyan', 'Andrew Zisserman']","source":"arxiv","tags":"[]","date":"2014-09-04 19:48:04+00:00","url":"http://arxiv.org/abs/1409.1556v6"},{"x":"10.919126","y":"6.115115","title":"Emergent Social Learning via Multi-agent Reinforcement Learning","cluster":"0","author":"['Kamal Ndousse', 'Douglas Eck', 'Sergey Levine', 'Natasha Jaques']","source":"alignment newsletter","tags":"[]","date":"2020-10-01 17:54:14+00:00","url":"http://arxiv.org/abs/2010.00581v3"},{"x":"10.575367","y":"7.602495","title":"Hierarchical Game-Theoretic Planning for Autonomous Vehicles","cluster":"0","author":"['Jaime F. Fisac', 'Eli Bronstein', 'Elis Stefansson', 'Dorsa Sadigh', 'S. Shankar Sastry', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2018-10-13 00:02:54+00:00","url":"http://arxiv.org/abs/1810.05766v1"},{"x":"7.8683195","y":"6.7747912","title":"Recursively Summarizing Books with Human Feedback","cluster":"2","author":"['Jeff Wu', 'Long Ouyang', 'Daniel M. Ziegler', 'Nisan Stiennon', 'Ryan Lowe', 'Jan Leike', 'Paul Christiano']","source":"arxiv","tags":"[]","date":"2021-09-22 17:34:18+00:00","url":"http://arxiv.org/abs/2109.10862v2"},{"x":"11.044804","y":"7.242186","title":"Meta-learning of Sequential Strategies","cluster":"0","author":"['Pedro A. Ortega', 'Jane X. Wang', 'Mark Rowland', 'Tim Genewein', 'Zeb Kurth-Nelson', 'Razvan Pascanu', 'Nicolas Heess', 'Joel Veness', 'Alex Pritzel', 'Pablo Sprechmann', 'Siddhant M. Jayakumar', 'Tom McGrath', 'Kevin Miller', 'Mohammad Azar', 'Ian Osband', 'Neil Rabinowitz', 'András György', 'Silvia Chiappa', 'Simon Osindero', 'Yee Whye Teh', 'Hado van Hasselt', 'Nando de Freitas', 'Matthew Botvinick', 'Shane Legg']","source":"alignment newsletter","tags":"[]","date":"2019-05-08 12:27:20+00:00","url":"http://arxiv.org/abs/1905.03030v2"},{"x":"10.41813","y":"5.7209873","title":"Reward learning from human preferences and demonstrations in Atari","cluster":"0","author":"['Borja Ibarz', 'Jan Leike', 'Tobias Pohlen', 'Geoffrey Irving', 'Shane Legg', 'Dario Amodei']","source":"arxiv","tags":"[]","date":"2018-11-15 18:33:43+00:00","url":"http://arxiv.org/abs/1811.06521v1"},{"x":"9.116744","y":"8.959101","title":"Limits to Verification and Validation of Agentic Behavior","cluster":"4","author":"['David J. Jilk']","source":"arxiv","tags":"[]","date":"2016-04-23 23:01:29+00:00","url":"http://arxiv.org/abs/1604.06963v2"},{"x":"10.928374","y":"9.230622","title":"Ontological Crises in Artificial Agents' Value Systems","cluster":"1","author":"['Peter de Blanc']","source":"arxiv","tags":"[]","date":"2011-05-19 09:32:46+00:00","url":"http://arxiv.org/abs/1105.3821v1"},{"x":"12.971161","y":"10.035794","title":"Asymptotic Convergence in Online Learning with Unbounded Delays","cluster":"2","author":"['Scott Garrabrant', 'Nate Soares', 'Jessica Taylor']","source":"arxiv","tags":"[]","date":"2016-04-18 19:04:59+00:00","url":"http://arxiv.org/abs/1604.05280v4"},{"x":"7.810227","y":"5.322463","title":"Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey","cluster":"2","author":"['Longlong Jing', 'Yingli Tian']","source":"arxiv","tags":"[]","date":"2019-02-16 21:30:18+00:00","url":"http://arxiv.org/abs/1902.06162v1"},{"x":"10.608214","y":"6.038403","title":"Towards Empathic Deep Q-Learning","cluster":"0","author":"['Bart Bussmann', 'Jacqueline Heinerman', 'Joel Lehman']","source":"alignment newsletter","tags":"[]","date":"2019-06-26 08:59:02+00:00","url":"http://arxiv.org/abs/1906.10918v1"},{"x":"8.550903","y":"5.1011443","title":"MixTrain: Scalable Training of Verifiably Robust Neural Networks","cluster":"2","author":"['Shiqi Wang', 'Yizheng Chen', 'Ahmed Abdou', 'Suman Jana']","source":"arxiv","tags":"[]","date":"2018-11-06 20:47:28+00:00","url":"http://arxiv.org/abs/1811.02625v2"},{"x":"7.508282","y":"6.47059","title":"Identifying Adversarial Attacks on Text Classifiers","cluster":"2","author":"['Zhouhang Xie', 'Jonathan Brophy', 'Adam Noack', 'Wencong You', 'Kalyani Asthana', 'Carter Perkins', 'Sabrina Reis', 'Sameer Singh', 'Daniel Lowd']","source":"arxiv","tags":"[]","date":"2022-01-21 06:16:04+00:00","url":"http://arxiv.org/abs/2201.08555v1"},{"x":"8.833944","y":"5.5590634","title":"Secure Deep Learning Engineering: A Software Quality Assurance Perspective","cluster":"2","author":"['Lei Ma', 'Felix Juefei-Xu', 'Minhui Xue', 'Qiang Hu', 'Sen Chen', 'Bo Li', 'Yang Liu', 'Jianjun Zhao', 'Jianxiong Yin', 'Simon See']","source":"arxiv","tags":"[]","date":"2018-10-10 14:04:08+00:00","url":"http://arxiv.org/abs/1810.04538v1"},{"x":"7.708728","y":"7.178078","title":"A General Language Assistant as a Laboratory for Alignment","cluster":"2","author":"['Amanda Askell', 'Yuntao Bai', 'Anna Chen', 'Dawn Drain', 'Deep Ganguli', 'Tom Henighan', 'Andy Jones', 'Nicholas Joseph', 'Ben Mann', 'Nova DasSarma', 'Nelson Elhage', 'Zac Hatfield-Dodds', 'Danny Hernandez', 'Jackson Kernion', 'Kamal Ndousse', 'Catherine Olsson', 'Dario Amodei', 'Tom Brown', 'Jack Clark', 'Sam McCandlish', 'Chris Olah', 'Jared Kaplan']","source":"arxiv","tags":"[]","date":"2021-12-01 22:24:34+00:00","url":"http://arxiv.org/abs/2112.00861v3"},{"x":"11.433834","y":"5.579913","title":"Dynamics-Aware Unsupervised Discovery of Skills","cluster":"0","author":"['Archit Sharma', 'Shixiang Gu', 'Sergey Levine', 'Vikash Kumar', 'Karol Hausman']","source":"arxiv","tags":"[]","date":"2019-07-02 21:32:19+00:00","url":"http://arxiv.org/abs/1907.01657v2"},{"x":"11.550351","y":"5.8741794","title":"Establishing Appropriate Trust via Critical States","cluster":"0","author":"['Sandy H. Huang', 'Kush Bhatia', 'Pieter Abbeel', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2018-10-18 17:29:47+00:00","url":"http://arxiv.org/abs/1810.08174v1"},{"x":"7.988996","y":"8.475206","title":"Social and Governance Implications of Improved Data Efficiency","cluster":"3","author":"['Aaron D. Tucker', 'Markus Anderljung', 'Allan Dafoe']","source":"alignment newsletter","tags":"[]","date":"2020-01-14 22:26:12+00:00","url":"http://arxiv.org/abs/2001.05068v1"},{"x":"9.128042","y":"8.00023","title":"On the Impossibility of Supersized Machines","cluster":"1","author":"['Ben Garfinkel', 'Miles Brundage', 'Daniel Filan', 'Carrick Flynn', 'Jelena Luketina', 'Michael Page', 'Anders Sandberg', 'Andrew Snyder-Beattie', 'Max Tegmark']","source":"arxiv","tags":"[]","date":"2017-03-31 17:14:39+00:00","url":"http://arxiv.org/abs/1703.10987v1"},{"x":"11.525058","y":"5.5153284","title":"End-to-End Robotic Reinforcement Learning without Reward Engineering","cluster":"0","author":"['Avi Singh', 'Larry Yang', 'Kristian Hartikainen', 'Chelsea Finn', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2019-04-16 17:59:23+00:00","url":"http://arxiv.org/abs/1904.07854v2"},{"x":"11.747549","y":"5.6504397","title":"Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration","cluster":"0","author":"['Chang Liu', 'Jessica B. Hamrick', 'Jaime F. Fisac', 'Anca D. Dragan', 'J. Karl Hedrick', 'S. Shankar Sastry', 'Thomas L. Griffiths']","source":"arxiv","tags":"[]","date":"2018-02-06 03:31:23+00:00","url":"http://arxiv.org/abs/1802.01780v1"},{"x":"12.195299","y":"8.341085","title":"Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice","cluster":"1","author":"['Lewis Hammond', 'James Fox', 'Tom Everitt', 'Alessandro Abate', 'Michael Wooldridge']","source":"arxiv","tags":"[]","date":"2021-02-09 18:20:50+00:00","url":"http://arxiv.org/abs/2102.05008v1"},{"x":"11.945825","y":"8.092108","title":"Modeling Friends and Foes","cluster":"1","author":"['Pedro A. Ortega', 'Shane Legg']","source":"alignment newsletter","tags":"[]","date":"2018-06-30 16:07:43+00:00","url":"http://arxiv.org/abs/1807.00196v1"},{"x":"8.310937","y":"6.0926986","title":"How Well do Feature Visualizations Support Causal Understanding of CNN Activations?","cluster":"2","author":"['Roland S. Zimmermann', 'Judy Borowski', 'Robert Geirhos', 'Matthias Bethge', 'Thomas S. A. Wallis', 'Wieland Brendel']","source":"arxiv","tags":"[]","date":"2021-06-23 14:52:23+00:00","url":"http://arxiv.org/abs/2106.12447v3"},{"x":"8.201018","y":"5.175096","title":"Scaling Out-of-Distribution Detection for Real-World Settings","cluster":"2","author":"['Dan Hendrycks', 'Steven Basart', 'Mantas Mazeika', 'Andy Zou', 'Joe Kwon', 'Mohammadreza Mostajabi', 'Jacob Steinhardt', 'Dawn Song']","source":"arxiv","tags":"[]","date":"2019-11-25 18:58:23+00:00","url":"http://arxiv.org/abs/1911.11132v3"},{"x":"10.07496","y":"5.871446","title":"Collaborating with Humans without Human Data","cluster":"0","author":"['DJ Strouse', 'Kevin R. McKee', 'Matt Botvinick', 'Edward Hughes', 'Richard Everett']","source":"alignment newsletter","tags":"[]","date":"2021-10-15 16:03:57+00:00","url":"http://arxiv.org/abs/2110.08176v2"},{"x":"8.5888405","y":"5.0327425","title":"Certified Defenses against Adversarial Examples","cluster":"2","author":"['Aditi Raghunathan', 'Jacob Steinhardt', 'Percy Liang']","source":"alignment newsletter","tags":"[]","date":"2018-01-29 02:08:21+00:00","url":"http://arxiv.org/abs/1801.09344v2"},{"x":"10.528623","y":"6.3104167","title":"Towards Characterizing Divergence in Deep Q-Learning","cluster":"0","author":"['Joshua Achiam', 'Ethan Knight', 'Pieter Abbeel']","source":"alignment newsletter","tags":"[]","date":"2019-03-21 09:42:41+00:00","url":"http://arxiv.org/abs/1903.08894v1"},{"x":"8.409739","y":"9.372959","title":"Axes for Sociotechnical Inquiry in AI Research","cluster":"3","author":"['Sarah Dean', 'Thomas Krendl Gilbert', 'Nathan Lambert', 'Tom Zick']","source":"arxiv","tags":"[]","date":"2021-04-26 16:49:04+00:00","url":"http://arxiv.org/abs/2105.06551v1"},{"x":"7.95178","y":"5.8480396","title":"A Constructive Prediction of the Generalization Error Across Scales","cluster":"2","author":"['Jonathan S. Rosenfeld', 'Amir Rosenfeld', 'Yonatan Belinkov', 'Nir Shavit']","source":"alignment newsletter","tags":"[]","date":"2019-09-27 13:27:53+00:00","url":"http://arxiv.org/abs/1909.12673v2"},{"x":"12.309114","y":"8.988184","title":"Counterfactual equivalence for POMDPs, and underlying deterministic environments","cluster":"1","author":"['Stuart Armstrong']","source":"arxiv","tags":"[]","date":"2018-01-11 12:40:59+00:00","url":"http://arxiv.org/abs/1801.03737v2"},{"x":"8.646349","y":"6.2873864","title":"Compositional Explanations of Neurons","cluster":"2","author":"['Jesse Mu', 'Jacob Andreas']","source":"alignment newsletter","tags":"[]","date":"2020-06-24 20:37:05+00:00","url":"http://arxiv.org/abs/2006.14032v2"},{"x":"7.9227004","y":"6.192669","title":"The Conditional Entropy Bottleneck","cluster":"2","author":"['Ian Fischer']","source":"alignment newsletter","tags":"[]","date":"2020-02-13 07:46:38+00:00","url":"http://arxiv.org/abs/2002.05379v1"},{"x":"7.4529953","y":"6.6232104","title":"On Adversarial Examples for Character-Level Neural Machine Translation","cluster":"2","author":"['Javid Ebrahimi', 'Daniel Lowd', 'Dejing Dou']","source":"arxiv","tags":"[]","date":"2018-06-23 20:08:56+00:00","url":"http://arxiv.org/abs/1806.09030v1"},{"x":"11.227539","y":"6.893692","title":"A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents","cluster":"0","author":"['Yueh-Hua Wu', 'Shou-De Lin']","source":"arxiv","tags":"[]","date":"2017-12-12 08:35:52+00:00","url":"http://arxiv.org/abs/1712.04172v2"},{"x":"8.142346","y":"5.1000714","title":"Natural Adversarial Examples","cluster":"2","author":"['Dan Hendrycks', 'Kevin Zhao', 'Steven Basart', 'Jacob Steinhardt', 'Dawn Song']","source":"alignment newsletter","tags":"[]","date":"2019-07-16 17:56:30+00:00","url":"http://arxiv.org/abs/1907.07174v4"},{"x":"10.899761","y":"5.4558096","title":"Generative Adversarial Imitation from Observation","cluster":"0","author":"['Faraz Torabi', 'Garrett Warnell', 'Peter Stone']","source":"arxiv","tags":"[]","date":"2018-07-17 00:25:15+00:00","url":"http://arxiv.org/abs/1807.06158v4"},{"x":"8.780084","y":"6.698268","title":"Machine Learning Explainability for External Stakeholders","cluster":"2","author":"['Umang Bhatt', 'McKane Andrus', 'Adrian Weller', 'Alice Xiang']","source":"arxiv","tags":"[]","date":"2020-07-10 14:27:06+00:00","url":"http://arxiv.org/abs/2007.05408v1"},{"x":"9.579122","y":"8.406655","title":"The Off-Switch Game","cluster":"4","author":"['Dylan Hadfield-Menell', 'Anca Dragan', 'Pieter Abbeel', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2016-11-24 15:23:48+00:00","url":"http://arxiv.org/abs/1611.08219v3"},{"x":"9.542377","y":"8.506287","title":"Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems","cluster":"4","author":"['Sandhya Saisubramanian', 'Shlomo Zilberstein', 'Ece Kamar']","source":"alignment newsletter","tags":"[]","date":"2020-08-24 16:48:46+00:00","url":"http://arxiv.org/abs/2008.12146v3"},{"x":"11.047051","y":"5.980103","title":"Deep Reinforcement Learning from Policy-Dependent Human Feedback","cluster":"0","author":"['Dilip Arumugam', 'Jun Ki Lee', 'Sophie Saskin', 'Michael L. Littman']","source":"alignment newsletter","tags":"[]","date":"2019-02-12 06:45:21+00:00","url":"http://arxiv.org/abs/1902.04257v1"},{"x":"9.642344","y":"5.812345","title":"No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling","cluster":"2","author":"['Xin Wang', 'Wenhu Chen', 'Yuan-Fang Wang', 'William Yang Wang']","source":"alignment newsletter","tags":"[]","date":"2018-04-24 17:41:24+00:00","url":"http://arxiv.org/abs/1804.09160v2"},{"x":"7.9636693","y":"6.1964445","title":"Measuring the Algorithmic Efficiency of Neural Networks","cluster":"2","author":"['Danny Hernandez', 'Tom B. Brown']","source":"arxiv","tags":"[]","date":"2020-05-08 22:26:37+00:00","url":"http://arxiv.org/abs/2005.04305v1"},{"x":"8.914004","y":"9.2401705","title":"Building Ethically Bounded AI","cluster":"4","author":"['Francesca Rossi', 'Nicholas Mattei']","source":"arxiv","tags":"[]","date":"2018-12-10 18:58:05+00:00","url":"http://arxiv.org/abs/1812.03980v1"},{"x":"11.22075","y":"6.221737","title":"Safe Reinforcement Learning by Imagining the Near Future","cluster":"0","author":"['Garrett Thomas', 'Yuping Luo', 'Tengyu Ma']","source":"arxiv","tags":"[]","date":"2022-02-15 23:28:24+00:00","url":"http://arxiv.org/abs/2202.07789v1"},{"x":"8.337878","y":"5.590299","title":"Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting","cluster":"2","author":"['Jun Shu', 'Qi Xie', 'Lixuan Yi', 'Qian Zhao', 'Sanping Zhou', 'Zongben Xu', 'Deyu Meng']","source":"arxiv","tags":"[]","date":"2019-02-20 02:29:55+00:00","url":"http://arxiv.org/abs/1902.07379v6"},{"x":"10.569013","y":"6.7341638","title":"Parenting: Safe Reinforcement Learning from Human Input","cluster":"0","author":"['Christopher Frye', 'Ilya Feige']","source":"arxiv","tags":"[]","date":"2019-02-18 19:10:18+00:00","url":"http://arxiv.org/abs/1902.06766v1"},{"x":"11.183335","y":"5.421615","title":"Imitation Learning from Video by Leveraging Proprioception","cluster":"0","author":"['Faraz Torabi', 'Garrett Warnell', 'Peter Stone']","source":"alignment newsletter","tags":"[]","date":"2019-05-22 19:21:05+00:00","url":"http://arxiv.org/abs/1905.09335v2"},{"x":"8.061547","y":"5.076547","title":"Predicting Out-of-Distribution Error with the Projection Norm","cluster":"2","author":"['Yaodong Yu', 'Zitong Yang', 'Alexander Wei', 'Yi Ma', 'Jacob Steinhardt']","source":"arxiv","tags":"[]","date":"2022-02-11 18:58:21+00:00","url":"http://arxiv.org/abs/2202.05834v1"},{"x":"8.439118","y":"8.933503","title":"Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database","cluster":"3","author":"['Sean McGregor']","source":"arxiv","tags":"[]","date":"2020-11-17 08:55:14+00:00","url":"http://arxiv.org/abs/2011.08512v1"},{"x":"11.311858","y":"6.433934","title":"More Robust Doubly Robust Off-policy Evaluation","cluster":"0","author":"['Mehrdad Farajtabar', 'Yinlam Chow', 'Mohammad Ghavamzadeh']","source":"arxiv","tags":"[]","date":"2018-02-10 01:32:03+00:00","url":"http://arxiv.org/abs/1802.03493v2"},{"x":"11.51231","y":"5.7205315","title":"Interactive Learning from Policy-Dependent Human Feedback","cluster":"0","author":"['James MacGlashan', 'Mark K Ho', 'Robert Loftin', 'Bei Peng', 'David Roberts', 'Matthew E. Taylor', 'Michael L. Littman']","source":"arxiv","tags":"[]","date":"2017-01-21 16:37:41+00:00","url":"http://arxiv.org/abs/1701.06049v1"},{"x":"10.928908","y":"5.723649","title":"Exploring Hierarchy-Aware Inverse Reinforcement Learning","cluster":"0","author":"['Chris Cundy', 'Daniel Filan']","source":"arxiv","tags":"[]","date":"2018-07-13 12:33:07+00:00","url":"http://arxiv.org/abs/1807.05037v1"},{"x":"11.307977","y":"7.151604","title":"REALab: An Embedded Perspective on Tampering","cluster":"0","author":"['Ramana Kumar', 'Jonathan Uesato', 'Richard Ngo', 'Tom Everitt', 'Victoria Krakovna', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2020-11-17 18:37:20+00:00","url":"http://arxiv.org/abs/2011.08820v1"},{"x":"7.947088","y":"5.542108","title":"Visualizing and Understanding Convolutional Networks","cluster":"2","author":"['Matthew D Zeiler', 'Rob Fergus']","source":"arxiv","tags":"[]","date":"2013-11-12 20:02:22+00:00","url":"http://arxiv.org/abs/1311.2901v3"},{"x":"11.129188","y":"6.4985304","title":"Maximum Causal Tsallis Entropy Imitation Learning","cluster":"0","author":"['Kyungjae Lee', 'Sungjoon Choi', 'Songhwai Oh']","source":"arxiv","tags":"[]","date":"2018-05-22 00:49:47+00:00","url":"http://arxiv.org/abs/1805.08336v2"},{"x":"9.422049","y":"6.296494","title":"Computational Power and the Social Impact of Artificial Intelligence","cluster":"0","author":"['Tim Hwang']","source":"alignment newsletter","tags":"[]","date":"2018-03-23 20:39:04+00:00","url":"http://arxiv.org/abs/1803.08971v1"},{"x":"7.568203","y":"7.011925","title":"Defending Against Neural Fake News","cluster":"2","author":"['Rowan Zellers', 'Ari Holtzman', 'Hannah Rashkin', 'Yonatan Bisk', 'Ali Farhadi', 'Franziska Roesner', 'Yejin Choi']","source":"arxiv","tags":"[]","date":"2019-05-29 17:58:52+00:00","url":"http://arxiv.org/abs/1905.12616v3"},{"x":"8.8364525","y":"6.141357","title":"Risks from Learned Optimization in Advanced Machine Learning Systems","cluster":"2","author":"['Evan Hubinger', 'Chris van Merwijk', 'Vladimir Mikulik', 'Joar Skalse', 'Scott Garrabrant']","source":"arxiv","tags":"[]","date":"2019-06-05 04:43:25+00:00","url":"http://arxiv.org/abs/1906.01820v3"},{"x":"8.501748","y":"5.009042","title":"Playing the Game of Universal Adversarial Perturbations","cluster":"2","author":"['Julien Perolat', 'Mateusz Malinowski', 'Bilal Piot', 'Olivier Pietquin']","source":"arxiv","tags":"[]","date":"2018-09-20 18:48:36+00:00","url":"http://arxiv.org/abs/1809.07802v2"},{"x":"11.269875","y":"5.8660326","title":"MADE: Exploration via Maximizing Deviation from Explored Regions","cluster":"0","author":"['Tianjun Zhang', 'Paria Rashidinejad', 'Jiantao Jiao', 'Yuandong Tian', 'Joseph Gonzalez', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2021-06-18 17:57:00+00:00","url":"http://arxiv.org/abs/2106.10268v1"},{"x":"10.977113","y":"6.8675694","title":"Adaptive Mechanism Design: Learning to Promote Cooperation","cluster":"0","author":"['Tobias Baumann', 'Thore Graepel', 'John Shawe-Taylor']","source":"arxiv","tags":"[]","date":"2018-06-11 15:48:37+00:00","url":"http://arxiv.org/abs/1806.04067v2"},{"x":"10.481898","y":"5.7488403","title":"Beyond Winning and Losing: Modeling Human Motivations and Behaviors Using Inverse Reinforcement Learning","cluster":"0","author":"['Baoxiang Wang', 'Tongfang Sun', 'Xianjun Sam Zheng']","source":"alignment newsletter","tags":"[]","date":"2018-07-01 18:20:23+00:00","url":"http://arxiv.org/abs/1807.00366v2"},{"x":"8.898322","y":"8.791858","title":"A Psychopathological Approach to Safety Engineering in AI and AGI","cluster":"4","author":"['Vahid Behzadan', 'Arslan Munir', 'Roman V. Yampolskiy']","source":"arxiv","tags":"[]","date":"2018-05-23 00:19:07+00:00","url":"http://arxiv.org/abs/1805.08915v1"},{"x":"11.264019","y":"5.7236094","title":"Safe Deep RL in 3D Environments using Human Feedback","cluster":"0","author":"['Matthew Rahtz', 'Vikrant Varma', 'Ramana Kumar', 'Zachary Kenton', 'Shane Legg', 'Jan Leike']","source":"arxiv","tags":"[]","date":"2022-01-20 10:26:34+00:00","url":"http://arxiv.org/abs/2201.08102v2"},{"x":"10.6636305","y":"6.68788","title":"Learning Existing Social Conventions via Observationally Augmented Self-Play","cluster":"0","author":"['Adam Lerer', 'Alexander Peysakhovich']","source":"arxiv","tags":"[]","date":"2018-06-26 15:46:44+00:00","url":"http://arxiv.org/abs/1806.10071v3"},{"x":"10.91998","y":"5.761738","title":"Making Efficient Use of Demonstrations to Solve Hard Exploration Problems","cluster":"0","author":"['Tom Le Paine', 'Caglar Gulcehre', 'Bobak Shahriari', 'Misha Denil', 'Matt Hoffman', 'Hubert Soyer', 'Richard Tanburn', 'Steven Kapturowski', 'Neil Rabinowitz', 'Duncan Williams', 'Gabriel Barth-Maron', 'Ziyu Wang', 'Nando de Freitas', 'Worlds Team']","source":"arxiv","tags":"[]","date":"2019-09-03 18:20:48+00:00","url":"http://arxiv.org/abs/1909.01387v1"},{"x":"8.898526","y":"8.3612795","title":"Building Safer AGI by introducing Artificial Stupidity","cluster":"4","author":"['Michaël Trazzi', 'Roman V. Yampolskiy']","source":"arxiv","tags":"[]","date":"2018-08-11 00:14:33+00:00","url":"http://arxiv.org/abs/1808.03644v1"},{"x":"7.8687315","y":"9.524412","title":"AI Ethics Statements -- Analysis and lessons learnt from NeurIPS Broader Impact Statements","cluster":"3","author":"['Carolyn Ashurst', 'Emmie Hine', 'Paul Sedille', 'Alexis Carlier']","source":"arxiv","tags":"[]","date":"2021-11-02 16:17:12+00:00","url":"http://arxiv.org/abs/2111.01705v1"},{"x":"8.035302","y":"6.4559283","title":"Applying Deep Learning To Airbnb Search","cluster":"2","author":"['Malay Haldar', 'Mustafa Abdool', 'Prashant Ramanathan', 'Tao Xu', 'Shulin Yang', 'Huizhong Duan', 'Qing Zhang', 'Nick Barrow-Williams', 'Bradley C. Turnbull', 'Brendan M. Collins', 'Thomas Legrand']","source":"arxiv","tags":"[]","date":"2018-10-22 23:11:01+00:00","url":"http://arxiv.org/abs/1810.09591v2"},{"x":"11.376151","y":"6.5925226","title":"Universal Reinforcement Learning Algorithms: Survey and Experiments","cluster":"0","author":"['John Aslanides', 'Jan Leike', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2017-05-30 11:41:00+00:00","url":"http://arxiv.org/abs/1705.10557v1"},{"x":"10.697028","y":"5.836713","title":"Towards Better Interpretability in Deep Q-Networks","cluster":"0","author":"['Raghuram Mandyam Annasamy', 'Katia Sycara']","source":"arxiv","tags":"[]","date":"2018-09-15 01:34:27+00:00","url":"http://arxiv.org/abs/1809.05630v2"},{"x":"11.196988","y":"5.7428465","title":"Learning Human Objectives by Evaluating Hypothetical Behavior","cluster":"0","author":"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine', 'Shane Legg', 'Jan Leike']","source":"arxiv","tags":"[]","date":"2019-12-05 18:25:48+00:00","url":"http://arxiv.org/abs/1912.05652v2"},{"x":"8.671601","y":"9.263684","title":"The Problem with Metrics is a Fundamental Problem for AI","cluster":"3","author":"['Rachel Thomas', 'David Uminsky']","source":"arxiv","tags":"[]","date":"2020-02-20 00:56:11+00:00","url":"http://arxiv.org/abs/2002.08512v1"},{"x":"11.281946","y":"5.828807","title":"Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning","cluster":"0","author":"['Tianhe Yu', 'Deirdre Quillen', 'Zhanpeng He', 'Ryan Julian', 'Avnish Narayan', 'Hayden Shively', 'Adithya Bellathur', 'Karol Hausman', 'Chelsea Finn', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2019-10-24 03:19:46+00:00","url":"http://arxiv.org/abs/1910.10897v2"},{"x":"10.854664","y":"5.39104","title":"Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text","cluster":"0","author":"['Felix Hill', 'Sona Mokra', 'Nathaniel Wong', 'Tim Harley']","source":"alignment newsletter","tags":"[]","date":"2020-05-19 12:16:58+00:00","url":"http://arxiv.org/abs/2005.09382v1"},{"x":"11.081652","y":"6.247667","title":"Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones","cluster":"0","author":"['Brijen Thananjeyan', 'Ashwin Balakrishna', 'Suraj Nair', 'Michael Luo', 'Krishnan Srinivasan', 'Minho Hwang', 'Joseph E. Gonzalez', 'Julian Ibarz', 'Chelsea Finn', 'Ken Goldberg']","source":"alignment newsletter","tags":"[]","date":"2020-10-29 20:10:02+00:00","url":"http://arxiv.org/abs/2010.15920v2"},{"x":"9.622346","y":"8.869021","title":"Artificial Intelligence, Values and Alignment","cluster":"4","author":"['Iason Gabriel']","source":"alignment newsletter","tags":"[]","date":"2020-01-13 10:32:16+00:00","url":"http://arxiv.org/abs/2001.09768v2"},{"x":"11.700111","y":"5.591923","title":"Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries","cluster":"0","author":"['Chandrayee Basu', 'Mukesh Singhal', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2018-02-05 19:03:26+00:00","url":"http://arxiv.org/abs/1802.01604v1"},{"x":"11.861078","y":"9.5557165","title":"Sequential Extensions of Causal and Evidential Decision Theory","cluster":"1","author":"['Tom Everitt', 'Jan Leike', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2015-06-24 13:16:16+00:00","url":"http://arxiv.org/abs/1506.07359v1"},{"x":"10.079171","y":"9.574579","title":"Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)","cluster":"1","author":"['Peter Eckersley']","source":"alignment newsletter","tags":"[]","date":"2018-12-31 23:51:27+00:00","url":"http://arxiv.org/abs/1901.00064v3"},{"x":"10.784699","y":"6.1129694","title":"Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization","cluster":"0","author":"['Paul Barde', 'Julien Roy', 'Wonseok Jeon', 'Joelle Pineau', 'Christopher Pal', 'Derek Nowrouzezahrai']","source":"alignment newsletter","tags":"[]","date":"2020-06-23 18:29:13+00:00","url":"http://arxiv.org/abs/2006.13258v6"},{"x":"8.329862","y":"5.1145315","title":"PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures","cluster":"2","author":"['Dan Hendrycks', 'Andy Zou', 'Mantas Mazeika', 'Leonard Tang', 'Bo Li', 'Dawn Song', 'Jacob Steinhardt']","source":"arxiv","tags":"[]","date":"2021-12-09 18:59:31+00:00","url":"http://arxiv.org/abs/2112.05135v2"},{"x":"8.311146","y":"5.383646","title":"Generative Adversarial Networks","cluster":"2","author":"['Ian J. Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio']","source":"arxiv","tags":"[]","date":"2014-06-10 18:58:17+00:00","url":"http://arxiv.org/abs/1406.2661v1"},{"x":"8.156785","y":"7.1242795","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods","cluster":"2","author":"['Stephanie Lin', 'Jacob Hilton', 'Owain Evans']","source":"alignment newsletter","tags":"[]","date":"2021-09-08 17:15:27+00:00","url":"http://arxiv.org/abs/2109.07958v1"},{"x":"10.466161","y":"6.048313","title":"Self-Imitation Learning","cluster":"0","author":"['Junhyuk Oh', 'Yijie Guo', 'Satinder Singh', 'Honglak Lee']","source":"arxiv","tags":"[]","date":"2018-06-14 16:25:55+00:00","url":"http://arxiv.org/abs/1806.05635v1"},{"x":"7.9187956","y":"5.3200088","title":"Image Synthesis with a Single (Robust) Classifier","cluster":"2","author":"['Shibani Santurkar', 'Dimitris Tsipras', 'Brandon Tran', 'Andrew Ilyas', 'Logan Engstrom', 'Aleksander Madry']","source":"alignment newsletter","tags":"[]","date":"2019-06-06 09:12:08+00:00","url":"http://arxiv.org/abs/1906.09453v2"},{"x":"11.289389","y":"6.2509084","title":"Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization","cluster":"0","author":"['Takayuki Osa', 'Voot Tangkaratt', 'Masashi Sugiyama']","source":"arxiv","tags":"[]","date":"2019-01-05 04:43:05+00:00","url":"http://arxiv.org/abs/1901.01365v2"},{"x":"8.565095","y":"9.014125","title":"Guidelines for Artificial Intelligence Containment","cluster":"3","author":"['James Babcock', 'Janos Kramar', 'Roman V. Yampolskiy']","source":"arxiv","tags":"[]","date":"2017-07-24 18:33:18+00:00","url":"http://arxiv.org/abs/1707.08476v1"},{"x":"8.695595","y":"7.134834","title":"Neurosymbolic AI: The 3rd Wave","cluster":"4","author":"[\"Artur d'Avila Garcez\", 'Luis C. Lamb']","source":"alignment newsletter","tags":"[]","date":"2020-12-10 18:31:38+00:00","url":"http://arxiv.org/abs/2012.05876v2"},{"x":"11.437059","y":"6.9204316","title":"Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective","cluster":"0","author":"['Tom Everitt', 'Marcus Hutter', 'Ramana Kumar', 'Victoria Krakovna']","source":"arxiv","tags":"[]","date":"2019-08-13 16:50:00+00:00","url":"http://arxiv.org/abs/1908.04734v5"},{"x":"9.448631","y":"7.3600097","title":"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense","cluster":"4","author":"['Yixin Zhu', 'Tao Gao', 'Lifeng Fan', 'Siyuan Huang', 'Mark Edmonds', 'Hangxin Liu', 'Feng Gao', 'Chi Zhang', 'Siyuan Qi', 'Ying Nian Wu', 'Joshua B. Tenenbaum', 'Song-Chun Zhu']","source":"arxiv","tags":"[]","date":"2020-04-20 04:07:28+00:00","url":"http://arxiv.org/abs/2004.09044v1"},{"x":"9.2944975","y":"5.9911737","title":"Meta-Learning without Memorization","cluster":"2","author":"['Mingzhang Yin', 'George Tucker', 'Mingyuan Zhou', 'Sergey Levine', 'Chelsea Finn']","source":"alignment newsletter","tags":"[]","date":"2019-12-09 02:30:46+00:00","url":"http://arxiv.org/abs/1912.03820v3"},{"x":"10.721693","y":"6.059867","title":"Ray Interference: a Source of Plateaus in Deep Reinforcement Learning","cluster":"0","author":"['Tom Schaul', 'Diana Borsa', 'Joseph Modayil', 'Razvan Pascanu']","source":"alignment newsletter","tags":"[]","date":"2019-04-25 16:54:02+00:00","url":"http://arxiv.org/abs/1904.11455v1"},{"x":"8.626796","y":"9.396067","title":"Building Ethics into Artificial Intelligence","cluster":"3","author":"['Han Yu', 'Zhiqi Shen', 'Chunyan Miao', 'Cyril Leung', 'Victor R. Lesser', 'Qiang Yang']","source":"arxiv","tags":"[]","date":"2018-12-07 09:18:01+00:00","url":"http://arxiv.org/abs/1812.02953v1"},{"x":"10.789923","y":"5.9205194","title":"Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences","cluster":"0","author":"['Daniel S. Brown', 'Russell Coleman', 'Ravi Srinivasan', 'Scott Niekum']","source":"alignment newsletter","tags":"[]","date":"2020-02-21 02:04:54+00:00","url":"http://arxiv.org/abs/2002.09089v4"},{"x":"11.394007","y":"6.2867045","title":"Positive-Unlabeled Reward Learning","cluster":"0","author":"['Danfei Xu', 'Misha Denil']","source":"alignment newsletter","tags":"[]","date":"2019-11-01 16:47:44+00:00","url":"http://arxiv.org/abs/1911.00459v1"},{"x":"7.83419","y":"8.838462","title":"Forecasting AI Progress: A Research Agenda","cluster":"3","author":"['Ross Gruetzemacher', 'Florian Dorner', 'Niko Bernaola-Alvarez', 'Charlie Giattino', 'David Manheim']","source":"alignment newsletter","tags":"[]","date":"2020-08-04 21:46:46+00:00","url":"http://arxiv.org/abs/2008.01848v1"},{"x":"7.494332","y":"6.9778676","title":"Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models","cluster":"2","author":"['Alex Tamkin', 'Miles Brundage', 'Jack Clark', 'Deep Ganguli']","source":"arxiv","tags":"[]","date":"2021-02-04 09:27:04+00:00","url":"http://arxiv.org/abs/2102.02503v1"},{"x":"10.276228","y":"8.941341","title":"Categorizing Wireheading in Partially Embedded Agents","cluster":"4","author":"['Arushi Majha', 'Sayan Sarkar', 'Davide Zagami']","source":"arxiv","tags":"[]","date":"2019-06-21 13:38:35+00:00","url":"http://arxiv.org/abs/1906.09136v1"},{"x":"10.954042","y":"6.8728156","title":"On Gradient-Based Learning in Continuous Games","cluster":"0","author":"['Eric Mazumdar', 'Lillian J. Ratliff', 'S. Shankar Sastry']","source":"arxiv","tags":"[]","date":"2018-04-16 01:14:17+00:00","url":"http://arxiv.org/abs/1804.05464v3"},{"x":"8.335863","y":"9.437691","title":"Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence","cluster":"3","author":"['Shakir Mohamed', 'Marie-Therese Png', 'William Isaac']","source":"arxiv","tags":"[]","date":"2020-07-08 12:36:21+00:00","url":"http://arxiv.org/abs/2007.04068v1"},{"x":"8.764194","y":"8.059408","title":"Human-Centered Artificial Intelligence and Machine Learning","cluster":"4","author":"['Mark O. Riedl']","source":"arxiv","tags":"[]","date":"2019-01-31 02:47:16+00:00","url":"http://arxiv.org/abs/1901.11184v1"},{"x":"8.290159","y":"5.6805224","title":"More Data Can Hurt for Linear Regression: Sample-wise Double Descent","cluster":"2","author":"['Preetum Nakkiran']","source":"alignment newsletter","tags":"[]","date":"2019-12-16 08:28:26+00:00","url":"http://arxiv.org/abs/1912.07242v1"},{"x":"11.179459","y":"5.6447906","title":"Robust Imitation Learning from Noisy Demonstrations","cluster":"0","author":"['Voot Tangkaratt', 'Nontawat Charoenphakdee', 'Masashi Sugiyama']","source":"alignment newsletter","tags":"[]","date":"2020-10-20 10:41:37+00:00","url":"http://arxiv.org/abs/2010.10181v3"},{"x":"10.908818","y":"6.01911","title":"IQ-Learn: Inverse soft-Q Learning for Imitation","cluster":"0","author":"['Divyansh Garg', 'Shuvam Chakraborty', 'Chris Cundy', 'Jiaming Song', 'Stefano Ermon']","source":"alignment newsletter","tags":"[]","date":"2021-06-23 03:43:10+00:00","url":"http://arxiv.org/abs/2106.12142v2"},{"x":"8.528451","y":"7.7666907","title":"Implications of Quantum Computing for Artificial Intelligence alignment research","cluster":"4","author":"['Jaime Sevilla', 'Pablo Moreno']","source":"arxiv","tags":"[]","date":"2019-08-19 17:53:34+00:00","url":"http://arxiv.org/abs/1908.07613v3"},{"x":"10.389102","y":"5.7632174","title":"Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning","cluster":"0","author":"['Pedro A. Tsividis', 'Joao Loula', 'Jake Burga', 'Nathan Foss', 'Andres Campero', 'Thomas Pouncy', 'Samuel J. Gershman', 'Joshua B. Tenenbaum']","source":"alignment newsletter","tags":"[]","date":"2021-07-27 01:38:13+00:00","url":"http://arxiv.org/abs/2107.12544v1"},{"x":"11.774027","y":"10.493368","title":"Temporal Inference with Finite Factored Sets","cluster":"1","author":"['Scott Garrabrant']","source":"arxiv","tags":"[]","date":"2021-09-23 17:33:30+00:00","url":"http://arxiv.org/abs/2109.11513v1"},{"x":"7.780323","y":"5.3389487","title":"Retrieval Augmented Classification for Long-Tail Visual Recognition","cluster":"2","author":"['Alexander Long', 'Wei Yin', 'Thalaiyasingam Ajanthan', 'Vu Nguyen', 'Pulak Purkait', 'Ravi Garg', 'Alan Blair', 'Chunhua Shen', 'Anton van den Hengel']","source":"arxiv","tags":"[]","date":"2022-02-22 23:40:51+00:00","url":"http://arxiv.org/abs/2202.11233v1"},{"x":"10.000555","y":"6.741922","title":"Detection of Dataset Shifts in Learning-Enabled Cyber-Physical Systems using Variational Autoencoder for Regression","cluster":"0","author":"['Feiyang Cai', 'Ali I. Ozdagli', 'Xenofon Koutsoukos']","source":"arxiv","tags":"[]","date":"2021-04-14 03:46:37+00:00","url":"http://arxiv.org/abs/2104.06613v1"},{"x":"8.392586","y":"5.9302177","title":"Gradient Surgery for Multi-Task Learning","cluster":"2","author":"['Tianhe Yu', 'Saurabh Kumar', 'Abhishek Gupta', 'Sergey Levine', 'Karol Hausman', 'Chelsea Finn']","source":"alignment newsletter","tags":"[]","date":"2020-01-19 06:33:47+00:00","url":"http://arxiv.org/abs/2001.06782v4"},{"x":"10.981413","y":"5.3247075","title":"Language Conditioned Imitation Learning over Unstructured Data","cluster":"0","author":"['Corey Lynch', 'Pierre Sermanet']","source":"arxiv","tags":"[]","date":"2020-05-15 17:08:50+00:00","url":"http://arxiv.org/abs/2005.07648v2"},{"x":"9.041709","y":"5.5221562","title":"Provably Robust Detection of Out-of-distribution Data (almost) for free","cluster":"2","author":"['Alexander Meinke', 'Julian Bitterwolf', 'Matthias Hein']","source":"arxiv","tags":"[]","date":"2021-06-08 11:40:49+00:00","url":"http://arxiv.org/abs/2106.04260v1"},{"x":"9.009478","y":"7.3865943","title":"Explaining Explanations in AI","cluster":"4","author":"['Brent Mittelstadt', 'Chris Russell', 'Sandra Wachter']","source":"arxiv","tags":"[]","date":"2018-11-04 21:35:16+00:00","url":"http://arxiv.org/abs/1811.01439v1"},{"x":"7.895678","y":"5.256427","title":"Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications","cluster":"2","author":"['Sandhini Agarwal', 'Gretchen Krueger', 'Jack Clark', 'Alec Radford', 'Jong Wook Kim', 'Miles Brundage']","source":"alignment newsletter","tags":"[]","date":"2021-08-05 19:05:57+00:00","url":"http://arxiv.org/abs/2108.02818v1"},{"x":"9.58457","y":"9.245046","title":"Ethical Artificial Intelligence","cluster":"4","author":"['Bill Hibbard']","source":"arxiv","tags":"[]","date":"2014-11-05 19:40:02+00:00","url":"http://arxiv.org/abs/1411.1373v9"},{"x":"10.549971","y":"6.4944105","title":"Human-AI Learning Performance in Multi-Armed Bandits","cluster":"0","author":"['Ravi Pandya', 'Sandy H. Huang', 'Dylan Hadfield-Menell', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2018-12-21 21:28:11+00:00","url":"http://arxiv.org/abs/1812.09376v1"},{"x":"7.937882","y":"5.3697743","title":"Interpretable Discovery in Large Image Data Sets","cluster":"2","author":"['Kiri L. Wagstaff', 'Jake Lee']","source":"arxiv","tags":"[]","date":"2018-06-21 17:30:26+00:00","url":"http://arxiv.org/abs/1806.08340v1"},{"x":"8.349675","y":"7.982878","title":"Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence","cluster":"2","author":"['Luca Bortolussi', 'Guido Sanguinetti']","source":"arxiv","tags":"[]","date":"2018-11-08 17:51:27+00:00","url":"http://arxiv.org/abs/1811.03571v2"},{"x":"7.7462835","y":"7.0742955","title":"Unsupervised Question Decomposition for Question Answering","cluster":"2","author":"['Ethan Perez', 'Patrick Lewis', 'Wen-tau Yih', 'Kyunghyun Cho', 'Douwe Kiela']","source":"alignment newsletter","tags":"[]","date":"2020-02-22 19:40:35+00:00","url":"http://arxiv.org/abs/2002.09758v3"},{"x":"11.092722","y":"6.340285","title":"What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study","cluster":"0","author":"['Marcin Andrychowicz', 'Anton Raichuk', 'Piotr Stańczyk', 'Manu Orsini', 'Sertan Girgin', 'Raphael Marinier', 'Léonard Hussenot', 'Matthieu Geist', 'Olivier Pietquin', 'Marcin Michalski', 'Sylvain Gelly', 'Olivier Bachem']","source":"alignment newsletter","tags":"[]","date":"2020-06-10 17:59:03+00:00","url":"http://arxiv.org/abs/2006.05990v1"},{"x":"11.555052","y":"5.656986","title":"On the Utility of Model Learning in HRI","cluster":"0","author":"['Gokul Swamy', 'Jens Schulz', 'Rohan Choudhury', 'Dylan Hadfield-Menell', 'Anca Dragan']","source":"alignment newsletter","tags":"[]","date":"2019-01-04 19:55:49+00:00","url":"http://arxiv.org/abs/1901.01291v2"},{"x":"9.196663","y":"5.817071","title":"Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness","cluster":"2","author":"['Greg Anderson', 'Shankara Pailoor', 'Isil Dillig', 'Swarat Chaudhuri']","source":"arxiv","tags":"[]","date":"2019-04-22 17:21:52+00:00","url":"http://arxiv.org/abs/1904.09959v2"},{"x":"10.859996","y":"6.356312","title":"Neurosymbolic Reinforcement Learning with Formally Verified Exploration","cluster":"0","author":"['Greg Anderson', 'Abhinav Verma', 'Isil Dillig', 'Swarat Chaudhuri']","source":"alignment newsletter","tags":"[]","date":"2020-09-26 14:51:04+00:00","url":"http://arxiv.org/abs/2009.12612v2"},{"x":"10.830522","y":"6.488911","title":"SafeLife 1.0: Exploring Side Effects in Complex Environments","cluster":"0","author":"['Carroll L. Wainwright', 'Peter Eckersley']","source":"arxiv","tags":"[]","date":"2019-12-03 06:44:48+00:00","url":"http://arxiv.org/abs/1912.01217v2"},{"x":"8.568071","y":"9.413305","title":"AI Research Considerations for Human Existential Safety (ARCHES)","cluster":"3","author":"['Andrew Critch', 'David Krueger']","source":"arxiv","tags":"[]","date":"2020-05-30 02:05:16+00:00","url":"http://arxiv.org/abs/2006.04948v1"},{"x":"7.5629354","y":"6.939673","title":"Training language models to follow instructions with human feedback","cluster":"2","author":"['Long Ouyang', 'Jeff Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll L. Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Ray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke Miller', 'Maddie Simens', 'Amanda Askell', 'Peter Welinder', 'Paul Christiano', 'Jan Leike', 'Ryan Lowe']","source":"arxiv","tags":"[]","date":"2022-03-04 07:04:42+00:00","url":"http://arxiv.org/abs/2203.02155v1"},{"x":"12.168029","y":"9.1980295","title":"Planning With Uncertain Specifications (PUnS)","cluster":"1","author":"['Ankit Shah', 'Shen Li', 'Julie Shah']","source":"alignment newsletter","tags":"[]","date":"2019-06-07 16:32:16+00:00","url":"http://arxiv.org/abs/1906.03218v3"},{"x":"11.388424","y":"7.947057","title":"User Tampering in Reinforcement Learning Recommender Systems","cluster":"1","author":"['Charles Evans', 'Atoosa Kasirzadeh']","source":"alignment newsletter","tags":"[]","date":"2021-09-09 07:53:23+00:00","url":"http://arxiv.org/abs/2109.04083v1"},{"x":"11.202083","y":"8.518608","title":"Algorithmic Fairness from a Non-ideal Perspective","cluster":"1","author":"['Sina Fazelpour', 'Zachary C. Lipton']","source":"alignment newsletter","tags":"[]","date":"2020-01-08 18:44:41+00:00","url":"http://arxiv.org/abs/2001.09773v1"},{"x":"10.2370615","y":"5.8630896","title":"Agent57: Outperforming the Atari Human Benchmark","cluster":"0","author":"['Adrià Puigdomènech Badia', 'Bilal Piot', 'Steven Kapturowski', 'Pablo Sprechmann', 'Alex Vitvitskyi', 'Daniel Guo', 'Charles Blundell']","source":"arxiv","tags":"[]","date":"2020-03-30 11:33:16+00:00","url":"http://arxiv.org/abs/2003.13350v1"},{"x":"10.942361","y":"6.944071","title":"Accumulating Risk Capital Through Investing in Cooperation","cluster":"0","author":"['Charlotte Roman', 'Michael Dennis', 'Andrew Critch', 'Stuart Russell']","source":"arxiv","tags":"[]","date":"2021-01-25 18:41:45+00:00","url":"http://arxiv.org/abs/2101.10305v2"},{"x":"11.716999","y":"8.976434","title":"The Incentives that Shape Behaviour","cluster":"1","author":"['Ryan Carey', 'Eric Langlois', 'Tom Everitt', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2020-01-20 14:32:07+00:00","url":"http://arxiv.org/abs/2001.07118v2"},{"x":"11.533927","y":"5.4468365","title":"Hierarchical visuomotor control of humanoids","cluster":"0","author":"['Josh Merel', 'Arun Ahuja', 'Vu Pham', 'Saran Tunyasuvunakool', 'Siqi Liu', 'Dhruva Tirumala', 'Nicolas Heess', 'Greg Wayne']","source":"arxiv","tags":"[]","date":"2018-11-23 19:55:55+00:00","url":"http://arxiv.org/abs/1811.09656v2"},{"x":"9.941369","y":"8.231454","title":"Artificial Fun: Mapping Minds to the Space of Fun","cluster":"4","author":"['Soenke Ziesche', 'Roman V. Yampolskiy']","source":"arxiv","tags":"[]","date":"2016-06-22 20:28:53+00:00","url":"http://arxiv.org/abs/1606.07092v1"},{"x":"11.127379","y":"5.8774366","title":"Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations","cluster":"0","author":"['Daniel S. Brown', 'Wonjoon Goo', 'Prabhat Nagarajan', 'Scott Niekum']","source":"alignment newsletter","tags":"[]","date":"2019-04-12 19:34:43+00:00","url":"http://arxiv.org/abs/1904.06387v5"},{"x":"11.495584","y":"5.5844536","title":"Scaling data-driven robotics with reward sketching and batch reinforcement learning","cluster":"0","author":"['Serkan Cabi', 'Sergio Gómez Colmenarejo', 'Alexander Novikov', 'Ksenia Konyushkova', 'Scott Reed', 'Rae Jeong', 'Konrad Zolna', 'Yusuf Aytar', 'David Budden', 'Mel Vecerik', 'Oleg Sushkov', 'David Barker', 'Jonathan Scholz', 'Misha Denil', 'Nando de Freitas', 'Ziyu Wang']","source":"alignment newsletter","tags":"[]","date":"2019-09-26 15:45:23+00:00","url":"http://arxiv.org/abs/1909.12200v3"},{"x":"9.223254","y":"7.6896377","title":"Asymptotically Unambitious Artificial General Intelligence","cluster":"4","author":"['Michael K Cohen', 'Badri Vellambi', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2019-05-29 02:48:15+00:00","url":"http://arxiv.org/abs/1905.12186v4"},{"x":"11.31341","y":"6.7697425","title":"Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement","cluster":"0","author":"['Benjamin Eysenbach', 'Xinyang Geng', 'Sergey Levine', 'Ruslan Salakhutdinov']","source":"alignment newsletter","tags":"[]","date":"2020-02-25 18:36:31+00:00","url":"http://arxiv.org/abs/2002.11089v1"},{"x":"9.166125","y":"6.460416","title":"AutoML-Zero: Evolving Machine Learning Algorithms From Scratch","cluster":"2","author":"['Esteban Real', 'Chen Liang', 'David R. So', 'Quoc V. Le']","source":"alignment newsletter","tags":"[]","date":"2020-03-06 19:00:04+00:00","url":"http://arxiv.org/abs/2003.03384v2"},{"x":"9.030328","y":"5.9805","title":"TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing","cluster":"2","author":"['Augustus Odena', 'Ian Goodfellow']","source":"arxiv","tags":"[]","date":"2018-07-28 02:11:40+00:00","url":"http://arxiv.org/abs/1807.10875v1"},{"x":"8.30733","y":"5.8716197","title":"Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization","cluster":"2","author":"['Satrajit Chatterjee']","source":"arxiv","tags":"[]","date":"2020-02-25 03:59:31+00:00","url":"http://arxiv.org/abs/2002.10657v1"},{"x":"11.449417","y":"6.3270435","title":"Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods","cluster":"0","author":"['Gergely Neu', 'Csaba Szepesvari']","source":"arxiv","tags":"[]","date":"2012-06-20 15:02:01+00:00","url":"http://arxiv.org/abs/1206.5264v1"},{"x":"8.918558","y":"6.780952","title":"Towards A Rigorous Science of Interpretable Machine Learning","cluster":"0","author":"['Finale Doshi-Velez', 'Been Kim']","source":"alignment newsletter","tags":"[]","date":"2017-02-28 02:19:20+00:00","url":"http://arxiv.org/abs/1702.08608v2"},{"x":"7.6584234","y":"6.284476","title":"Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures","cluster":"2","author":"['Gongbo Tang', 'Mathias Müller', 'Annette Rios', 'Rico Sennrich']","source":"arxiv","tags":"[]","date":"2018-08-27 17:51:27+00:00","url":"http://arxiv.org/abs/1808.08946v3"},{"x":"10.027363","y":"6.996052","title":"Conservative Objective Models for Effective Offline Model-Based Optimization","cluster":"0","author":"['Brandon Trabucco', 'Aviral Kumar', 'Xinyang Geng', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2021-07-14 17:55:28+00:00","url":"http://arxiv.org/abs/2107.06882v1"},{"x":"11.3116045","y":"5.800724","title":"IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL","cluster":"0","author":"['Nirbhay Modhe', 'Prithvijit Chattopadhyay', 'Mohit Sharma', 'Abhishek Das', 'Devi Parikh', 'Dhruv Batra', 'Ramakrishna Vedantam']","source":"arxiv","tags":"[]","date":"2019-07-24 17:30:39+00:00","url":"http://arxiv.org/abs/1907.10580v5"},{"x":"10.956475","y":"5.6709275","title":"Learning Latent Dynamics for Planning from Pixels","cluster":"0","author":"['Danijar Hafner', 'Timothy Lillicrap', 'Ian Fischer', 'Ruben Villegas', 'David Ha', 'Honglak Lee', 'James Davidson']","source":"alignment newsletter","tags":"[]","date":"2018-11-12 04:30:10+00:00","url":"http://arxiv.org/abs/1811.04551v5"},{"x":"8.920524","y":"7.003711","title":"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?","cluster":"1","author":"['Peter Hase', 'Mohit Bansal']","source":"arxiv","tags":"[]","date":"2020-05-04 20:35:17+00:00","url":"http://arxiv.org/abs/2005.01831v1"},{"x":"7.889513","y":"5.388263","title":"Deeper Interpretability of Deep Networks","cluster":"2","author":"['Tian Xu', 'Jiayu Zhan', 'Oliver G. B. Garrod', 'Philip H. S. Torr', 'Song-Chun Zhu', 'Robin A. A. Ince', 'Philippe G. Schyns']","source":"arxiv","tags":"[]","date":"2018-11-19 17:10:44+00:00","url":"http://arxiv.org/abs/1811.07807v2"},{"x":"8.139905","y":"5.714526","title":"Reconciling modern machine learning practice and the bias-variance trade-off","cluster":"2","author":"['Mikhail Belkin', 'Daniel Hsu', 'Siyuan Ma', 'Soumik Mandal']","source":"alignment newsletter","tags":"[]","date":"2018-12-28 17:15:38+00:00","url":"http://arxiv.org/abs/1812.11118v2"},{"x":"11.156717","y":"6.367295","title":"Interpretable Reinforcement Learning with Ensemble Methods","cluster":"0","author":"['Alexander Brown', 'Marek Petrik']","source":"arxiv","tags":"[]","date":"2018-09-19 03:23:35+00:00","url":"http://arxiv.org/abs/1809.06995v1"},{"x":"7.456843","y":"6.5477614","title":"BERT-ATTACK: Adversarial Attack Against BERT Using BERT","cluster":"2","author":"['Linyang Li', 'Ruotian Ma', 'Qipeng Guo', 'Xiangyang Xue', 'Xipeng Qiu']","source":"arxiv","tags":"[]","date":"2020-04-21 13:30:02+00:00","url":"http://arxiv.org/abs/2004.09984v3"},{"x":"11.671463","y":"8.875253","title":"Agent Incentives: A Causal Perspective","cluster":"1","author":"['Tom Everitt', 'Ryan Carey', 'Eric Langlois', 'Pedro A Ortega', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2021-02-02 18:52:41+00:00","url":"http://arxiv.org/abs/2102.01685v2"},{"x":"8.40928","y":"9.75796","title":"AI and Shared Prosperity","cluster":"3","author":"['Katya Klinova', 'Anton Korinek']","source":"arxiv","tags":"[]","date":"2021-05-18 12:37:18+00:00","url":"http://arxiv.org/abs/2105.08475v1"},{"x":"10.958519","y":"5.811999","title":"What Matters for Adversarial Imitation Learning?","cluster":"0","author":"['Manu Orsini', 'Anton Raichuk', 'Léonard Hussenot', 'Damien Vincent', 'Robert Dadashi', 'Sertan Girgin', 'Matthieu Geist', 'Olivier Bachem', 'Olivier Pietquin', 'Marcin Andrychowicz']","source":"alignment newsletter","tags":"[]","date":"2021-06-01 17:58:08+00:00","url":"http://arxiv.org/abs/2106.00672v1"},{"x":"7.5511804","y":"6.7152467","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","cluster":"2","author":"['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova']","source":"arxiv","tags":"[]","date":"2018-10-11 00:50:01+00:00","url":"http://arxiv.org/abs/1810.04805v2"},{"x":"8.482623","y":"6.0641174","title":"Unsupervised Learning of Neural Networks to Explain Neural Networks","cluster":"2","author":"['Quanshi Zhang', 'Yu Yang', 'Yuchen Liu', 'Ying Nian Wu', 'Song-Chun Zhu']","source":"arxiv","tags":"[]","date":"2018-05-18 23:02:14+00:00","url":"http://arxiv.org/abs/1805.07468v1"},{"x":"8.209797","y":"5.32172","title":"Likelihood Ratios for Out-of-Distribution Detection","cluster":"2","author":"['Jie Ren', 'Peter J. Liu', 'Emily Fertig', 'Jasper Snoek', 'Ryan Poplin', 'Mark A. DePristo', 'Joshua V. Dillon', 'Balaji Lakshminarayanan']","source":"arxiv","tags":"[]","date":"2019-06-07 00:01:42+00:00","url":"http://arxiv.org/abs/1906.02845v2"},{"x":"10.542786","y":"9.343514","title":"Agents and Devices: A Relative Definition of Agency","cluster":"1","author":"['Laurent Orseau', 'Simon McGregor McGill', 'Shane Legg']","source":"alignment newsletter","tags":"[]","date":"2018-05-31 09:12:14+00:00","url":"http://arxiv.org/abs/1805.12387v1"},{"x":"11.187119","y":"6.3931017","title":"Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning","cluster":"0","author":"['David Janz', 'Jiri Hron', 'Przemysław Mazur', 'Katja Hofmann', 'José Miguel Hernández-Lobato', 'Sebastian Tschiatschek']","source":"arxiv","tags":"[]","date":"2018-10-15 17:30:53+00:00","url":"http://arxiv.org/abs/1810.06530v5"},{"x":"8.420027","y":"5.691816","title":"Meta-Learning Update Rules for Unsupervised Representation Learning","cluster":"2","author":"['Luke Metz', 'Niru Maheswaranathan', 'Brian Cheung', 'Jascha Sohl-Dickstein']","source":"arxiv","tags":"[]","date":"2018-03-31 22:44:28+00:00","url":"http://arxiv.org/abs/1804.00222v3"},{"x":"9.7541275","y":"8.466321","title":"The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities","cluster":"1","author":"['Joel Lehman', 'Jeff Clune', 'Dusan Misevic', 'Christoph Adami', 'Lee Altenberg', 'Julie Beaulieu', 'Peter J. Bentley', 'Samuel Bernard', 'Guillaume Beslon', 'David M. Bryson', 'Patryk Chrabaszcz', 'Nick Cheney', 'Antoine Cully', 'Stephane Doncieux', 'Fred C. Dyer', 'Kai Olav Ellefsen', 'Robert Feldt', 'Stephan Fischer', 'Stephanie Forrest', 'Antoine Frénoy', 'Christian Gagné', 'Leni Le Goff', 'Laura M. Grabowski', 'Babak Hodjat', 'Frank Hutter', 'Laurent Keller', 'Carole Knibbe', 'Peter Krcah', 'Richard E. Lenski', 'Hod Lipson', 'Robert MacCurdy', 'Carlos Maestre', 'Risto Miikkulainen', 'Sara Mitri', 'David E. Moriarty', 'Jean-Baptiste Mouret', 'Anh Nguyen', 'Charles Ofria', 'Marc Parizeau', 'David Parsons', 'Robert T. Pennock', 'William F. Punch', 'Thomas S. Ray', 'Marc Schoenauer', 'Eric Shulte', 'Karl Sims', 'Kenneth O. Stanley', 'François Taddei', 'Danesh Tarapore', 'Simon Thibault', 'Westley Weimer', 'Richard Watson', 'Jason Yosinski']","source":"arxiv","tags":"[]","date":"2018-03-09 10:17:18+00:00","url":"http://arxiv.org/abs/1803.03453v4"},{"x":"8.823812","y":"6.452749","title":"Neural Network Quine","cluster":"2","author":"['Oscar Chang', 'Hod Lipson']","source":"arxiv","tags":"[]","date":"2018-03-15 16:54:43+00:00","url":"http://arxiv.org/abs/1803.05859v4"},{"x":"8.492973","y":"4.927879","title":"Certified Adversarial Defenses Meet Out-of-Distribution Corruptions: Benchmarking Robustness and Simple Baselines","cluster":"2","author":"['Jiachen Sun', 'Akshay Mehra', 'Bhavya Kailkhura', 'Pin-Yu Chen', 'Dan Hendrycks', 'Jihun Hamm', 'Z. Morley Mao']","source":"arxiv","tags":"[]","date":"2021-12-01 17:11:22+00:00","url":"http://arxiv.org/abs/2112.00659v1"},{"x":"11.2133465","y":"6.085933","title":"Learning Robust Rewards with Adversarial Inverse Reinforcement Learning","cluster":"0","author":"['Justin Fu', 'Katie Luo', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2017-10-30 21:22:28+00:00","url":"http://arxiv.org/abs/1710.11248v2"},{"x":"8.671762","y":"5.192624","title":"Training verified learners with learned verifiers","cluster":"2","author":"['Krishnamurthy Dvijotham', 'Sven Gowal', 'Robert Stanforth', 'Relja Arandjelovic', \"Brendan O'Donoghue\", 'Jonathan Uesato', 'Pushmeet Kohli']","source":"arxiv","tags":"[]","date":"2018-05-25 17:35:39+00:00","url":"http://arxiv.org/abs/1805.10265v2"},{"x":"11.121378","y":"6.3993936","title":"Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning","cluster":"0","author":"['Stephanie Milani', 'Nicholay Topin', 'Brandon Houghton', 'William H. Guss', 'Sharada P. Mohanty', 'Keisuke Nakata', 'Oriol Vinyals', 'Noboru Sean Kuno']","source":"alignment newsletter","tags":"[]","date":"2020-03-10 21:39:52+00:00","url":"http://arxiv.org/abs/2003.05012v4"},{"x":"12.350553","y":"9.770517","title":"Forecasting using incomplete models","cluster":"1","author":"['Vanessa Kosoy']","source":"arxiv","tags":"[]","date":"2017-05-12 15:38:57+00:00","url":"http://arxiv.org/abs/1705.04630v6"},{"x":"11.491474","y":"8.793015","title":"Asking the Right Questions: Learning Interpretable Action Models Through Query Answering","cluster":"1","author":"['Pulkit Verma', 'Shashank Rao Marpally', 'Siddharth Srivastava']","source":"arxiv","tags":"[]","date":"2019-12-29 09:05:06+00:00","url":"http://arxiv.org/abs/1912.12613v6"},{"x":"10.936084","y":"7.903515","title":"Identifying and Correcting Label Bias in Machine Learning","cluster":"2","author":"['Heinrich Jiang', 'Ofir Nachum']","source":"arxiv","tags":"[]","date":"2019-01-15 18:40:06+00:00","url":"http://arxiv.org/abs/1901.04966v1"},{"x":"10.2906275","y":"5.731629","title":"Playing Atari with Deep Reinforcement Learning","cluster":"0","author":"['Volodymyr Mnih', 'Koray Kavukcuoglu', 'David Silver', 'Alex Graves', 'Ioannis Antonoglou', 'Daan Wierstra', 'Martin Riedmiller']","source":"arxiv","tags":"[]","date":"2013-12-19 16:00:08+00:00","url":"http://arxiv.org/abs/1312.5602v1"},{"x":"10.340383","y":"5.743469","title":"Relational inductive bias for physical construction in humans and machines","cluster":"0","author":"['Jessica B. Hamrick', 'Kelsey R. Allen', 'Victor Bapst', 'Tina Zhu', 'Kevin R. McKee', 'Joshua B. Tenenbaum', 'Peter W. Battaglia']","source":"arxiv","tags":"[]","date":"2018-06-04 16:45:19+00:00","url":"http://arxiv.org/abs/1806.01203v1"},{"x":"7.5960426","y":"6.930131","title":"Analyzing Dynamic Adversarial Training Data in the Limit","cluster":"2","author":"['Eric Wallace', 'Adina Williams', 'Robin Jia', 'Douwe Kiela']","source":"arxiv","tags":"[]","date":"2021-10-16 08:48:52+00:00","url":"http://arxiv.org/abs/2110.08514v1"},{"x":"11.2462635","y":"6.76624","title":"Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning","cluster":"0","author":"['El Mahdi El Mhamdi', 'Rachid Guerraoui', 'Hadrien Hendrikx', 'Alexandre Maurer']","source":"arxiv","tags":"[]","date":"2017-04-10 14:38:37+00:00","url":"http://arxiv.org/abs/1704.02882v2"},{"x":"7.631321","y":"6.8264112","title":"Finetuned Language Models Are Zero-Shot Learners","cluster":"2","author":"['Jason Wei', 'Maarten Bosma', 'Vincent Y. Zhao', 'Kelvin Guu', 'Adams Wei Yu', 'Brian Lester', 'Nan Du', 'Andrew M. Dai', 'Quoc V. Le']","source":"alignment newsletter","tags":"[]","date":"2021-09-03 17:55:52+00:00","url":"http://arxiv.org/abs/2109.01652v5"},{"x":"8.453273","y":"4.9703045","title":"Towards the first adversarially robust neural network model on MNIST","cluster":"2","author":"['Lukas Schott', 'Jonas Rauber', 'Matthias Bethge', 'Wieland Brendel']","source":"alignment newsletter","tags":"[]","date":"2018-05-23 14:16:22+00:00","url":"http://arxiv.org/abs/1805.09190v3"},{"x":"11.134099","y":"7.10962","title":"Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions","cluster":"0","author":"['Michael Chang', 'Sidhant Kaushik', 'S. Matthew Weinberg', 'Thomas L. Griffiths', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2020-07-05 16:41:09+00:00","url":"http://arxiv.org/abs/2007.02382v2"},{"x":"8.492786","y":"5.0362196","title":"First-order Adversarial Vulnerability of Neural Networks and Input Dimension","cluster":"2","author":"['Carl-Johann Simon-Gabriel', 'Yann Ollivier', 'Léon Bottou', 'Bernhard Schölkopf', 'David Lopez-Paz']","source":"alignment newsletter","tags":"[]","date":"2018-02-05 14:36:44+00:00","url":"http://arxiv.org/abs/1802.01421v4"},{"x":"8.213592","y":"5.801737","title":"Gradient Descent: The Ultimate Optimizer","cluster":"2","author":"['Kartik Chandra', 'Erik Meijer', 'Samantha Andow', 'Emilio Arroyo-Fang', 'Irene Dea', 'Johann George', 'Melissa Grueter', 'Basil Hosmer', 'Steffi Stumpos', 'Alanna Tempest', 'Shannon Yang']","source":"alignment newsletter","tags":"[]","date":"2019-09-29 21:41:49+00:00","url":"http://arxiv.org/abs/1909.13371v1"},{"x":"8.264999","y":"8.75032","title":"A narrowing of AI research?","cluster":"3","author":"['Joel Klinger', 'Juan Mateos-Garcia', 'Konstantinos Stathoulopoulos']","source":"alignment newsletter","tags":"[]","date":"2020-09-22 08:23:56+00:00","url":"http://arxiv.org/abs/2009.10385v4"},{"x":"10.5644865","y":"6.5535355","title":"AI Safety Gridworlds","cluster":"0","author":"['Jan Leike', 'Miljan Martic', 'Victoria Krakovna', 'Pedro A. Ortega', 'Tom Everitt', 'Andrew Lefrancq', 'Laurent Orseau', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2017-11-27 18:57:13+00:00","url":"http://arxiv.org/abs/1711.09883v2"},{"x":"7.5319963","y":"6.633531","title":"Language Models are Few-Shot Learners","cluster":"2","author":"['Tom B. Brown', 'Benjamin Mann', 'Nick Ryder', 'Melanie Subbiah', 'Jared Kaplan', 'Prafulla Dhariwal', 'Arvind Neelakantan', 'Pranav Shyam', 'Girish Sastry', 'Amanda Askell', 'Sandhini Agarwal', 'Ariel Herbert-Voss', 'Gretchen Krueger', 'Tom Henighan', 'Rewon Child', 'Aditya Ramesh', 'Daniel M. Ziegler', 'Jeffrey Wu', 'Clemens Winter', 'Christopher Hesse', 'Mark Chen', 'Eric Sigler', 'Mateusz Litwin', 'Scott Gray', 'Benjamin Chess', 'Jack Clark', 'Christopher Berner', 'Sam McCandlish', 'Alec Radford', 'Ilya Sutskever', 'Dario Amodei']","source":"alignment newsletter","tags":"[]","date":"2020-05-28 17:29:03+00:00","url":"http://arxiv.org/abs/2005.14165v4"},{"x":"10.963568","y":"5.656008","title":"VILD: Variational Imitation Learning with Diverse-quality Demonstrations","cluster":"0","author":"['Voot Tangkaratt', 'Bo Han', 'Mohammad Emtiyaz Khan', 'Masashi Sugiyama']","source":"alignment newsletter","tags":"[]","date":"2019-09-15 09:42:33+00:00","url":"http://arxiv.org/abs/1909.06769v1"},{"x":"9.294262","y":"8.040768","title":"A Model for General Intelligence","cluster":"4","author":"['Paul Yaworsky']","source":"arxiv","tags":"[]","date":"2018-11-06 18:37:04+00:00","url":"http://arxiv.org/abs/1811.02546v2"},{"x":"11.610432","y":"7.1399746","title":"A Strongly Asymptotically Optimal Agent in General Environments","cluster":"0","author":"['Michael K. Cohen', 'Elliot Catt', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2019-03-04 00:02:58+00:00","url":"http://arxiv.org/abs/1903.01021v2"},{"x":"11.210328","y":"8.4734535","title":"Predicting human decisions with behavioral theories and machine learning","cluster":"1","author":"['Ori Plonsky', 'Reut Apel', 'Eyal Ert', 'Moshe Tennenholtz', 'David Bourgin', 'Joshua C. Peterson', 'Daniel Reichman', 'Thomas L. Griffiths', 'Stuart J. Russell', 'Evan C. Carter', 'James F. Cavanagh', 'Ido Erev']","source":"arxiv","tags":"[]","date":"2019-04-15 06:12:44+00:00","url":"http://arxiv.org/abs/1904.06866v1"},{"x":"8.180107","y":"5.022062","title":"Data Augmentation Can Improve Robustness","cluster":"2","author":"['Sylvestre-Alvise Rebuffi', 'Sven Gowal', 'Dan A. Calian', 'Florian Stimberg', 'Olivia Wiles', 'Timothy Mann']","source":"arxiv","tags":"[]","date":"2021-11-09 18:57:00+00:00","url":"http://arxiv.org/abs/2111.05328v1"},{"x":"11.575504","y":"6.0364676","title":"PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation","cluster":"0","author":"['Perttu Hämäläinen', 'Amin Babadi', 'Xiaoxiao Ma', 'Jaakko Lehtinen']","source":"arxiv","tags":"[]","date":"2018-10-05 06:59:29+00:00","url":"http://arxiv.org/abs/1810.02541v9"},{"x":"8.322363","y":"7.6833305","title":"The 30-Year Cycle In The AI Debate","cluster":"2","author":"['Jean-Marie Chauvet']","source":"arxiv","tags":"[]","date":"2018-10-08 16:35:06+00:00","url":"http://arxiv.org/abs/1810.04053v1"},{"x":"7.6530237","y":"6.33982","title":"Neural Machine Translation by Jointly Learning to Align and Translate","cluster":"2","author":"['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio']","source":"arxiv","tags":"[]","date":"2014-09-01 16:33:02+00:00","url":"http://arxiv.org/abs/1409.0473v7"},{"x":"8.555648","y":"5.6843","title":"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask","cluster":"2","author":"['Hattie Zhou', 'Janice Lan', 'Rosanne Liu', 'Jason Yosinski']","source":"arxiv","tags":"[]","date":"2019-05-03 08:21:07+00:00","url":"http://arxiv.org/abs/1905.01067v4"},{"x":"11.059157","y":"6.7471476","title":"Scalable agent alignment via reward modeling: a research direction","cluster":"0","author":"['Jan Leike', 'David Krueger', 'Tom Everitt', 'Miljan Martic', 'Vishal Maini', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2018-11-19 18:48:04+00:00","url":"http://arxiv.org/abs/1811.07871v1"},{"x":"9.314377","y":"8.864795","title":"Stakeholders in Explainable AI","cluster":"4","author":"['Alun Preece', 'Dan Harborne', 'Dave Braines', 'Richard Tomsett', 'Supriyo Chakraborty']","source":"alignment newsletter","tags":"[]","date":"2018-09-29 10:15:18+00:00","url":"http://arxiv.org/abs/1810.00184v1"},{"x":"11.607147","y":"6.698331","title":"Reinforcement Learning with a Corrupted Reward Channel","cluster":"0","author":"['Tom Everitt', 'Victoria Krakovna', 'Laurent Orseau', 'Marcus Hutter', 'Shane Legg']","source":"arxiv","tags":"[]","date":"2017-05-23 17:06:56+00:00","url":"http://arxiv.org/abs/1705.08417v2"},{"x":"11.697615","y":"5.5904765","title":"Expressing Robot Incapability","cluster":"0","author":"['Minae Kwon', 'Sandy H. Huang', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2018-10-18 17:14:04+00:00","url":"http://arxiv.org/abs/1810.08167v2"},{"x":"11.61938","y":"6.15891","title":"Human irrationality: both bad and good for reward inference","cluster":"0","author":"['Lawrence Chan', 'Andrew Critch', 'Anca Dragan']","source":"alignment newsletter","tags":"[]","date":"2021-11-12 21:44:15+00:00","url":"http://arxiv.org/abs/2111.06956v1"},{"x":"11.304596","y":"6.5074043","title":"Constrained Policy Improvement for Safe and Efficient Reinforcement Learning","cluster":"0","author":"['Elad Sarafian', 'Aviv Tamar', 'Sarit Kraus']","source":"arxiv","tags":"[]","date":"2018-05-20 17:47:03+00:00","url":"http://arxiv.org/abs/1805.07805v3"},{"x":"8.8982115","y":"6.119561","title":"A Marauder's Map of Security and Privacy in Machine Learning","cluster":"2","author":"['Nicolas Papernot']","source":"arxiv","tags":"[]","date":"2018-11-03 00:25:50+00:00","url":"http://arxiv.org/abs/1811.01134v1"},{"x":"7.805639","y":"5.4298353","title":"A Simple Framework for Contrastive Learning of Visual Representations","cluster":"2","author":"['Ting Chen', 'Simon Kornblith', 'Mohammad Norouzi', 'Geoffrey Hinton']","source":"alignment newsletter","tags":"[]","date":"2020-02-13 18:50:45+00:00","url":"http://arxiv.org/abs/2002.05709v3"},{"x":"7.552982","y":"6.2193513","title":"Zero-Shot Text-to-Image Generation","cluster":"2","author":"['Aditya Ramesh', 'Mikhail Pavlov', 'Gabriel Goh', 'Scott Gray', 'Chelsea Voss', 'Alec Radford', 'Mark Chen', 'Ilya Sutskever']","source":"arxiv","tags":"[]","date":"2021-02-24 06:42:31+00:00","url":"http://arxiv.org/abs/2102.12092v2"},{"x":"8.4481325","y":"5.035639","title":"Certifying Model Accuracy under Distribution Shifts","cluster":"2","author":"['Aounon Kumar', 'Alexander Levine', 'Tom Goldstein', 'Soheil Feizi']","source":"arxiv","tags":"[]","date":"2022-01-28 22:03:50+00:00","url":"http://arxiv.org/abs/2201.12440v1"},{"x":"10.872766","y":"6.053563","title":"DERAIL: Diagnostic Environments for Reward And Imitation Learning","cluster":"0","author":"['Pedro Freire', 'Adam Gleave', 'Sam Toyer', 'Stuart Russell']","source":"alignment newsletter","tags":"[]","date":"2020-12-02 18:07:09+00:00","url":"http://arxiv.org/abs/2012.01365v1"},{"x":"8.623361","y":"5.1884646","title":"On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models","cluster":"2","author":"['Sven Gowal', 'Krishnamurthy Dvijotham', 'Robert Stanforth', 'Rudy Bunel', 'Chongli Qin', 'Jonathan Uesato', 'Relja Arandjelovic', 'Timothy Mann', 'Pushmeet Kohli']","source":"arxiv","tags":"[]","date":"2018-10-30 13:12:47+00:00","url":"http://arxiv.org/abs/1810.12715v4"},{"x":"11.343947","y":"6.0710263","title":"Multitask Soft Option Learning","cluster":"0","author":"['Maximilian Igl', 'Andrew Gambardella', 'Jinke He', 'Nantas Nardelli', 'N. Siddharth', 'Wendelin Böhmer', 'Shimon Whiteson']","source":"alignment newsletter","tags":"[]","date":"2019-04-01 18:01:34+00:00","url":"http://arxiv.org/abs/1904.01033v3"},{"x":"10.333284","y":"5.6643233","title":"Generating Multi-Agent Trajectories using Programmatic Weak Supervision","cluster":"0","author":"['Eric Zhan', 'Stephan Zheng', 'Yisong Yue', 'Long Sha', 'Patrick Lucey']","source":"arxiv","tags":"[]","date":"2018-03-20 19:19:13+00:00","url":"http://arxiv.org/abs/1803.07612v6"},{"x":"11.386213","y":"6.065743","title":"Reinforcement Learning with Perturbed Rewards","cluster":"0","author":"['Jingkang Wang', 'Yang Liu', 'Bo Li']","source":"arxiv","tags":"[]","date":"2018-10-02 01:43:45+00:00","url":"http://arxiv.org/abs/1810.01032v4"},{"x":"10.44445","y":"6.129219","title":"Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning","cluster":"0","author":"['Michael Everett', 'Bjorn Lutjens', 'Jonathan P. How']","source":"alignment newsletter","tags":"[]","date":"2020-04-11 21:36:13+00:00","url":"http://arxiv.org/abs/2004.06496v6"},{"x":"12.1635685","y":"8.342368","title":"A Game-Theoretic Approach for Hierarchical Policy-Making","cluster":"1","author":"['Feiran Jia', 'Aditya Mate', 'Zun Li', 'Shahin Jabbari', 'Mithun Chakraborty', 'Milind Tambe', 'Michael Wellman', 'Yevgeniy Vorobeychik']","source":"arxiv","tags":"[]","date":"2021-02-21 17:01:34+00:00","url":"http://arxiv.org/abs/2102.10646v1"},{"x":"10.425191","y":"6.157907","title":"Combining Deep Reinforcement Learning and Search for Imperfect-Information Games","cluster":"0","author":"['Noam Brown', 'Anton Bakhtin', 'Adam Lerer', 'Qucheng Gong']","source":"alignment newsletter","tags":"[]","date":"2020-07-27 15:21:22+00:00","url":"http://arxiv.org/abs/2007.13544v2"},{"x":"7.8639793","y":"9.613519","title":"The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity","cluster":"3","author":"['Mohamed Abdalla', 'Moustafa Abdalla']","source":"alignment newsletter","tags":"[]","date":"2020-09-28 23:00:49+00:00","url":"http://arxiv.org/abs/2009.13676v4"},{"x":"11.008571","y":"7.2142224","title":"Self-Modification of Policy and Utility Function in Rational Agents","cluster":"0","author":"['Tom Everitt', 'Daniel Filan', 'Mayank Daswani', 'Marcus Hutter']","source":"arxiv","tags":"[]","date":"2016-05-10 18:25:49+00:00","url":"http://arxiv.org/abs/1605.03142v1"},{"x":"11.233974","y":"6.085488","title":"Lyapunov-based Safe Policy Optimization for Continuous Control","cluster":"0","author":"['Yinlam Chow', 'Ofir Nachum', 'Aleksandra Faust', 'Edgar Duenez-Guzman', 'Mohammad Ghavamzadeh']","source":"arxiv","tags":"[]","date":"2019-01-28 23:14:58+00:00","url":"http://arxiv.org/abs/1901.10031v2"},{"x":"10.443159","y":"5.9633384","title":"A Narration-based Reward Shaping Approach using Grounded Natural Language Commands","cluster":"0","author":"['Nicholas Waytowich', 'Sean L. Barton', 'Vernon Lawhern', 'Garrett Warnell']","source":"alignment newsletter","tags":"[]","date":"2019-10-31 22:37:54+00:00","url":"http://arxiv.org/abs/1911.00497v1"},{"x":"7.943801","y":"5.479925","title":"Big Self-Supervised Models are Strong Semi-Supervised Learners","cluster":"2","author":"['Ting Chen', 'Simon Kornblith', 'Kevin Swersky', 'Mohammad Norouzi', 'Geoffrey Hinton']","source":"alignment newsletter","tags":"[]","date":"2020-06-17 17:48:22+00:00","url":"http://arxiv.org/abs/2006.10029v2"},{"x":"11.717275","y":"5.93432","title":"Probabilistically Safe Robot Planning with Confidence-Based Human Predictions","cluster":"0","author":"['Jaime F. Fisac', 'Andrea Bajcsy', 'Sylvia L. Herbert', 'David Fridovich-Keil', 'Steven Wang', 'Claire J. Tomlin', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2018-05-31 21:47:34+00:00","url":"http://arxiv.org/abs/1806.00109v1"},{"x":"11.740209","y":"5.9572988","title":"A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning","cluster":"0","author":"['Somil Bansal', 'Andrea Bajcsy', 'Ellis Ratner', 'Anca D. Dragan', 'Claire J. Tomlin']","source":"arxiv","tags":"[]","date":"2019-10-29 16:34:00+00:00","url":"http://arxiv.org/abs/1910.13369v2"},{"x":"9.835324","y":"6.9941955","title":"Oversight of Unsafe Systems via Dynamic Safety Envelopes","cluster":"1","author":"['David Manheim']","source":"arxiv","tags":"[]","date":"2018-11-22 17:31:41+00:00","url":"http://arxiv.org/abs/1811.09246v1"},{"x":"8.22885","y":"7.1980176","title":"Finding Generalizable Evidence by Learning to Convince Q&A Models","cluster":"2","author":"['Ethan Perez', 'Siddharth Karamcheti', 'Rob Fergus', 'Jason Weston', 'Douwe Kiela', 'Kyunghyun Cho']","source":"alignment newsletter","tags":"[]","date":"2019-09-12 18:00:00+00:00","url":"http://arxiv.org/abs/1909.05863v1"},{"x":"7.7560782","y":"5.2313266","title":"What Makes for Good Views for Contrastive Learning?","cluster":"2","author":"['Yonglong Tian', 'Chen Sun', 'Ben Poole', 'Dilip Krishnan', 'Cordelia Schmid', 'Phillip Isola']","source":"arxiv","tags":"[]","date":"2020-05-20 17:59:57+00:00","url":"http://arxiv.org/abs/2005.10243v3"},{"x":"8.0029125","y":"5.7687516","title":"Attentive Neural Processes","cluster":"2","author":"['Hyunjik Kim', 'Andriy Mnih', 'Jonathan Schwarz', 'Marta Garnelo', 'Ali Eslami', 'Dan Rosenbaum', 'Oriol Vinyals', 'Yee Whye Teh']","source":"arxiv","tags":"[]","date":"2019-01-17 12:37:26+00:00","url":"http://arxiv.org/abs/1901.05761v2"},{"x":"9.192541","y":"8.6358595","title":"Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity","cluster":"4","author":"['Adrien Ecoffet', 'Jeff Clune', 'Joel Lehman']","source":"alignment newsletter","tags":"[]","date":"2020-06-12 22:28:09+00:00","url":"http://arxiv.org/abs/2006.07495v1"},{"x":"10.179492","y":"7.6344695","title":"Penalizing side effects using stepwise relative reachability","cluster":"4","author":"['Victoria Krakovna', 'Laurent Orseau', 'Ramana Kumar', 'Miljan Martic', 'Shane Legg']","source":"alignment newsletter","tags":"[]","date":"2018-06-04 16:30:17+00:00","url":"http://arxiv.org/abs/1806.01186v2"},{"x":"9.633143","y":"6.1996045","title":"Evolving simple programs for playing Atari games","cluster":"2","author":"['Dennis G Wilson', 'Sylvain Cussat-Blanc', 'Hervé Luga', 'Julian F Miller']","source":"arxiv","tags":"[]","date":"2018-06-14 18:10:46+00:00","url":"http://arxiv.org/abs/1806.05695v1"},{"x":"11.180561","y":"5.9358797","title":"Safe Reinforcement Learning with Model Uncertainty Estimates","cluster":"0","author":"['Björn Lütjens', 'Michael Everett', 'Jonathan P. How']","source":"arxiv","tags":"[]","date":"2018-10-19 22:04:59+00:00","url":"http://arxiv.org/abs/1810.08700v2"},{"x":"11.21502","y":"8.678349","title":"What are you optimizing for? Aligning Recommender Systems with Human Values","cluster":"1","author":"['Jonathan Stray', 'Ivan Vendrov', 'Jeremy Nixon', 'Steven Adler', 'Dylan Hadfield-Menell']","source":"arxiv","tags":"[]","date":"2021-07-22 21:52:43+00:00","url":"http://arxiv.org/abs/2107.10939v1"},{"x":"8.375122","y":"9.244114","title":"AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks","cluster":"3","author":"['McKane Andrus', 'Sarah Dean', 'Thomas Krendl Gilbert', 'Nathan Lambert', 'Tom Zick']","source":"arxiv","tags":"[]","date":"2021-02-04 18:54:20+00:00","url":"http://arxiv.org/abs/2102.04255v1"},{"x":"8.466412","y":"5.001493","title":"Adversarial Reprogramming of Neural Networks","cluster":"2","author":"['Gamaleldin F. Elsayed', 'Ian Goodfellow', 'Jascha Sohl-Dickstein']","source":"arxiv","tags":"[]","date":"2018-06-28 19:06:26+00:00","url":"http://arxiv.org/abs/1806.11146v2"},{"x":"8.566704","y":"5.727468","title":"Stabilizing the Lottery Ticket Hypothesis","cluster":"2","author":"['Jonathan Frankle', 'Gintare Karolina Dziugaite', 'Daniel M. Roy', 'Michael Carbin']","source":"alignment newsletter","tags":"[]","date":"2019-03-05 00:52:12+00:00","url":"http://arxiv.org/abs/1903.01611v3"},{"x":"10.671812","y":"7.7885056","title":"Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction","cluster":"2","author":"['Jacob Steinhardt', 'Gregory Valiant', 'Moses Charikar']","source":"arxiv","tags":"[]","date":"2016-06-16 21:45:14+00:00","url":"http://arxiv.org/abs/1606.05374v1"},{"x":"7.816565","y":"5.528389","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","cluster":"2","author":"['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov', 'Dirk Weissenborn', 'Xiaohua Zhai', 'Thomas Unterthiner', 'Mostafa Dehghani', 'Matthias Minderer', 'Georg Heigold', 'Sylvain Gelly', 'Jakob Uszkoreit', 'Neil Houlsby']","source":"arxiv","tags":"[]","date":"2020-10-22 17:55:59+00:00","url":"http://arxiv.org/abs/2010.11929v2"},{"x":"8.440138","y":"5.7402477","title":"Deconstructing Distributions: A Pointwise Framework of Learning","cluster":"2","author":"['Gal Kaplun', 'Nikhil Ghosh', 'Saurabh Garg', 'Boaz Barak', 'Preetum Nakkiran']","source":"arxiv","tags":"[]","date":"2022-02-20 23:25:28+00:00","url":"http://arxiv.org/abs/2202.09931v1"},{"x":"10.465591","y":"6.4481125","title":"Reinforcement Learning under Threats","cluster":"0","author":"['Victor Gallego', 'Roi Naveiro', 'David Rios Insua']","source":"alignment newsletter","tags":"[]","date":"2018-09-05 14:56:09+00:00","url":"http://arxiv.org/abs/1809.01560v2"},{"x":"8.052812","y":"5.504759","title":"Cold Case: The Lost MNIST Digits","cluster":"2","author":"['Chhavi Yadav', 'Léon Bottou']","source":"alignment newsletter","tags":"[]","date":"2019-05-25 01:50:51+00:00","url":"http://arxiv.org/abs/1905.10498v2"},{"x":"7.6643505","y":"6.6830335","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","cluster":"2","author":"['Brian Lester', 'Rami Al-Rfou', 'Noah Constant']","source":"alignment newsletter","tags":"[]","date":"2021-04-18 03:19:26+00:00","url":"http://arxiv.org/abs/2104.08691v2"},{"x":"11.234945","y":"6.0441008","title":"Generative Exploration and Exploitation","cluster":"0","author":"['Jiechuan Jiang', 'Zongqing Lu']","source":"arxiv","tags":"[]","date":"2019-04-21 14:15:24+00:00","url":"http://arxiv.org/abs/1904.09605v2"},{"x":"11.557006","y":"6.204856","title":"Generalized Hindsight for Reinforcement Learning","cluster":"0","author":"['Alexander C. Li', 'Lerrel Pinto', 'Pieter Abbeel']","source":"alignment newsletter","tags":"[]","date":"2020-02-26 18:57:05+00:00","url":"http://arxiv.org/abs/2002.11708v1"},{"x":"8.75677","y":"8.455643","title":"Concrete Problems in AI Safety","cluster":"4","author":"['Dario Amodei', 'Chris Olah', 'Jacob Steinhardt', 'Paul Christiano', 'John Schulman', 'Dan Mané']","source":"arxiv","tags":"[]","date":"2016-06-21 13:37:05+00:00","url":"http://arxiv.org/abs/1606.06565v2"},{"x":"8.0070305","y":"6.734471","title":"The Benchmark Lottery","cluster":"2","author":"['Mostafa Dehghani', 'Yi Tay', 'Alexey A. Gritsenko', 'Zhe Zhao', 'Neil Houlsby', 'Fernando Diaz', 'Donald Metzler', 'Oriol Vinyals']","source":"alignment newsletter","tags":"[]","date":"2021-07-14 21:08:30+00:00","url":"http://arxiv.org/abs/2107.07002v1"},{"x":"8.642591","y":"6.2456822","title":"Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations","cluster":"2","author":"['Xander Steenbrugge', 'Sam Leroux', 'Tim Verbelen', 'Bart Dhoedt']","source":"arxiv","tags":"[]","date":"2018-11-12 15:23:26+00:00","url":"http://arxiv.org/abs/1811.04784v1"},{"x":"7.6245246","y":"6.938122","title":"Measuring Massive Multitask Language Understanding","cluster":"2","author":"['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andy Zou', 'Mantas Mazeika', 'Dawn Song', 'Jacob Steinhardt']","source":"alignment newsletter","tags":"[]","date":"2020-09-07 17:59:25+00:00","url":"http://arxiv.org/abs/2009.03300v3"},{"x":"8.500189","y":"8.366795","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions","cluster":"3","author":"['Hammond Pearce', 'Baleegh Ahmad', 'Benjamin Tan', 'Brendan Dolan-Gavitt', 'Ramesh Karri']","source":"arxiv","tags":"[]","date":"2021-08-20 17:30:33+00:00","url":"http://arxiv.org/abs/2108.09293v3"},{"x":"7.9740157","y":"6.7917037","title":"Solving Probability and Statistics Problems by Program Synthesis","cluster":"2","author":"['Leonard Tang', 'Elizabeth Ke', 'Nikhil Singh', 'Nakul Verma', 'Iddo Drori']","source":"arxiv","tags":"[]","date":"2021-11-16 07:34:25+00:00","url":"http://arxiv.org/abs/2111.08267v1"},{"x":"9.31497","y":"5.4963303","title":"A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees","cluster":"0","author":"['Min Wu', 'Matthew Wicker', 'Wenjie Ruan', 'Xiaowei Huang', 'Marta Kwiatkowska']","source":"arxiv","tags":"[]","date":"2018-07-10 11:28:46+00:00","url":"http://arxiv.org/abs/1807.03571v2"},{"x":"11.421106","y":"6.283386","title":"Generative Temporal Difference Learning for Infinite-Horizon Prediction","cluster":"0","author":"['Michael Janner', 'Igor Mordatch', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2020-10-27 17:54:12+00:00","url":"http://arxiv.org/abs/2010.14496v4"},{"x":"9.810982","y":"6.2232985","title":"SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver","cluster":"2","author":"['Po-Wei Wang', 'Priya L. Donti', 'Bryan Wilder', 'Zico Kolter']","source":"alignment newsletter","tags":"[]","date":"2019-05-29 00:47:35+00:00","url":"http://arxiv.org/abs/1905.12149v1"},{"x":"10.968323","y":"5.951627","title":"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","cluster":"0","author":"['Rohin Shah', 'Noah Gundotra', 'Pieter Abbeel', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2019-06-23 18:41:31+00:00","url":"http://arxiv.org/abs/1906.09624v1"},{"x":"11.250347","y":"6.2474613","title":"Offline Reinforcement Learning as One Big Sequence Modeling Problem","cluster":"0","author":"['Michael Janner', 'Qiyang Li', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2021-06-03 17:58:51+00:00","url":"http://arxiv.org/abs/2106.02039v4"},{"x":"8.096017","y":"5.052644","title":"Reinforcement Learning with Augmented Data","cluster":"2","author":"['Michael Laskin', 'Kimin Lee', 'Adam Stooke', 'Lerrel Pinto', 'Pieter Abbeel', 'Aravind Srinivas']","source":"alignment newsletter","tags":"[]","date":"2020-04-30 17:35:32+00:00","url":"http://arxiv.org/abs/2004.14990v5"},{"x":"11.624872","y":"5.8933516","title":"Batch Active Preference-Based Learning of Reward Functions","cluster":"0","author":"['Erdem Bıyık', 'Dorsa Sadigh']","source":"arxiv","tags":"[]","date":"2018-10-10 00:02:55+00:00","url":"http://arxiv.org/abs/1810.04303v1"},{"x":"8.357087","y":"5.2398524","title":"On the Geometry of Adversarial Examples","cluster":"2","author":"['Marc Khoury', 'Dylan Hadfield-Menell']","source":"arxiv","tags":"[]","date":"2018-11-01 17:47:10+00:00","url":"http://arxiv.org/abs/1811.00525v2"},{"x":"11.653699","y":"5.652422","title":"LESS is More: Rethinking Probabilistic Models of Human Behavior","cluster":"0","author":"['Andreea Bobu', 'Dexter R. R. Scobee', 'Jaime F. Fisac', 'S. Shankar Sastry', 'Anca D. Dragan']","source":"arxiv","tags":"[]","date":"2020-01-13 18:59:01+00:00","url":"http://arxiv.org/abs/2001.04465v1"},{"x":"10.525321","y":"6.457248","title":"Towards Learning Multi-agent Negotiations via Self-Play","cluster":"0","author":"['Yichuan Charlie Tang']","source":"alignment newsletter","tags":"[]","date":"2020-01-28 08:37:33+00:00","url":"http://arxiv.org/abs/2001.10208v1"},{"x":"10.983774","y":"6.765223","title":"Variational Option Discovery Algorithms","cluster":"0","author":"['Joshua Achiam', 'Harrison Edwards', 'Dario Amodei', 'Pieter Abbeel']","source":"arxiv","tags":"[]","date":"2018-07-26 18:05:45+00:00","url":"http://arxiv.org/abs/1807.10299v1"},{"x":"10.258011","y":"5.9770145","title":"Mastering Complex Control in MOBA Games with Deep Reinforcement Learning","cluster":"0","author":"['Deheng Ye', 'Zhao Liu', 'Mingfei Sun', 'Bei Shi', 'Peilin Zhao', 'Hao Wu', 'Hongsheng Yu', 'Shaojie Yang', 'Xipeng Wu', 'Qingwei Guo', 'Qiaobo Chen', 'Yinyuting Yin', 'Hao Zhang', 'Tengfei Shi', 'Liang Wang', 'Qiang Fu', 'Wei Yang', 'Lanxiao Huang']","source":"arxiv","tags":"[]","date":"2019-12-20 09:56:50+00:00","url":"http://arxiv.org/abs/1912.09729v3"},{"x":"10.544352","y":"6.606739","title":"Causal Confusion in Imitation Learning","cluster":"0","author":"['Pim de Haan', 'Dinesh Jayaraman', 'Sergey Levine']","source":"arxiv","tags":"[]","date":"2019-05-28 17:56:19+00:00","url":"http://arxiv.org/abs/1905.11979v2"},{"x":"10.899524","y":"6.457855","title":"Formal Language Constraints for Markov Decision Processes","cluster":"0","author":"['Eleanor Quint', 'Dong Xu', 'Samuel Flint', 'Stephen Scott', 'Matthew Dwyer']","source":"arxiv","tags":"[]","date":"2019-10-02 16:45:23+00:00","url":"http://arxiv.org/abs/1910.01074v3"},{"x":"10.718557","y":"6.277488","title":"Risk-Sensitive Generative Adversarial Imitation Learning","cluster":"0","author":"['Jonathan Lacotte', 'Mohammad Ghavamzadeh', 'Yinlam Chow', 'Marco Pavone']","source":"arxiv","tags":"[]","date":"2018-08-13 21:08:46+00:00","url":"http://arxiv.org/abs/1808.04468v2"},{"x":"11.736337","y":"7.864885","title":"Theory of Minds: Understanding Behavior in Groups Through Inverse Planning","cluster":"1","author":"['Michael Shum', 'Max Kleiman-Weiner', 'Michael L. Littman', 'Joshua B. Tenenbaum']","source":"arxiv","tags":"[]","date":"2019-01-18 04:50:08+00:00","url":"http://arxiv.org/abs/1901.06085v1"},{"x":"11.048448","y":"5.452644","title":"Reward Learning from Narrated Demonstrations","cluster":"0","author":"['Hsiao-Yu Fish Tung', 'Adam W. Harley', 'Liang-Kang Huang', 'Katerina Fragkiadaki']","source":"arxiv","tags":"[]","date":"2018-04-27 21:26:08+00:00","url":"http://arxiv.org/abs/1804.10692v1"},{"x":"10.78445","y":"5.808175","title":"Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels","cluster":"0","author":"['Ilya Kostrikov', 'Denis Yarats', 'Rob Fergus']","source":"alignment newsletter","tags":"[]","date":"2020-04-28 16:48:16+00:00","url":"http://arxiv.org/abs/2004.13649v4"},{"x":"11.227781","y":"5.6625795","title":"Perceptual Values from Observation","cluster":"0","author":"['Ashley D. Edwards', 'Charles L. Isbell']","source":"arxiv","tags":"[]","date":"2019-05-20 03:59:44+00:00","url":"http://arxiv.org/abs/1905.07861v1"},{"x":"11.4151","y":"6.2281594","title":"Model-Based Reinforcement Learning via Meta-Policy Optimization","cluster":"0","author":"['Ignasi Clavera', 'Jonas Rothfuss', 'John Schulman', 'Yasuhiro Fujita', 'Tamim Asfour', 'Pieter Abbeel']","source":"arxiv","tags":"[]","date":"2018-09-14 01:15:28+00:00","url":"http://arxiv.org/abs/1809.05214v1"},{"x":"8.115304","y":"5.5760474","title":"Auto-Encoding Variational Bayes","cluster":"2","author":"['Diederik P Kingma', 'Max Welling']","source":"arxiv","tags":"[]","date":"2013-12-20 20:58:10+00:00","url":"http://arxiv.org/abs/1312.6114v10"},{"x":"11.273655","y":"5.7223425","title":"Meta-Inverse Reinforcement Learning with Probabilistic Context Variables","cluster":"0","author":"['Lantao Yu', 'Tianhe Yu', 'Chelsea Finn', 'Stefano Ermon']","source":"arxiv","tags":"[]","date":"2019-09-20 04:22:13+00:00","url":"http://arxiv.org/abs/1909.09314v2"},{"x":"8.269566","y":"5.4606786","title":"Response: Learning from Incorrectly Labeled Data","cluster":"2","author":"Eric Wallace","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://distill.pub/2019/advex-bugs-discussion/response-6/"},{"x":"8.512391","y":"5.18469","title":"Response: Robust Feature Leakage","cluster":"2","author":"Gabriel Goh","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://distill.pub/2019/advex-bugs-discussion/response-2/"},{"x":"8.565928","y":"5.207541","title":"Response: Adversarial Examples are Just Bugs, Too","cluster":"2","author":"Preetum Nakkiran","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://distill.pub/2019/advex-bugs-discussion/response-5/"},{"x":"8.349616","y":"4.891032","title":"Response: Adversarial Example Researchers Need to Expand What is Meant by ‘Robustness’","cluster":"2","author":"Justin Gilmer, Dan Hendrycks","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://distill.pub/2019/advex-bugs-discussion/response-1/"},{"x":"7.994875","y":"5.2986655","title":"Response: Adversarially Robust Neural Style Transfer","cluster":"2","author":"Reiichiro Nakano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://distill.pub/2019/advex-bugs-discussion/response-4/"},{"x":"8.228727","y":"4.9739532","title":"Feature Denoising for Improving Adversarial Robustness","cluster":"2","author":"Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1812.03411"},{"x":"8.642315","y":"5.013144","title":"Motivating the Rules of the Game for Adversarial Example Research","cluster":"2","author":"Justin Gilmer, Ryan P. Adams, Ian Goodfellow, David Andersen, George E. Dahl","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1807.06732"},{"x":"8.366868","y":"5.2006035","title":"Introducing the Unrestricted Adversarial Examples Challenge","cluster":"2","author":"Tom B. Brown and Catherine Olsson, Reserach Engineers, Google Brain Team","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html"},{"x":"8.520003","y":"4.945529","title":"Testing Robustness Against Unforeseen Adversaries","cluster":"2","author":"Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/testing-robustness/"},{"x":"8.574526","y":"5.1409774","title":"On the Geometry of Adversarial Examples","cluster":"2","author":"Marc Khoury, Dylan Hadfield-Menell","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1811.00525"},{"x":"8.547108","y":"4.9975095","title":"Towards Deep Learning Models Resistant to Adversarial Attacks","cluster":"2","author":"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1706.06083"},{"x":"8.072547","y":"5.0481973","title":"Pixels still beat text: Attacking the OpenAI CLIP model with text patches and adversarial pixel perturbations","cluster":"2","author":"Stanislav Fort","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://stanislavfort.github.io/2021/03/05/OpenAI_CLIP_stickers_and_adversarial_examples.html"},{"x":"7.61502","y":"5.699508","title":"Adversarial examples for the OpenAI CLIP in its zero-shot classification regime and their semantic generalization","cluster":"2","author":"Stanislav Fort","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://stanislavfort.github.io/2021/01/12/OpenAI_CLIP_adversarial_examples.html"},{"x":"10.450109","y":"6.076822","title":"AXRP 1: Adversarial Policies","cluster":"0","author":"Daniel Filan and Adam Gleave","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://axrp.net/episode/2020/12/11/episode-1-adversarial-policies-adam-gleave.html"},{"x":"8.562444","y":"5.043111","title":"A learning and masking approach to secure learning","cluster":"2","author":"Linh Nguyen, Sky Wang, Arunesh Sinha","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"7.859447","y":"5.15846","title":"E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles","cluster":"2","author":"['Markus Kettunen', 'Erik Härkönen', 'Jaakko Lehtinen']","source":"alignment newsletter","tags":"[]","date":"2019-06-10 13:40:37+00:00","url":"http://arxiv.org/abs/1906.03973v2"},{"x":"8.504435","y":"5.1076636","title":"Is Robustness [at] the Cost of Accuracy?","cluster":"2","author":"Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, Yupeng Gao","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1808.01688"},{"x":"7.759132","y":"6.857516","title":"Avoiding textual adversarial examples","cluster":"2","author":"Noa Nabeshima","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://nitter.cc/NoaNabeshima/status/1368662246885265409"},{"x":"8.057942","y":"5.3804407","title":"Fooling the primate brain with minimal, targeted image manipulation","cluster":"2","author":"['Li Yuan', 'Will Xiao', 'Giorgia Dellaferrera', 'Gabriel Kreiman', 'Francis E. H. Tay', 'Jiashi Feng', 'Margaret S. Livingstone']","source":"alignment newsletter","tags":"[]","date":"2020-11-11 08:30:54+00:00","url":"http://arxiv.org/abs/2011.05623v3"},{"x":"8.26026","y":"5.0525465","title":"Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation","cluster":"2","author":"Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Mingyan Liu, and Dawn Song","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1810.05162"},{"x":"11.459528","y":"10.592702","title":"Finite Factored Sets sequence","cluster":"1","author":"Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/s/kxs3eeEti9ouwWFzr"},{"x":"11.85867","y":"9.491142","title":"Infra-Bayesianism sequence","cluster":"1","author":"Diffractor and Vanessa Kosoy","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/s/CmrW8fCmSLK7E25sa"},{"x":"11.084542","y":"10.457646","title":"Cartesian Frames","cluster":"4","author":"Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/s/2A7rrZ4ySx6R8mfoT"},{"x":"10.274044","y":"7.4352093","title":"The ground of optimization","cluster":"4","author":"Alex Flint","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1"},{"x":"10.850861","y":"10.1143","title":"Embedded Agency via Abstraction","cluster":"1","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/hLFD6qSN9MmQxKjG5/embedded-agency-via-abstraction"},{"x":"10.383462","y":"9.390256","title":"Theory of Ideal Agents, or of Existing Agents?","cluster":"4","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/zQZcWkvEA8DLjKR7C/theory-of-ideal-agents-or-of-existing-agents"},{"x":"11.613897","y":"8.89734","title":"Troll Bridge","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/hpAbfXtqYC2BrpeiC/troll-bridge-5"},{"x":"10.702853","y":"7.8461957","title":"Selection vs Control","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/ZDZmopKquzHYPRNxq/selection-vs-control"},{"x":"12.436944","y":"8.413743","title":"Pavlov Generalizes","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/XTgkhjNTEi97WHMi6/pavlov-generalizes"},{"x":"9.888517","y":"8.838929","title":"Grokking the Intentional Stance","cluster":"4","author":"Jack Koch","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/jHSi6BwDKTLt5dmsG/grokking-the-intentional-stance"},{"x":"11.053953","y":"10.008831","title":"The Accumulation of Knowledge","cluster":"1","author":"Alex Flint","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/s/H6kiZXJwYgxZubtmD"},{"x":"11.907462","y":"9.151557","title":"Conceptual Problems with UDT and Policy Selection","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection"},{"x":"11.514538","y":"9.341623","title":"A Critique of Functional Decision Theory","cluster":"1","author":"Will MacAskill","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory"},{"x":"12.0769415","y":"9.855041","title":"In Logical Time, All Games are Iterated Games","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games"},{"x":"11.48177","y":"9.419412","title":"When wishful thinking works","cluster":"1","author":"Alex Mennen","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/KbCHcb8yyjAMFAAPJ/when-wishful-thinking-works"},{"x":"10.18223","y":"7.834001","title":"Bottle Caps Aren't Optimisers","cluster":"4","author":"Daniel Filan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers"},{"x":"9.32279","y":"9.357918","title":"Corrigibility doesn't always have a good action to take","cluster":"3","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/nbhTzEosM9sqEvr6P/corrigibility-doesn-t-always-have-a-good-action-to-take"},{"x":"11.356055","y":"9.066928","title":"Using expected utility for Good(hart)","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/urZzJPwHtjewdKKHc/using-expected-utility-for-good-hart"},{"x":"12.101956","y":"8.154967","title":"Reducing collective rationality to individual optimization in common-payoff games using MCMC","cluster":"1","author":"jessicata","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/JKSS8GEu7DGX4YuxN/reducing-collective-rationality-to-individual-optimization"},{"x":"11.423593","y":"9.404348","title":"Computational efficiency reasons not to model VNM-rational preference relations with utility functions","cluster":"1","author":"Alex Mennen","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/WXCBFpLj894dEte8Q/computational-efficiency-reasons-not-to-model-vnm-rational"},{"x":"11.786314","y":"9.989664","title":"Stable Pointers to Value III: Recursive Quantilization","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/bEa4FuLS4r7hExoty/stable-pointers-to-value-iii-recursive-quantilization"},{"x":"12.295348","y":"8.541352","title":"Buridan's ass in coordination games","cluster":"1","author":"jessicata","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/4xpDnGaKz472qB4LY/buridan-s-ass-in-coordination-games"},{"x":"11.320936","y":"9.815173","title":"Probability is Real, and Value is Complex","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/oheKfWA7SsvpK7SGp/probability-is-real-and-value-is-complex"},{"x":"9.717247","y":"8.303496","title":"Agency in Conway’s Game of Life","cluster":"4","author":"Alex Flint","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/3SG4WbNPoP8fsuZgs/agency-in-conway-s-game-of-life"},{"x":"11.516791","y":"10.130928","title":"Bayesian Probability is for things that are Space-like Separated from You","cluster":"1","author":"Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/FvcyMMaJKhYibtFDD/bayesian-probability-is-for-things-that-are-space-like"},{"x":"11.893996","y":"9.734254","title":"The Many Faces of Infra-Beliefs","cluster":"1","author":"Diffractor","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/GS5P7LLLbSSExb3Sk/the-many-faces-of-infra-beliefs"},{"x":"11.903799","y":"9.737585","title":"Radical Probabilism","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"11.84013","y":"9.858967","title":"Weak arguments against the universal prior being malign","cluster":"1","author":"X4vier","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/Ecxevhvx85Y4eyFcu/weak-arguments-against-the-universal-prior-being-malign"},{"x":"10.969007","y":"10.377328","title":"Public Static: What is Abstraction?","cluster":"1","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction"},{"x":"12.603482","y":"8.474738","title":"Prisoners' Dilemma with Costs to Modeling","cluster":"1","author":"Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/XjMkPyaPYTf7LrKiT/prisoners-dilemma-with-costs-to-modeling"},{"x":"11.461137","y":"9.892806","title":"An Orthodox Case Against Utility Functions","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions"},{"x":"12.676124","y":"9.670591","title":"Markets are Universal for Logical Induction","cluster":"1","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/WmNeCipNwg9CmGy3T/markets-are-universal-for-logical-induction"},{"x":"12.539695","y":"8.477858","title":"Robust program equilibrium","cluster":"1","author":"Caspar Oesterheld","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://foundational-research.org/robust-program-equilibrium/"},{"x":"10.539702","y":"7.877142","title":"Focus: you are allowed to be bad at accomplishing your goals","cluster":"4","author":"Adam Shimi","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/X5WTgfX5Ly4ZNHWZD/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals"},{"x":"11.633751","y":"8.558019","title":"Predictors exist: CDT going bonkers... forever","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/Kr76XzME7TFkN937z/predictors-exist-cdt-going-bonkers-forever"},{"x":"12.372297","y":"10.02266","title":"Exorcizing the Speed Prior?","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/Say4sCQ2g22HGsbRT/exorcizing-the-speed-prior"},{"x":"9.161284","y":"7.740777","title":"Steps toward super intelligence ([1](http://rodneybrooks.com/forai-steps-toward-super-intelligence-i-how-we-got-here/), [2](http://rodneybrooks.com/forai-steps-toward-super-intelligence-ii-beyond-the-turing-test/), [3](http://rodneybrooks.com/forai-steps-toward-super-intelligence-iii-hard-things-today/), [4](http://rodneybrooks.com/forai-steps-toward-super-intelligence-iv-things-to-work-on-now/))","cluster":"4","author":"Rodney Brooks","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"8.083226","y":"9.855855","title":"80K podcast with Allan Dafoe","cluster":"3","author":"Allan Dafoe and Rob Wiblin","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/"},{"x":"8.076409","y":"9.657063","title":"The new 30-person research group in DC investigating how emerging technologies could affect national security","cluster":"3","author":"Rob Wiblin and Helen Toner","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://80000hours.org/podcast/episodes/helen-toner-on-security-and-emerging-technology/"},{"x":"8.06983","y":"9.760835","title":"AI Alignment Podcast: On the Governance of AI","cluster":"3","author":"Lucas Perry and Jade Leung","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/07/22/on-the-governance-of-ai-with-jade-leung/"},{"x":"7.89678","y":"9.760574","title":"Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development","cluster":"3","author":"Peter Cihon","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.fhi.ox.ac.uk/standards-technical-report/"},{"x":"8.680627","y":"8.723498","title":"Regulatory Markets for AI Safety","cluster":"3","author":"Jack Clark, Gillian K. Hadfield","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/2001.00078"},{"x":"8.122552","y":"9.644975","title":"80K podcast: How can policy keep up with AI advances?","cluster":"3","author":"Rob Wiblin, Jack Clark, Miles Brundage and Amanda Askell","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://80000hours.org/podcast/episodes/openai-askell-brundage-clark-latest-in-ai-policy-and-strategy/"},{"x":"8.45425","y":"9.837679","title":"Thinking About Risks From AI: Accidents, Misuse and Structure","cluster":"3","author":"Remco Zwetsloot, Allan Dafoe","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure"},{"x":"8.534841","y":"9.429367","title":"‘Solving for X?’ Towards a problem-finding framework to ground long-term governance strategies for artificial intelligence","cluster":"3","author":"Hin-Yan Liu, Matthijs M. Maas","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.researchgate.net/profile/Matthijs_Maas/publication/342774816_'Solving_for_X'_Towards_a_problem-finding_framework_to_ground_long-term_governance_strategies_for_artificial_intelligence/links/5fbbd04592851c933f517ad3/Solving-for-X-Towards-a-problem-finding-framework-to-ground-long-term-governance-strategies-for-artificial-intelligence.pdf"},{"x":"8.16161","y":"9.675129","title":"AI Governance: Opportunity and Theory of Impact","cluster":"3","author":"Allan Dafoe","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact"},{"x":"8.193259","y":"9.312091","title":"Technology Roulette: Managing Loss of Control as Many Militaries Pursue Technological Superiority","cluster":"3","author":"Richard Danzig","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.cnas.org/publications/reports/technology-roulette"},{"x":"8.054101","y":"9.703173","title":"AI Governance in 2019 - A Year in Review: Observations from 50 Global Experts","cluster":"3","author":"Shi Qian*, Li Hui*, Brian Tse*, others","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.aigovernancereview.com/"},{"x":"8.380488","y":"9.353955","title":"Improving Verifiability in AI Development","cluster":"3","author":"Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong, Tegan Maharaj, Pang Wei Koh, Sara Hooker, Jade Leung, Andrew Trask, Emma Bluemke, Jonathan Lebensbold, Cullen O'Keefe, Mark Koren, Théo Ryffel, JB Rubinovitz, Tamay Besiroglu, Federica Carugati, Jack Clark, Peter Eckersley, Sarah de Haas, Maritza Johnson, Ben Laurie, Alex Ingerman, Igor Krawczuk, Amanda Askell, Rosario Cammarota, Andrew Lohn, David Krueger, Charlotte Stix, Peter Henderson, Logan Graham, Carina Prunkl, Bianca Martin, Elizabeth Seger, Noa Zilberman, Seán Ó hÉigeartaigh, Frens Kroeger, Girish Sastry, Rebecca Kagan, Adrian Weller, Brian Tse, Elizabeth Barnes, Allan Dafoe, Paul Scharre, Ariel Herbert-Voss, Martijn Rasser, Shagun Sodhani, Carrick Flynn, Thomas Krendl Gilbert, Lisa Dyer, Saif Khan, Yoshua Bengio, Markus Anderljung","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://openai.com/blog/improving-verifiability/"},{"x":"8.502386","y":"9.921866","title":"The Windfall Clause: Distributing the Benefits of AI","cluster":"3","author":"Cullen O’Keefe, Peter Cihon, Ben Garfinkel, Carrick Flynn, Jade Leung, Allan Dafoe","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.fhi.ox.ac.uk/windfallclause/"},{"x":"8.287959","y":"9.918636","title":"How does the offense-defense balance scale?","cluster":"3","author":"Ben Garfinkel, Allan Dafoe","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.tandfonline.com/doi/full/10.1080/01402390.2019.1631810"},{"x":"8.404963","y":"9.424539","title":"Why Responsible AI Development Needs Cooperation on Safety","cluster":"3","author":"Amanda Askell, Miles Brundage, Jack Clark, Gillian Hadfield","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/cooperation-on-safety/"},{"x":"8.234145","y":"8.83402","title":"AGI will drastically increase economies of scale","cluster":"3","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://alignmentforum.org/posts/Sn5NiiD5WBi4dLzaB/agi-will-drastically-increase-economies-of-scale-1"},{"x":"8.467855","y":"9.90064","title":"Stable Agreements in Turbulent Times","cluster":"3","author":"Cullen O’Keefe","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.fhi.ox.ac.uk/wp-content/uploads/Stable-Agreements.pdf"},{"x":"8.897257","y":"7.635064","title":"How Sure are we about this AI Stuff?","cluster":"4","author":"Ben Garfinkel","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://forum.effectivealtruism.org/posts/9sBAW3qKppnoG3QPq/how-sure-are-we-about-this-ai-stuff"},{"x":"8.677542","y":"8.271945","title":"AI development incentive gradients are not uniformly terrible","cluster":"4","author":"rk","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/bkG4qj9BFEkNva3EX/ai-development-incentive-gradients-are-not-uniformly"},{"x":"8.167296","y":"9.877025","title":"The Vulnerable World Hypothesis","cluster":"3","author":"Nick Bostrom","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://nickbostrom.com/papers/vulnerable.pdf"},{"x":"8.690778","y":"5.2731924","title":"The Future of Surveillance","cluster":"2","author":"Ben Garfinkel","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.effectivealtruism.org/articles/ea-global-2018-the-future-of-surveillance/"},{"x":"9.379805","y":"9.56641","title":"Decoupling deliberation from competition","cluster":"3","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition"},{"x":"7.7889733","y":"9.546882","title":"A personal take on longtermist AI governance","cluster":"3","author":"Luke Muehlhauser","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://forum.effectivealtruism.org/posts/M2SBwctwC6vBqAmZW/a-personal-take-on-longtermist-ai-governance"},{"x":"8.323081","y":"9.355316","title":"Corporate Governance of Artificial Intelligence in the Public Interest","cluster":"3","author":"Peter Cihon, Jonas Schuett, Seth D. Baum","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.mdpi.com/2078-2489/12/7/275"},{"x":"8.207199","y":"9.847157","title":"Debunking the AI Arms Race Theory","cluster":"3","author":"Paul Scharre","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/"},{"x":"7.5052","y":"7.0781846","title":"Truth, Lies, and Automation: How Language Models Could Change Disinformation","cluster":"3","author":"Ben Buchanan, Andrew Lohn, Micah Musser, Katerina Sedova","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://cset.georgetown.edu/publication/truth-lies-and-automation/"},{"x":"8.133133","y":"9.9088","title":"International Control of Powerful Technology: Lessons from the Baruch Plan for Nuclear Weapons","cluster":"3","author":"Waqar Zaidi, Allan Dafoe","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.fhi.ox.ac.uk/wp-content/uploads/2021/03/International-Control-of-Powerful-Technology-Lessons-from-the-Baruch-Plan-Zaidi-Dafoe-2021.pdf"},{"x":"8.027792","y":"9.4529295","title":"NSCAI Final Report","cluster":"3","author":"Eric Schmidt, Robert Work, Safra Catz, Eric Horvitz, Steve Chien, Andrew Jassy, Mignon Clyburn, Gilman Louie, Chris Darby, William Mark, Kenneth Ford, Jason Matheny, José-Marie Griffiths, Katharina McFarland, Andrew Moore","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://reports.nscai.gov/final-report/table-of-contents/"},{"x":"8.28673","y":"9.866051","title":"Why those who care about catastrophic and existential risk should care about autonomous weapons","cluster":"3","author":"Anthony Aguirre","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://forum.effectivealtruism.org/posts/oR9tLNRSAep293rr5/why-those-who-care-about-catastrophic-and-existential-risk-2"},{"x":"9.261496","y":"9.228142","title":"Society-in-the-loop: programming the algorithmic social contract","cluster":"4","author":"Iyad Rahwan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://link.springer.com/content/pdf/10.1007/s10676-017-9430-8.pdf"},{"x":"8.1996355","y":"9.839113","title":"AI Benefits","cluster":"3","author":"Cullen O'Keefe","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://cullenokeefe.com/ai-benefits-index"},{"x":"8.086281","y":"8.378992","title":"Machine Learning and Artificial Intelligence: how do we make sure technology serves the open society?","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.ditchley.com/events/past-events/2010-2019/2017/machine-learning-and-artificial-intelligence-how-do-we-make-sure"},{"x":"8.183069","y":"9.859459","title":"Activism by the AI Community: Analysing Recent Achievements and Future Prospects","cluster":"3","author":"Haydn Belfield","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.cser.ac.uk/resources/activism-ai-community-analysing-recent-achievements-and-future-prospects/"},{"x":"8.479818","y":"9.445021","title":"Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society","cluster":"3","author":"Carina Prunkl and Jess Whittlestone","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.cser.ac.uk/resources/beyond-near-long-term/"},{"x":"8.064704","y":"9.409825","title":"AI Alignment Podcast: On the Long-term Importance of Current AI Policy","cluster":"3","author":"Lucas Perry, Nicolas Moës and Jared Brown","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://futureoflife.org/2020/02/17/on-the-long-term-importance-of-current-ai-policy-with-nicolas-moes-and-jared-brown/"},{"x":"7.9204473","y":"9.4810705","title":"Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter","cluster":"3","author":"Nathan Calvin, Jade Leung","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.fhi.ox.ac.uk/wp-content/uploads/Patents_-FHI-Working-Paper-Final-.pdf"},{"x":"8.553582","y":"9.284755","title":"Should Artificial Intelligence Governance be Centralised? Design Lessons from History","cluster":"3","author":"Peter Cihon, Matthijs Maas, and Luke Kemp","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.cser.ac.uk/media/uploads/files/Cihon_et_al-_2019-_Should_AI_Governance_be_Centralised.pdf"},{"x":"8.589249","y":"9.267338","title":"AI Alignment Podcast: Machine Ethics and AI Governance","cluster":"3","author":"Lucas Perry and Wendell Wallach","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/11/15/machine-ethics-and-ai-governance-with-wendell-wallach/"},{"x":"8.162155","y":"9.801094","title":"Information security careers for GCR reduction","cluster":"3","author":"Claire Zabel and Luke Muehlhauser","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://forum.effectivealtruism.org/posts/ZJiCfwTy5dC4CoxqA/information-security-careers-for-gcr-reduction"},{"x":"8.030812","y":"9.596698","title":"Google’s brand-new AI ethics board is already falling apart","cluster":"3","author":"Kelsey Piper","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.vox.com/future-perfect/2019/4/3/18292526/google-ai-ethics-board-letter-acquisti-kay-coles-james"},{"x":"7.965152","y":"9.251981","title":"Rationally Speaking #231 - Helen Toner on \"Misconceptions about China and artificial intelligence\"","cluster":"3","author":"Julia Galef and Helen Toner","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://www.rationallyspeakingpodcast.org/show/rs-231-helen-toner-on-misconceptions-about-china-and-artific.html"},{"x":"8.276097","y":"9.247558","title":"A Survey of the EU's AI Ecosystem","cluster":"3","author":"Charlotte Stix","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.charlottestix.com/european-union-ai-ecosystem"},{"x":"8.220484","y":"9.638291","title":"Toward AI Security: Global Aspirations for a More Resilient Future","cluster":"3","author":"Jessica Cussins Newman","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://cltc.berkeley.edu/TowardAISecurity/"},{"x":"8.526458","y":"8.900568","title":"The American Public’s Attitudes Concerning Artificial Intelligence","cluster":"3","author":"Baobao Zhang and Allan Dafoe","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.fhi.ox.ac.uk/aipublic2019/"},{"x":"9.209008","y":"9.048664","title":"Countering Superintelligence Misinformation","cluster":"3","author":"Seth D. Baum","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://gcrinstitute.org/countering-superintelligence-misinformation/"},{"x":"8.297258","y":"9.7441845","title":"What the AI Community Can Learn From Sneezing Ferrets and a Mutant Virus Debate","cluster":"3","author":"Jasmine Wang","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://medium.com/partnership-on-ai/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e"},{"x":"8.369512","y":"9.398866","title":"AI Certification: Advancing Ethical Practice by Reducing Information Asymmetries","cluster":"3","author":"Peter Cihon, Moritz J. Kleinaltenkamp, Jonas Schuett, Seth D. Baum","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"http://gcrinstitute.org/ai-certification-advancing-ethical-practice-by-reducing-information-asymmetries/"},{"x":"7.9471884","y":"9.799899","title":"Case studies of self-governance to reduce technology risk","cluster":"3","author":"Jia","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://forum.effectivealtruism.org/posts/Xf6QE6txgvfCGvZpk/case-studies-of-self-governance-to-reduce-technology-risk"},{"x":"8.220641","y":"9.28302","title":"AI and International Stability: Risks and Confidence-Building Measures","cluster":"3","author":"Michael Horowitz, Paul Scharre","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures"},{"x":"8.304729","y":"9.592","title":"Bridging the Gap: The Case for an ‘Incompletely Theorized Agreement’ on AI Policy.","cluster":"3","author":"Charlotte Stix, Matthijs M. Maas","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://link.springer.com/article/10.1007%2Fs43681-020-00037-w"},{"x":"8.001422","y":"9.847042","title":"Fragmentation and the Future: Investigating Architectures for International AI Governance","cluster":"3","author":"Peter Cihon, Matthijs M. Maas, Luke Kemp","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12890"},{"x":"8.010237","y":"9.745862","title":"Our AI governance grantmaking so far","cluster":"3","author":"Luke Muehlhauser","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.openphilanthropy.org/blog/ai-governance-grantmaking"},{"x":"7.675177","y":"8.522384","title":"Future Indices: How Crowd Forecasting Can Inform the Big Picture","cluster":"2","author":"Michael Page, Catherine Aiken, Dewey Murdick","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://cset.georgetown.edu/research/future-indices/"},{"x":"8.048602","y":"9.587548","title":"Decision Points in AI Governance","cluster":"3","author":"Jessica Cussins Newman","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://cltc.berkeley.edu/wp-content/uploads/2020/05/Decision_Points_AI_Governance.pdf"},{"x":"8.380546","y":"9.294983","title":"Antitrust-Compliant AI Industry Self-Regulation","cluster":"3","author":"Cullen O'Keefe","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://cullenokeefe.com/blog/antitrust-compliant-ai-industry-self-regulation"},{"x":"7.869449","y":"9.637351","title":"Institutionalizing ethics in AI through broader impact requirements","cluster":"3","author":"Carina E. A. Prunkl, Carolyn Ashurst, Markus Anderljung, Helena Webb, Jan Leike, Allan Dafoe ","source":"alignment newsletter","tags":"[]","date":"2021.0","url":""},{"x":"8.32582","y":"9.655514","title":"AI Alignment Podcast: On Lethal Autonomous Weapons","cluster":"3","author":"Lucas Perry and Paul Scharre","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/xEzudcydk7APZbnai/ai-alignment-podcast-on-lethal-autonomous-weapons-with-paul"},{"x":"8.831322","y":"9.20136","title":"Exploring AI Futures Through Role Play","cluster":"3","author":"Shahar Avin, Ross Gruetzemacher, James Fox","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.cser.ac.uk/resources/ai-futures-role-play/"},{"x":"8.332927","y":"9.699683","title":"An Interview with Ben Garfinkel","cluster":"3","author":"Joshua Monrad, Mojmír Stehlík and Ben Garfinkel","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://thepolitic.org/an-interview-with-ben-garfinkel-governance-of-ai-program-researcher/"},{"x":"8.5907","y":"8.701432","title":"The Hacker Learns to Trust","cluster":"3","author":"Connor Leahy","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://medium.com/@NPCollapse/the-hacker-learns-to-trust-62f3c1490f51"},{"x":"8.018643","y":"9.490654","title":"When Is It Appropriate to Publish High-Stakes AI Research?","cluster":"3","author":"Claire Leibowicz, Steven Adler, and Peter Eckersley","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.partnershiponai.org/when-is-it-appropriate-to-publish-high-stakes-ai-research/"},{"x":"8.907811","y":"5.783936","title":"Scaling shared model governance via model splitting","cluster":"2","author":"Miljan Martic, Jan Leike, Andrew Trask, Matteo Hessel, Shane Legg, Pushmeet Kohli","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1812.05979"},{"x":"8.552336","y":"9.2906","title":"Medium-Term Artificial Intelligence and Society","cluster":"3","author":"Seth D. Baum","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.mdpi.com/2078-2489/11/6/290/htm"},{"x":"11.8952265","y":"9.911888","title":"Clarifying Consequentialists in the Solomonoff Prior","cluster":"1","author":"vlad_m","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/jP3vRbtvDtBtgvkeb/clarifying-consequentialists-in-the-solomonoff-prior"},{"x":"12.307324","y":"9.35277","title":"An environment for studying counterfactuals","cluster":"1","author":"Nisan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/hhNH3knNHgdkonAKB/an-environment-for-studying-counterfactuals"},{"x":"11.673653","y":"9.295167","title":"Conditioning, Counterfactuals, Exploration, and Gears","cluster":"1","author":"Diffractor","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/uQHAJ7TdBbweRR5iS/conditioning-counterfactuals-exploration-and-gears"},{"x":"12.000817","y":"10.028331","title":"Dependent Type Theory and Zero-Shot Reasoning","cluster":"1","author":"evhub","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/Xfw2d5horPunP2MSK/dependent-type-theory-and-zero-shot-reasoning"},{"x":"11.456219","y":"10.046594","title":"Mathematical Mindset","cluster":"1","author":"komponisto","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/wxBBRzR4FS7nGBjbD/mathematical-mindset"},{"x":"9.831275","y":"8.828005","title":"Monk Treehouse: some problems defining simulation","cluster":"4","author":"dranorter","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/o7FBQkwgKJPDjDKnh/monk-treehouse-some-problems-defining-simulation"},{"x":"10.092277","y":"9.20686","title":"No, I won't go there, it feels like you're trying to Pascal-mug me","cluster":"4","author":"Rupert","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/o7MXZgx3SGpqSxHYZ/no-i-won-t-go-there-it-feels-like-you-re-trying-to-pascal"},{"x":"8.408614","y":"6.720394","title":"AlphaFold: Using AI for scientific discovery","cluster":"2","author":"Andrew Senior, John Jumper and Demis Hassabis","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://deepmind.com/blog/alphafold/"},{"x":"8.348993","y":"5.5255947","title":"A major milestone for the treatment of eye disease","cluster":"2","author":"Mustafa Suleyman","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://deepmind.com/blog/moorfields-major-milestone/"},{"x":"8.348882","y":"6.0567937","title":"The Machine Learning Behind Android Smart Linkify","cluster":"2","author":"Lukas Zilka","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ai.googleblog.com/2018/08/the-machine-learning-behind-android.html"},{"x":"8.778542","y":"8.076295","title":"Artificial Intelligence — The Revolution Hasn’t Happened Yet","cluster":"4","author":"Michael Jordan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7"},{"x":"8.508839","y":"8.1739645","title":"Ben Garfinkel on scrutinising classic AI risk arguments","cluster":"3","author":"Howie Lempel and Ben Garfinkel","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/"},{"x":"9.320002","y":"8.299178","title":"We Shouldn’t be Scared by ‘Superintelligent A.I.’","cluster":"4","author":"Melanie Mitchell","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.nytimes.com/2019/10/31/opinion/superintelligent-artificial-intelligence.html"},{"x":"8.002415","y":"6.295589","title":"ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters","cluster":"2","author":"Rangan Majumder, Junhua Wang","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/"},{"x":"7.927402","y":"5.222286","title":"ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring","cluster":"2","author":"David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, Colin Raffel","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/1911.09785"},{"x":"8.208671","y":"5.881291","title":"Deep Double Descent","cluster":"2","author":"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/deep-double-descent/"},{"x":"8.3178835","y":"5.823846","title":"Are Deep Neural Networks Dramatically Overfitted?","cluster":"2","author":"Lilian Weng","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html"},{"x":"7.5887246","y":"6.241653","title":"Transformer-XL: Unleashing the Potential of Attention Models","cluster":"2","author":"Zihang Dai, Zhilin Yang and \nQuoc Le, Google AI","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html"},{"x":"8.78385","y":"5.9493704","title":"Reptile: A Scalable Meta-Learning Algorithm","cluster":"2","author":"Alex Nichol and John Schulman","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/reptile/"},{"x":"8.325491","y":"6.2447486","title":"Relational inductive biases, deep learning, and graph networks","cluster":"2","author":"Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1806.01261"},{"x":"7.834851","y":"6.0408206","title":"The Scaling Hypothesis","cluster":"2","author":"Gwern Branwen","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.gwern.net/Scaling-hypothesis"},{"x":"7.7797146","y":"5.4705453","title":"Feature-wise transformations","cluster":"2","author":"Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville and Yoshua Bengio","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://distill.pub/2018/feature-wise-transformations/"},{"x":"7.698462","y":"6.3240757","title":"Pretrained Transformers as Universal Computation Engines","cluster":"2","author":"Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://bair.berkeley.edu/blog/2021/03/23/universal-computation/"},{"x":"7.687304","y":"6.1396246","title":"[Explaining Neural Scaling Laws](https://arxiv.org/abs/2102.06701) and [A Neural Scaling Law from the Dimension of the Data Manifold](https://arxiv.org/abs/2004.10802)","cluster":"2","author":"Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma","source":"alignment newsletter","tags":"[]","date":"2021.0","url":""},{"x":"8.206158","y":"5.8671136","title":"Why does deep and cheap learning work so well?","cluster":"2","author":"Henry W. Lin, Max Tegmark, David Rolnick","source":"alignment newsletter","tags":"[]","date":"2017.0","url":"https://arxiv.org/abs/1608.08225"},{"x":"7.85771","y":"5.843916","title":"Fast and Easy Infinitely Wide Networks with Neural Tangents","cluster":"2","author":"Roman Novak*, Lechao Xiao*, Samuel S. Schoenholz*, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html"},{"x":"8.23063","y":"5.7851987","title":"Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization","cluster":"2","author":"Satrajit Chatterjee","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://arxiv.org/abs/2002.10657"},{"x":"7.996708","y":"6.2391253","title":"SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems","cluster":"2","author":"Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, Anshumali Shrivastava","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://arxiv.org/abs/1903.03129"},{"x":"7.667803","y":"6.3157954","title":"A new model and dataset for long-range memory","cluster":"2","author":"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory"},{"x":"8.264166","y":"5.384016","title":"The Quiet Semi-Supervised Revolution","cluster":"2","author":"Vincent Vanhoucke","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://towardsdatascience.com/the-quiet-semi-supervised-revolution-edec1e9ad8c"},{"x":"8.247182","y":"5.8798466","title":"Uniform convergence may be unable to explain generalization in deep learning","cluster":"2","author":"Vaishnavh Nagarajan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://locuslab.github.io/2019-07-09-uniform-convergence/"},{"x":"8.236451","y":"5.5383177","title":"Understanding the generalization of ‘lottery tickets’ in neural networks","cluster":"2","author":"Ari Morcos, Yuandong Tian","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks"},{"x":"8.6413145","y":"5.719476","title":"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask","cluster":"2","author":"Hattie Zhou, Janice Lan, Rosanne Liu, Jason Yosinski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://eng.uber.com/deconstructing-lottery-tickets/"},{"x":"7.87953","y":"5.2933397","title":"Semantic Image Synthesis with Spatially-Adaptive Normalization","cluster":"2","author":"Taesung Park, Ming-Yu Liu, Ting-Chun Wang and Jun-Yan Zhu","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/pdf/1903.07291.pdf"},{"x":"8.141914","y":"5.767404","title":"Measuring the Intrinsic Dimension of Objective Landscapes","cluster":"2","author":" Chunyuan Li, Rosanne Liu, and Jason Yosinski\t","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://eng.uber.com/intrinsic-dimension/"},{"x":"7.958185","y":"6.2707777","title":"Measuring the Limits of Data Parallel Training for Neural Networks","cluster":"2","author":"Chris Shallue and George Dahl","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai.googleblog.com/2019/03/measuring-limits-of-data-parallel.html"},{"x":"8.299074","y":"6.86544","title":"How AI Training Scales","cluster":"2","author":"Sam McCandlish, Jared Kaplan and Dario Amodei","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/science-of-ai/"},{"x":"10.404297","y":"5.7659655","title":"Relational Deep Reinforcement Learning","cluster":"0","author":"Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, Peter Battaglia","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1806.01830"},{"x":"8.133227","y":"6.0603337","title":"Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets","cluster":"2","author":"Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf"},{"x":"7.898205","y":"5.562964","title":"Branch Specialization","cluster":"2","author":"Chelsea Voss, Gabriel Goh, Nick Cammarata, Michael Petrov, Ludwig Schubert, Chris Olah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://distill.pub/2020/circuits/branch-specialization/"},{"x":"7.6654034","y":"5.7064257","title":"Multimodal Neurons in Artificial Neural Networks","cluster":"2","author":"Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://openai.com/blog/multimodal-neurons/"},{"x":"8.332253","y":"6.978095","title":"AlphaFold: a solution to a 50-year-old grand challenge in biology","cluster":"2","author":"The AlphaFold team, John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology"},{"x":"8.00067","y":"5.289903","title":"Identifying Statistical Bias in Dataset Replication","cluster":"2","author":"Logan Engstrom, Andrew Ilyas, Aleksander Mądry, Shibani Santurkar, Jacob Steinhardt, Dimitris Tsipras","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://gradientscience.org/data_rep_bias/"},{"x":"7.515423","y":"6.901209","title":"More Efficient NLP Model Pre-training with ELECTRA","cluster":"2","author":"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html"},{"x":"8.963949","y":"7.2460413","title":"Growing Neural Cellular Automata","cluster":"2","author":"Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://distill.pub/2020/growing-ca/"},{"x":"8.711046","y":"6.1080384","title":"Inductive biases stick around","cluster":"2","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/nGqzNC6uNueum2w8T/inductive-biases-stick-around"},{"x":"8.397195","y":"6.269778","title":"Understanding “Deep Double Descent”","cluster":"2","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent"},{"x":"8.791123","y":"5.994535","title":"Weight Agnostic Neural Networks","cluster":"2","author":"Adam Gaier, David Ha","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://weightagnostic.github.io/"},{"x":"7.7370625","y":"6.0577793","title":"Generative Modeling with Sparse Transformers","cluster":"2","author":"Rewon Child and Scott Gray","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/sparse-transformer/"},{"x":"8.001572","y":"6.0240836","title":"MLPerf","cluster":"2","author":"nan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://mlperf.org/"},{"x":"7.4986405","y":"6.9079533","title":"A Conservative Human Baseline Estimate for GLUE: People Still (Mostly) Beat Machines","cluster":"2","author":"Nikita Nangia, Samuel R. Bowman","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://woollysocks.github.io/assets/GLUE_Human_Baseline.pdf"},{"x":"7.81959","y":"5.435631","title":"Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet","cluster":"2","author":"Anonymous","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://openreview.net/pdf?id=SkfMWhAqYQ"},{"x":"7.7470016","y":"6.0512667","title":"Relational recurrent neural networks","cluster":"2","author":"Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1806.01822"},{"x":"8.265431","y":"5.2569747","title":"Discriminator Rejection Sampling","cluster":"2","author":"Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, Augustus Odena","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1810.06758"},{"x":"9.15779","y":"6.3961062","title":"Neural Guided Constraint Logic Programming for Program Synthesis","cluster":"2","author":"Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William E. Byrd, Matthew Might, Raquel Urtasun, Richard Zemel","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1809.02840"},{"x":"7.7786965","y":"5.9897623","title":"When Recurrent Models Don't Need to be Recurrent","cluster":"2","author":"John Miller","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://bair.berkeley.edu/blog/2018/08/06/recurrent/"},{"x":"7.809088","y":"5.5036855","title":"Objects that Sound","cluster":"2","author":"Relja Arandjelović, Andrew Zisserman","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://deepmind.com/blog/objects-that-sound/"},{"x":"8.052165","y":"5.9243145","title":"MnasNet: Towards Automating the Design of Mobile Machine Learning Models","cluster":"2","author":"Mingxing Tan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html"},{"x":"8.159998","y":"5.652436","title":"Glow: Better Reversible Generative Models","cluster":"2","author":"Prafulla Dhariwal and Durk Kingma","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/glow/"},{"x":"7.888393","y":"5.521117","title":"Weight Banding","cluster":"2","author":"Michael Petrov, Chelsea Voss, Ludwig Schubert, Nick Cammarata, Gabriel Goh, Chris Olah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://distill.pub/2020/circuits/weight-banding/"},{"x":"7.8127856","y":"5.3562226","title":"Transformers for Image Recognition at Scale","cluster":"2","author":"Alexey Dosovitskiy*, Lucas Beyer*, Alexander Kolesnikov*, Dirk Weissenborn*, Xiaohua Zhai*, Neil Houlsby*, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"},{"x":"7.4413586","y":"6.721067","title":"Beijing Academy of Artificial Intelligence announces 1,75 trillion parameters model, Wu Dao 2.0","cluster":"2","author":"nan","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.lesswrong.com/posts/2dG7vXDZjd6crkdLa/beijing-academy-of-artificial-intelligence-announces-1-75"},{"x":"8.96051","y":"9.476171","title":"Embedded Curiosities","cluster":"3","author":"Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/j9CbmSsnprxB2uFY9/embedded-curiosities"},{"x":"11.711828","y":"9.503917","title":"Embedded World-Models (Addendum)","cluster":"1","author":"Abram Demski and Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/efWfvrWLgJmbBAs3m/embedded-world-models"},{"x":"10.86622","y":"9.710615","title":"Decision Theory","cluster":"1","author":"Abram Demski and Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/zcPLNNw4wgBX5k8kQ/decision-theory"},{"x":"11.184592","y":"9.667404","title":"Embedded World-Models","cluster":"1","author":"Abram Demski and Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/efWfvrWLgJmbBAs3m/embedded-world-models"},{"x":"10.60546","y":"5.9280477","title":"Reinforcement Learning with Prediction-Based Rewards","cluster":"0","author":"Yuri Burda and Harri Edwards","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/"},{"x":"10.865929","y":"5.757977","title":"Making Efficient Use of Demonstrations to Solve Hard Exploration Problems","cluster":"0","author":"Caglar Gulcehre*, Tom Le Paine*, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, Worlds Team","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://deepmind.com/research/publications/Making-Efficient-Use-of-Demonstrations-to-Solve-Hard-Exploration-Problems"},{"x":"10.991257","y":"5.9700704","title":"Exploration Strategies in Deep Reinforcement Learning","cluster":"0","author":"Lilian Weng","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"11.056354","y":"5.9616356","title":"Self-Supervised Exploration via Disagreement","cluster":"0","author":"Deepak Pathak*, Dhiraj Gandhi*, Abhinav Gupta","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1906.04161"},{"x":"10.66688","y":"5.775856","title":"Episodic Curiosity through Reachability","cluster":"0","author":"Nikolay Savinov, Anton Raichuk, Raphaël Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, Sylvain Gelly","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1810.02274"},{"x":"10.718524","y":"5.7144775","title":"Montezuma’s Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems","cluster":"0","author":"Adrien Ecoffet,  Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://eng.uber.com/go-explore/"},{"x":"10.760692","y":"7.392136","title":"Delayed Impact of Fair Machine Learning","cluster":"2","author":"Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, Moritz Hardt","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://bair.berkeley.edu/blog/2018/05/17/delayed-impact/"},{"x":"8.070228","y":"9.542444","title":"The case for building expertise to work on US AI policy, and how to do it","cluster":"3","author":"Niel Bowerman","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://80000hours.org/articles/us-ai-policy/"},{"x":"8.470499","y":"6.9684696","title":"The fastest way into a high-impact role as a machine learning engineer, according to Catherine Olsson & Daniel Ziegler","cluster":"2","author":"Catherine Olsson, Daniel Ziegler, and Rob Wiblin","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/"},{"x":"8.42722","y":"9.582284","title":"Critch on career advice for junior AI-x-risk-concerned researchers","cluster":"3","author":"Andrew Critch, via Rob Bensinger","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/7uJnA3XDpTgemRH2c/critch-on-career-advice-for-junior-ai-x-risk-concerned"},{"x":"8.495577","y":"8.005717","title":"What I talk about when I talk about AI x-risk: 3 core claims I want machine learning researchers to address.","cluster":"4","author":"David Krueger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/bJdaB2Mz4mBvwFBeb/what-i-talk-about-when-i-talk-about-ai-x-risk-3-core-claims-1"},{"x":"7.9549294","y":"6.931472","title":"AI and Efficiency","cluster":"2","author":"Danny Hernandez, Tom Brown","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://openai.com/blog/ai-and-efficiency/"},{"x":"7.676642","y":"8.977999","title":"Discontinuous progress in history: an update","cluster":"3","author":"Katja Grace","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://aiimpacts.org/discontinuous-progress-in-history-an-update/"},{"x":"7.7363477","y":"8.827673","title":"Description vs simulated prediction","cluster":"3","author":"Rick Korzekwa","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://aiimpacts.org/description-vs-simulated-prediction/"},{"x":"8.006053","y":"6.2825284","title":"AI and Compute","cluster":"2","author":"Dario Amodei and Danny Hernandez","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/ai-and-compute/"},{"x":"7.76164","y":"8.975385","title":"Reinterpreting “AI and Compute”","cluster":"3","author":"Ben Garfinkel","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://aiimpacts.org/reinterpreting-ai-and-compute/"},{"x":"7.600517","y":"8.923153","title":"Could Advanced AI Drive Explosive Economic Growth?","cluster":"3","author":"Tom Davidson","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth"},{"x":"7.7716455","y":"6.6669803","title":"Extrapolating GPT-N performance","cluster":"2","author":"Lukas Finnveden","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance"},{"x":"8.070236","y":"7.3498836","title":"Draft report on AI timelines","cluster":"2","author":"Ajeya Cotra","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"},{"x":"7.4643364","y":"8.717369","title":"Modeling the Human Trajectory","cluster":"1","author":"David Roodman","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.openphilanthropy.org/blog/modeling-human-trajectory"},{"x":"7.673132","y":"7.2194333","title":"Danny Hernandez on forecasting and the drivers of AI progress","cluster":"2","author":"Arden Koehler and Danny Hernandez","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/?utm_campaign=podcast__danny-hernandez&utm_source=80000+Hours+Podcast&utm_medium=podcast"},{"x":"8.165268","y":"7.0681243","title":"Discussion of \"Takeoff Speeds\"","cluster":"2","author":"Eliezer Yudkowsky and Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":""},{"x":"7.6789947","y":"8.957816","title":"Various trends relevant to AI alignment","cluster":"3","author":"Asya Bergal and Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"8.132291","y":"9.566448","title":"Openness Norms in AGI Development","cluster":"3","author":"Sublation","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/RvrTZ3qKWpg9aiFqZ/openness-norms-in-agi-development"},{"x":"7.9980154","y":"6.2037945","title":"2019 trends in GPU price per FLOPS","cluster":"2","author":"Asya Bergal","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/"},{"x":"8.837496","y":"8.309799","title":"Distinguishing definitions of takeoff","cluster":"4","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff"},{"x":"7.878085","y":"8.998085","title":"Historic trends in technological progress","cluster":"3","author":"AI Impacts","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://forum.effectivealtruism.org/posts/APAD7PaEHgFyW3Nc4/ai-impacts-historic-trends-in-technological-progress"},{"x":"11.139822","y":"8.762657","title":"Coordination Surveys: why we should survey to organize responsibilities, not just predictions","cluster":"3","author":"Andrew Critch","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/Lds9opZsAMbjuZp7h/coordination-surveys-why-we-should-survey-to-organize"},{"x":"9.575737","y":"8.212828","title":"Musings on Cumulative Cultural Evolution and AI","cluster":"4","author":"calebo","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/K686EFdXysfRBdob2/musings-on-cumulative-cultural-evolution-and-ai"},{"x":"8.024327","y":"8.8067","title":"Signup form for AI Metaculus","cluster":"3","author":"Jacob Lagerros and Ben Goldhaber","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://docs.google.com/forms/d/e/1FAIpQLSduBjn3W_MpHHjsKUEhzV6Krkup78ujE5-8bpNJ5HDE7GGnmA/viewform?usp=sf_link"},{"x":"7.5476246","y":"6.591354","title":"Forecasting progress in language models","cluster":"2","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/tepqESMuRmyhtmDS7/forecasting-progress-in-language-models"},{"x":"8.138055","y":"8.143279","title":"AXRP Episode 10 - AI’s Future and Impacts","cluster":"3","author":"Daniel Filan and Katja Grace","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/xbABZRxoSTAnsf8os/axrp-episode-10-ai-s-future-and-impacts-with-katja-grace"},{"x":"7.688846","y":"8.750437","title":"AXRP Episode 7.5 - Forecasting Transformative AI from Biological Anchors","cluster":"3","author":"Daniel Filan and Ajeya Cotra","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/CuDYhLLXq6FuHvGZc/axrp-episode-7-5-forecasting-transformative-ai-from"},{"x":"7.8178754","y":"7.1051536","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning ","cluster":"2","author":"Jian Liu*, Leyang Cui*, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2007.08124"},{"x":"8.668545","y":"6.7789803","title":"How does bee learning compare with machine learning?","cluster":"2","author":"Guilhermo Costa","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning"},{"x":"10.331866","y":"5.744929","title":"Measuring Progress in Deep Reinforcement Learning Sample Efficiency","cluster":"0","author":"Anonymous","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://openreview.net/forum?id=_QdvdkxOii6"},{"x":"8.04823","y":"6.0711145","title":"How Much Computational Power It Takes to Match the Human Brain","cluster":"2","author":"Joseph Carlsmith","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.openphilanthropy.org/blog/new-report-brain-computation"},{"x":"7.5722537","y":"8.851474","title":"Does Economic History Point Toward a Singularity?","cluster":"3","author":"Ben Garfinkel","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity"},{"x":"9.961635","y":"8.255069","title":"Amplified forecasting: What will Buck's informed prediction of compute used in the largest ML training run before 2030 be?","cluster":"2","author":"Ought","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.metaculus.com/questions/4732/amplified-forecasting-what-will-bucks-informed-prediction-of-compute-used-in-the-largest-ml-training-run-before-2030-be/"},{"x":"8.539423","y":"9.643939","title":"Competition: Amplify Rohin’s Prediction on AGI researchers & Safety Concerns","cluster":"3","author":"Andreas Stuhlmüller","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/Azqmzp5JoXJihMcr4/competition-amplify-rohin-s-prediction-on-agi-researchers"},{"x":"7.518429","y":"7.1382923","title":"Alignment As A Bottleneck To Usefulness Of GPT-3","cluster":"2","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3"},{"x":"9.006656","y":"7.4102707","title":"Environments as a bottleneck in AGI development","cluster":"2","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development"},{"x":"7.8670335","y":"8.836452","title":"Reasons you might think human level AI soon is unlikely","cluster":"3","author":"Asya Bergal","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.youtube.com/watch?v=oFJcvyxpGSo"},{"x":"7.66003","y":"8.636209","title":"Surveys on fractional progress towards HLAI","cluster":"3","author":"Asya Bergal","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/"},{"x":"9.426951","y":"8.063329","title":"AlphaStar: Impressive for RL progress, not for AGI progress","cluster":"3","author":"orthonormal","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/SvhzEQkwFGNTy6CsN/alphastar-impressive-for-rl-progress-not-for-agi-progress"},{"x":"9.748804","y":"7.5717697","title":"Two explanations for variation in human abilities","cluster":"4","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/ZwSrTsP3YkgnmHWnJ/two-explanations-for-variability-in-human-abilities"},{"x":"7.739276","y":"8.505722","title":"AI conference attendance","cluster":"3","author":"Katja Grace","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://aiimpacts.org/ai-conference-attendance/"},{"x":"9.012917","y":"7.21249","title":"Primates vs birds: Is one brain architecture better than the other?","cluster":"2","author":"Tegan McCaslin","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://aiimpacts.org/primates-vs-birds-is-one-brain-architecture-better-than-the-other/"},{"x":"10.268393","y":"8.507021","title":"Evidence on good forecasting practices from the Good Judgment Project","cluster":"2","author":"Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project/"},{"x":"8.10633","y":"6.8770094","title":"Reasons compute may not drive AI capabilities growth","cluster":"2","author":"Kythe","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/hSw4MNTc3gAwZWdx9/reasons-compute-may-not-drive-ai-capabilities-growth"},{"x":"7.6933503","y":"8.798984","title":"Thoughts on short timelines","cluster":"3","author":"Tobias Baumann","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://s-risks.org/thoughts-on-short-timelines/"},{"x":"8.418207","y":"9.536114","title":"Distinguishing AI takeover scenarios","cluster":"3","author":"Sam Clarke, Sammy Martin","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios"},{"x":"7.7772837","y":"8.548439","title":"Updates and Lessons from AI Forecasting","cluster":"3","author":"Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://bounded-regret.ghost.io/ai-forecasting/"},{"x":"7.7226944","y":"9.006994","title":"What 2026 looks like","cluster":"3","author":"Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like-daniel-s-median-future"},{"x":"7.772231","y":"8.737265","title":"Fractional progress estimates for AI timelines and implied resource requirements","cluster":"3","author":"Mark Xu, Carl Shulman","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied"},{"x":"8.578847","y":"6.402463","title":"Deep limitations? Examining expert disagreement over deep learning","cluster":"2","author":"Carla Zoe Cremer","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://link.springer.com/article/10.1007/s13748-021-00239-1"},{"x":"8.238018","y":"7.8372364","title":"Interpreting AI Compute Trends","cluster":"3","author":"Ryan Carey","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://aiimpacts.org/interpreting-ai-compute-trends/"},{"x":"7.803223","y":"8.908963","title":"Three reasons to expect long AI timelines","cluster":"3","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/Z5gPrKTR2oDmm6fqJ/three-reasons-to-expect-long-ai-timelines"},{"x":"8.258602","y":"9.41993","title":"2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy","cluster":"3","author":"McKenna Fitzgerald, Aaron Boddy, Seth D. Baum","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/"},{"x":"11.466017","y":"5.4191465","title":"How The Hell Do We Create General-Purpose Robots?","cluster":"0","author":"Sergey Alexashenko","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://howthehell.substack.com/p/general-purpose-robots"},{"x":"9.262954","y":"8.096315","title":"Birds, Brains, Planes, and AI: Against Appeals to the Complexity/Mysteriousness/Efficiency of the Brain","cluster":"4","author":"Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/HhWhaSzQr6xmBki8F/birds-planes-brains-and-ai-against-appeals-to-the-complexity"},{"x":"9.234551","y":"8.270071","title":"How energy efficient are human-engineered flight designs relative to natural ones?","cluster":"4","author":"Ronny Fernandez","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://aiimpacts.org/are-human-engineered-flight-designs-better-or-worse-than-natural-ones/"},{"x":"10.362473","y":"8.805847","title":"Automating reasoning about the future at Ought","cluster":"1","author":"Ought","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ought.org/updates/2020-10-09-forecasting"},{"x":"8.806952","y":"7.9876337","title":"Canaries in Technology Mines: Warning Signs of Transformative Progress in AI","cluster":"4","author":"Carla Zoe Cremer, Jess Whittlestone","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://dmip.webs.upv.es/EPAI2020/papers/EPAI_2020_paper_4.pdf"},{"x":"7.989273","y":"9.595053","title":"Roadmap to a Roadmap: How Could We Tell When AGI is a ‘Manhattan Project’ Away? ","cluster":"3","author":"John-Clark Levin, Matthijs M. Maas","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://dmip.webs.upv.es/EPAI2020/papers/EPAI_2020_paper_11.pdf?fbclid=IwAR15Z0CMX4rBBUJEHhn6NdcMK2ZCF07pPpkcmfD36_oEI9WhV310bRkbaiQ"},{"x":"8.478856","y":"7.2051673","title":"My AI Timelines Have Sped Up","cluster":"2","author":"Alex Irpan","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"7.666606","y":"8.907814","title":"More on disambiguating \"discontinuity\"","cluster":"3","author":"Aryeh Englander","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/C9YMrPAyMXfB8cLPb/more-on-disambiguating-discontinuity"},{"x":"8.657435","y":"8.350821","title":"Will AI undergo discontinuous progress?","cluster":"4","author":"Sammy Martin","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress"},{"x":"9.007146","y":"7.346359","title":"Might humans not be the most intelligent animals?","cluster":"4","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/XjuT9vgBfwXPxsdfN/might-humans-not-be-the-most-intelligent-animals"},{"x":"7.826529","y":"8.427752","title":"AI Forecasting Dictionary","cluster":"2","author":"Jacob Lagerros and Ben Goldhaber","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/8y7DcSF4eAkXoru4u/ai-forecasting-dictionary-forecasting-infrastructure-part-1-2"},{"x":"7.91367","y":"8.745288","title":"AI Forecasting Resolution Council ","cluster":"3","author":"Jacob Lagerros and Ben Goldhaber","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/9G6CCNXkA7JZoorpY/ai-forecasting-resolution-council-forecasting-infrastructure"},{"x":"11.723297","y":"9.434727","title":"[A voting theory primer for rationalists](https://www.lesswrong.com/posts/D6trAzh6DApKPhbv4/a-voting-theory-primer-for-rationalists) and [5 voting pathologies: lesser names of Moloch](https://www.lesswrong.com/posts/4vEFX6EPpdQZfqnnS/5-voting-pathologies-lesser-names-of-moloch)","cluster":"1","author":"Jameson Quinn","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"9.049761","y":"8.7788725","title":"Cooperative AI: machines must learn to find common ground","cluster":"4","author":"Allan Dafoe, Yoram Bachrach, Gillian Hadfield, Eric Horvitz, Kate Larson, Thore Graepel","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.nature.com/articles/d41586-021-01170-0"},{"x":"10.536592","y":"6.3769116","title":"Shaping Safer Goals","cluster":"0","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/s/boLPsyNwd6teK5key"},{"x":"10.647834","y":"6.782217","title":"Collaborating with Humans Requires Understanding Them","cluster":"0","author":"Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia, Pieter Abbeel, Anca Dragan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://bair.berkeley.edu/blog/2019/10/21/coordination/"},{"x":"10.646425","y":"6.845162","title":"Learning Existing Social Conventions via Observationally Augmented Self-Play","cluster":"0","author":"Adam Lerer and Alexander Peysakhovich","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1806.10071"},{"x":"12.396833","y":"8.4638405","title":"Collaborative game specification: arriving at common models in bargaining","cluster":"1","author":"Jesse Clifton","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://longtermrisk.org/collaborative-game-specification/"},{"x":"9.479026","y":"9.238303","title":"Social choice ethics in artificial intelligence","cluster":"4","author":"Seth D Baum","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://link.springer.com/content/pdf/10.1007/s00146-017-0760-1.pdf"},{"x":"11.236464","y":"7.086793","title":"AXRP 3: Negotiable Reinforcement Learning","cluster":"0","author":"Daniel Filan and Andrew Critch","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://axrp.net/episode/2020/12/11/episode-3-negotiable-reinforcement-learning-andrew-critch.html"},{"x":"12.345879","y":"8.313372","title":"What counts as defection?","cluster":"1","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/8LEPDY36jBYpijrSw/formalizing-game-theoretic-defection"},{"x":"11.230362","y":"6.8996773","title":"Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making","cluster":"0","author":"Nishant Desai, Andrew Critch, and Stuart Russell","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"11.558698","y":"7.9003997","title":"Equilibrium and prior selection problems in multipolar deployment","cluster":"1","author":"Jesse Clifton","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1"},{"x":"10.439381","y":"6.3560195","title":"Social Influence as Intrinsic Motivation for Multi-Agent Deep RL","cluster":"0","author":"Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1810.08647"},{"x":"11.709022","y":"7.599189","title":"Weak identifiability and its consequences in strategic settings","cluster":"1","author":"Jesse Clifton","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://longtermrisk.org/weak-identifiability-and-its-consequences-in-strategic-settings/"},{"x":"11.390478","y":"9.4528885","title":"Comparing Utilities","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/cYsGrWEzjb324Zpjx/comparing-utilities"},{"x":"11.748398","y":"8.346463","title":"Theory of Minds: Understanding Behavior in Groups Through Inverse Planning","cluster":"1","author":"Michael Shum, Max Kleiman-Weiner, Michael L. Littman, Joshua B. Tenenbaum","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1901.06085"},{"x":"10.843691","y":"7.503202","title":"Multi-Agent Overoptimization, and Embedded Agent World Models","cluster":"4","author":"David Manheim","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/dfZLLEfFvkrMwmiMw/multi-agent-overoptimization-and-embedded-agent-world-models"},{"x":"11.186719","y":"6.6698313","title":"Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions","cluster":"0","author":"Michael Chang, Sidhant Kaushik, S. Matthew Weinberg, Thomas L. Griffiths, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://bair.berkeley.edu/blog/2020/07/11/auction/"},{"x":"11.551901","y":"6.445556","title":"Learning Reward Machines for Partially Observable Reinforcement Learning","cluster":"0","author":"Rodrigo Toro Icarte, Ethan Waldie, Toryn Q. Klassen, Richard Valenzano, Margarita P. Castro, Sheila A. McIlraith","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://www.cs.toronto.edu/~rntoro/docs/LRM_paper.pdf"},{"x":"11.569094","y":"5.4555616","title":"DADS: Unsupervised Reinforcement Learning for Skill Discovery","cluster":"0","author":"Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ai.googleblog.com/2020/05/dads-unsupervised-reinforcement.html"},{"x":"8.285122","y":"5.8254175","title":"LCA: Loss Change Allocation for Neural Network Training","cluster":"2","author":"Janice Lan, Rosanne Liu, Hattie Zhou, Jason Yosinski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1909.01440"},{"x":"7.95672","y":"5.6016483","title":"Exploring Neural Networks with Activation Atlases","cluster":"2","author":"Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://distill.pub/2019/activation-atlas/"},{"x":"10.75854","y":"8.696241","title":"Interpretability and Post-Rationalization","cluster":"1","author":"Vincent Vanhoucke","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://medium.com/@vanhoucke/interpretability-and-post-rationalization-b812eda13783"},{"x":"8.358698","y":"6.568612","title":"The What-If Tool: Code-Free Probing of Machine Learning Models","cluster":"2","author":"James Wexler","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html"},{"x":"7.8982677","y":"5.5572405","title":"Differentiable Image Parameterizations","cluster":"2","author":"Alexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, and Chris Olah\n","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://distill.pub/2018/differentiable-parameterizations/"},{"x":"8.612646","y":"6.467409","title":"Opinions on Interpretable Machine Learning and 70 Summaries of Recent Papers","cluster":"2","author":"Peter Hase and Owen Shen","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries"},{"x":"8.030413","y":"5.8527284","title":"Circuits Thread","cluster":"2","author":"Various people","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://distill.pub/2020/circuits/"},{"x":"8.069734","y":"5.7116704","title":"Thread: Circuits","cluster":"2","author":"Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://distill.pub/2020/circuits/"},{"x":"8.520371","y":"6.6940804","title":"What the hell is going on inside neural networks","cluster":"2","author":"Rob Wiblin and Chris Olah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/"},{"x":"10.362853","y":"5.760915","title":"Understanding RL Vision","cluster":"0","author":"Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://distill.pub/2020/understanding-rl-vision/"},{"x":"10.433536","y":"5.7455935","title":"Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning","cluster":"0","author":"Akanksha Atrey, Kaleigh Clary, David Jensen","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/1912.05743"},{"x":"10.465144","y":"8.633006","title":"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?","cluster":"1","author":"Peter Hase, Mohit Bansal","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2005.01831"},{"x":"10.368715","y":"5.666424","title":"Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents","cluster":"0","author":"Christian Rupprecht, Cyril Ibrahim, Christopher J. Pal","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/1904.01318"},{"x":"7.919955","y":"5.5276732","title":"Visualizing Neural Networks with the Grand Tour","cluster":"2","author":"Mingwei Li, Zhenge Zhao, Carlos Scheidegger","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://distill.pub/2020/grand-tour/"},{"x":"10.877177","y":"9.103838","title":"Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior","cluster":"1","author":"Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David E. Smith, Subbarao Kambhampati2","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1811.09722"},{"x":"7.820388","y":"6.2186875","title":"Visualizing memorization in RNNs","cluster":"2","author":"Andreas Madsen","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://distill.pub/2019/memorization-in-rnns/"},{"x":"10.035107","y":"5.9758496","title":"Programmatically Interpretable Reinforcement Learning","cluster":"0","author":"Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, Swarat Chaudhuri","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1804.02477"},{"x":"8.667753","y":"6.015881","title":"Techniques for Interpretable Machine Learning","cluster":"2","author":"Mengnan Du, Ninghao Liu, Xia Hu","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1808.00033"},{"x":"10.402372","y":"7.536539","title":"What mechanisms drive agent behaviour?","cluster":"4","author":"Grégoire Déletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, Markus Kunesch, Shane Legg, Pedro A. Ortega","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://medium.com/@deepmindsafetyresearch/what-mechanisms-drive-agent-behaviour-e7b8d9aee88"},{"x":"9.341113","y":"8.195213","title":"Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems","cluster":"4","author":"Zana Buçinca*, Phoebe Lin*, Krzysztof Z. Gajos, Elena L. Glassman","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://glassmanlab.seas.harvard.edu/papers/bucinca_iui20_proxy.pdf"},{"x":"8.035941","y":"5.9449725","title":"Sparsity and interpretability?","cluster":"2","author":"Stanislav Böhm, Robert Kirk, Tomáš Gavenčiak","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/maBNBgopYxb9YZP8B/sparsity-and-interpretability-1"},{"x":"8.271115","y":"8.412798","title":"How can Interpretability help Alignment?","cluster":"3","author":"Robert Kirk, Tomáš Gavenčiak, Flo Dorner","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment"},{"x":"7.847323","y":"5.409831","title":"A Benchmark for Interpretability Methods in Deep Neural Networks","cluster":"2","author":"Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1806.10758"},{"x":"10.313886","y":"5.691255","title":"Towards Interpretable Reinforcement Learning Using Attention Augmented Agents","cluster":"0","author":"Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, Danilo Jimenez Rezende","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://papers.nips.cc/paper/9400-towards-interpretable-reinforcement-learning-using-attention-augmented-agents"},{"x":"10.892349","y":"8.60906","title":"How much can value learning be disentangled?","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/Q7WiHdSSShkNsgDpa/how-much-can-value-learning-be-disentangled"},{"x":"8.926865","y":"9.0596285","title":"Automating Auditing: An ambitious concrete technical research proposal","cluster":"1","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research"},{"x":"11.444012","y":"7.3082385","title":"Contrastive Explanations for Reinforcement Learning in terms of Expected Consequences","cluster":"0","author":"Jasper van der Waa, Jurriaan van Diggelen, Karel van den Bosch, Mark Neerincx","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1807.08706"},{"x":"9.707007","y":"9.41045","title":"Universality and security amplification","cluster":"1","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ai-alignment.com/universality-and-security-amplification-551b314a3bab"},{"x":"9.601227","y":"10.004373","title":"Writeup: Progress on AI Safety via Debate","cluster":"1","author":"Beth Barnes, Paul Christiano, Long Ouyang, Geoffrey Irving","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"},{"x":"8.4689245","y":"7.340916","title":"Evaluating Arguments One Step at a Time","cluster":"2","author":"Ought","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ought.org/updates/2020-01-11-arguments"},{"x":"10.536974","y":"10.082033","title":"Introduction to ascription universality","cluster":"1","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"8.864063","y":"7.374714","title":"Informed oversight (revisited)","cluster":"2","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai-alignment.com/informed-oversight-18fcb5d3d1e1"},{"x":"10.2419615","y":"7.6770535","title":"Worst-case guarantees (Revisited)","cluster":"2","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"},{"x":"11.408299","y":"8.527472","title":"Universality and model-based RL","cluster":"1","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd"},{"x":"12.36136","y":"9.689254","title":"Universality and consequentialism within HCH","cluster":"1","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd"},{"x":"10.744385","y":"10.13534","title":"Towards formalizing universality","cluster":"1","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai-alignment.com/towards-formalizing-universality-409ab893a456"},{"x":"9.720112","y":"7.5027094","title":"Challenges to Christiano’s capability amplification proposal","cluster":"4","author":"Eliezer Yudkowsky","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal"},{"x":"9.510704","y":"9.954618","title":"AI safety via debate","cluster":"4","author":"Geoffrey Irving and Dario Amodei","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/debate/"},{"x":"9.47197","y":"7.0846105","title":"Learning Complex Goals with Iterated Amplification","cluster":"4","author":"Paul Christiano and Dario Amodei","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/amplifying-ai-training/"},{"x":"9.704525","y":"9.855906","title":"Understanding Iterated Distillation and Amplification: Claims and Oversight","cluster":"1","author":"William_S","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/yxzrKb2vFXRkwndQ4/understanding-iterated-distillation-and-amplification-claims"},{"x":"9.743423","y":"10.059117","title":"Debate update: Obfuscated arguments problem","cluster":"1","author":"Beth Barnes, Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"},{"x":"10.392551","y":"9.757048","title":"Factored Cognition (old)","cluster":"1","author":"Andreas Stuhlmuller","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ought.org/presentations/factored-cognition-2018-05"},{"x":"10.417291","y":"9.98988","title":"Nuances with ascription universality","cluster":"1","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/R5Euq7gZgobJi5S25/nuances-with-ascription-universality"},{"x":"8.639041","y":"7.4710345","title":"Ought Progress Update October 2019","cluster":"4","author":"Jungwon Byun and Andreas Stuhlmüller","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ought.org/updates/2019-10-28-progress-update"},{"x":"8.650172","y":"6.5885234","title":"Machine Learning Projects on IDA","cluster":"2","author":"Owain Evans, William Saunders, Andreas Stuhlmüller","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/Y9xD78kufNsF7wL6f/machine-learning-projects-on-ida"},{"x":"10.1029625","y":"7.374994","title":"Aligning a toy model of optimization","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/H5gXpFtg93qDMZ6Xn/aligning-a-toy-model-of-optimization"},{"x":"9.4078045","y":"9.943882","title":"AI Alignment Podcast: AI Alignment through Debate","cluster":"3","author":"Lucas Perry and Geoffrey Irving","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/03/06/ai-alignment-through-debate-with-geoffrey-irving/"},{"x":"10.016829","y":"6.5919056","title":"An unaligned benchmark","cluster":"0","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/ZHXutm7KpoWEj9G2s/an-unaligned-benchmark"},{"x":"9.783224","y":"7.491626","title":"Synthesizing amplification and debate","cluster":"4","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate"},{"x":"9.452316","y":"9.882958","title":"(When) is Truth-telling Favored in AI debate?","cluster":"4","author":"Vojtěch Kovařík, Ryan Carey","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://medium.com/@RyanCarey/new-paper-when-is-truth-telling-favored-in-ai-debate-8f58f14562e5"},{"x":"9.148353","y":"8.321482","title":"Towards a mechanistic understanding of corrigibility","cluster":"4","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility"},{"x":"10.384103","y":"8.8372555","title":"Delegating open-ended cognitive work","cluster":"1","author":"Andreas Stuhlmüller","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ought.org/presentations/delegating-cognitive-work-2019-06"},{"x":"10.235913","y":"8.770513","title":"Disagreement with Paul: alignment induction","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/h3Fp3Erddwnm5uthZ/disagreement-with-paul-alignment-induction"},{"x":"10.092649","y":"8.007193","title":"Can corrigibility be learned safely?","cluster":"4","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/o22kP33tumooBtia3/can-corrigibility-be-learned-safely"},{"x":"9.548567","y":"9.170949","title":"The limits of corrigibility","cluster":"3","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/T5ZyNq3fzN59aQG5y/the-limits-of-corrigibility"},{"x":"9.634363","y":"7.7902055","title":"A comment on the IDA-AlphaGoZero metaphor; capabilities versus alignment","cluster":"4","author":"Alex Mennen","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/yXFKh2jGysQNfX2NM/a-comment-on-the-ida-alphagozero-metaphor-capabilities"},{"x":"10.536479","y":"9.866103","title":"Factored Cognition sequence","cluster":"1","author":"Rafael Harth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.lesswrong.com/s/xezt7HYfpWR6nwp7Z"},{"x":"9.570483","y":"9.962571","title":"Parallels Between AI Safety by Debate and Evidence Law","cluster":"3","author":"Cullen O'Keefe","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/SrsH2MyyH8MqH9QSr/parallels-between-ai-safety-by-debate-and-evidence-law"},{"x":"12.387807","y":"10.202124","title":"Alignment proposals and complexity classes","cluster":"1","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes"},{"x":"9.938069","y":"8.125398","title":"AI safety via market making","cluster":"4","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making"},{"x":"9.556526","y":"9.792232","title":"AI Safety Debate and Its Applications","cluster":"4","author":"Vojta Kovarik","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://alignmentforum.org/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications"},{"x":"9.0503","y":"7.3041496","title":"A Concrete Proposal for Adversarial IDA","cluster":"1","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/jYvm4mmjvGHcPXtGL/a-concrete-proposal-for-adversarial-ida"},{"x":"8.985184","y":"8.225171","title":"Techniques for optimizing worst-case performance","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/E2aZ9Xwdz3i2ghPtn/techniques-for-optimizing-worst-case-performance"},{"x":"11.583663","y":"6.7909536","title":"Thoughts on reward engineering","cluster":"0","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2016.0","url":"https://www.alignmentforum.org/posts/NtX7LKhCXMW2vjWx6/thoughts-on-reward-engineering"},{"x":"10.01515","y":"7.586904","title":"Capability amplification","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2016.0","url":"https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/capability-amplification"},{"x":"11.3444805","y":"6.4462237","title":"The reward engineering problem","cluster":"0","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2016.0","url":"https://www.alignmentforum.org/posts/4nZRzoGTqg8xy5rr8/the-reward-engineering-problem"},{"x":"9.89033","y":"7.3555093","title":"AlphaGo Zero and capability amplification","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2017.0","url":"https://www.alignmentforum.org/posts/HA3oArypzNANvXC38/alphago-zero-and-capability-amplification"},{"x":"9.69818","y":"7.557167","title":"Directions and desiderata for AI alignment","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2017.0","url":"https://www.alignmentforum.org/posts/kphJvksj5TndGapuh/directions-and-desiderata-for-ai-alignment"},{"x":"10.78692","y":"7.059443","title":"Benign model-free RL","cluster":"0","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/PRaxzmDJdvie46ahL/benign-model-free-rl"},{"x":"9.920159","y":"8.582611","title":"Corrigibility","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility"},{"x":"9.601277","y":"7.468398","title":"Iterated Distillation and Amplification","cluster":"4","author":"Ajeya Cotra","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/HqLxuZ4LhaFhmAHWk/iterated-distillation-and-amplification"},{"x":"9.923915","y":"7.4354653","title":"[Approval-directed agents: overview](https://www.alignmentforum.org/posts/7Hr8t6xwuuxBTqADK/approval-directed-agents-overview) and [Approval-directed agents: details](https://www.alignmentforum.org/posts/njRjnmaBnZquqPfBa/approval-directed-agents-details)","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"9.987634","y":"8.9168415","title":"Approval-directed bootstrapping","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/approval-directed-bootstrapping"},{"x":"11.069932","y":"9.748996","title":"Humans Consulting HCH","cluster":"1","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch"},{"x":"10.530725","y":"7.570122","title":"The Steering Problem","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/4iPBctHSeHx8AkS6Z/the-steering-problem"},{"x":"11.726601","y":"6.029614","title":"The Assistive Multi-Armed Bandit","cluster":"0","author":"Lawrence Chan, Dylan Hadfield-Menell, Siddhartha Srinivasa, Anca Dragan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1901.08654"},{"x":"11.533026","y":"5.7539477","title":"Cooperative Inverse Reinforcement Learning","cluster":"0","author":"Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell","source":"alignment newsletter","tags":"[]","date":"2016.0","url":"https://arxiv.org/abs/1606.03137"},{"x":"11.747355","y":"7.5837708","title":"The Off-Switch Game","cluster":"1","author":"Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell","source":"alignment newsletter","tags":"[]","date":"2017.0","url":"https://arxiv.org/abs/1611.08219"},{"x":"11.617383","y":"6.27528","title":"Inverse Reward Design","cluster":"0","author":"Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, Anca Dragan","source":"alignment newsletter","tags":"[]","date":"2017.0","url":"https://arxiv.org/abs/1711.02827"},{"x":"10.851397","y":"5.816771","title":"Learning to Interactively Learn and Assist","cluster":"0","author":"Mark Woodward, Chelsea Finn, Karol Hausman","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://interactive-learning.github.io"},{"x":"9.905027","y":"6.6042733","title":"Why Build an Assistant in Minecraft?","cluster":"4","author":"Arthur Szlam, Jonathan Gray, Kavya Srinet, Yacine Jernite, Armand Joulin, Gabriel Synnaeve, Douwe Kiela, Haonan Yu, Zhuoyuan Chen, Siddharth Goyal, Demi Guo, Danielle Rothermel, C. Lawrence Zitnick, Jason Weston","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1907.09273"},{"x":"9.002229","y":"8.967837","title":"Thoughts on Human Models","cluster":"4","author":"Ramana Kumar and Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models"},{"x":"11.587765","y":"7.0860596","title":"Learning Preferences by Looking at the World","cluster":"0","author":"Rohin Shah and Dmitrii Krasheninnikov","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/"},{"x":"10.640886","y":"6.4284353","title":"AI Alignment Podcast: Cooperative Inverse Reinforcement Learning","cluster":"0","author":"Lucas Perry and Dylan Hadfield-Menell","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/01/17/cooperative-inverse-reinforcement-learning-with-dylan-hadfield-menell/"},{"x":"9.617991","y":"8.382193","title":"Non-Consequentialist Cooperation?","cluster":"4","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/F9vcbEMKW48j4Z6h9/non-consequentialist-cooperation"},{"x":"11.122993","y":"9.202428","title":"Why we need a *theory* of human values","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values"},{"x":"10.173711","y":"6.204205","title":"Scalable agent alignment via reward modeling","cluster":"0","author":"Jan Leike","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"},{"x":"10.547044","y":"5.7693973","title":"Reward learning from human preferences and demonstrations in Atari","cluster":"0","author":"Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, Dario Amodei","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1811.06521"},{"x":"11.401814","y":"5.3347654","title":"Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human Videos","cluster":"0","author":"Annie S. Chen, Suraj Nair, Chelsea Finn","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://sites.google.com/view/dvd-human-videos"},{"x":"10.058875","y":"6.889363","title":"BASALT: A Benchmark for Learning from Human Feedback","cluster":"0","author":"Rohin Shah, Cody Wild, Steven H. Wang, Neel Alex, Brandon Houghton, William Guss, Sharada Mohanty, Anssi Kanervisto, Stephanie Milani, Nicholay Topin, Pieter Abbeel, Stuart Russell, Anca Dragan","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://bair.berkeley.edu/blog/2021/07/08/basalt/"},{"x":"11.239314","y":"5.3200283","title":"One-Shot Imitation from Watching Videos","cluster":"0","author":"Tianhe Yu and Chelsea Finn","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://bair.berkeley.edu/blog/2018/06/28/daml/"},{"x":"11.189074","y":"5.3456006","title":"Grounding Language in Play","cluster":"0","author":"Corey Lynch, Pierre Sermanet","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://language-play.github.io/"},{"x":"11.636689","y":"6.6088166","title":"Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition","cluster":"0","author":"Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1805.11686"},{"x":"11.714303","y":"5.6798844","title":"LESS is More: Rethinking Probabilistic Models of Human Behavior","cluster":"0","author":"Andreea Bobu*, Dexter R.R. Scobee*, Jaime F. Fisac, S. Shankar Sastry, Anca D. Dragan","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2001.04465"},{"x":"11.388581","y":"5.9854093","title":"Meta-Inverse Reinforcement Learning with Probabilistic Context Variables","cluster":"0","author":"Lantao Yu*, Tianhe Yu*, Chelsea Finn, Stefano Ermon","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1909.09314"},{"x":"11.106599","y":"5.814262","title":"Deep Bayesian Reward Learning from Preferences","cluster":"0","author":"Daniel S. Brown, Scott Niekum","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1912.04472"},{"x":"11.05669","y":"5.8944645","title":"Learning human objectives by evaluating hypothetical behaviours","cluster":"0","author":"Siddharth Reddy, Anca D. Dragan, Sergey Levine, Shane Legg, Jan Leike","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://deepmind.com/blog/article/learning-human-objectives-by-evaluating-hypothetical-behaviours"},{"x":"10.573245","y":"6.394117","title":"Causal Confusion in Imitation Learning","cluster":"0","author":"Pim de Haan, Dinesh Jayaraman, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1905.11979"},{"x":"11.055988","y":"7.5620184","title":"Norms, Rewards, and the Intentional Stance: Comparing Machine Learning Approaches to Ethical Training","cluster":"0","author":"Daniel Kasenberg, Thomas Arnold, Matthias Scheutz","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://hrilab.tufts.edu/publications/kasenbergetal18aies.pdf"},{"x":"10.855611","y":"5.6994705","title":"Leveraging Human Guidance for Deep Reinforcement Learning Tasks","cluster":"0","author":"Ruohan Zhang, Faraz Torabi, Lin Guan, Dana H. Ballard, Peter Stone","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1909.09906"},{"x":"7.6507764","y":"6.814256","title":"Fine-Tuning GPT-2 from Human Preferences","cluster":"2","author":"Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/fine-tuning-gpt-2/"},{"x":"11.418475","y":"6.327042","title":"Learning biases and rewards simultaneously","cluster":"0","author":"Rohin Shah, Noah Gundotra, Pieter Abbeel, Anca D. Dragan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/xxnPxELC4jLKaFKqG/learning-biases-and-rewards-simultaneously"},{"x":"10.951054","y":"8.695424","title":"Cognitive Model Priors for Predicting Human Decisions","cluster":"2","author":"David D. Bourgin*, Joshua C. Peterson*, Daniel Reichman, Thomas L. Griffiths, Stuart J. Russell","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1905.09397"},{"x":"7.6419873","y":"6.763933","title":"Prompting: Better Ways of Using Language Models for NLP Tasks","cluster":"2","author":"Tianyu Gao","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://thegradient.pub/prompting/"},{"x":"11.217674","y":"5.9586506","title":"Transfer Reinforcement Learning across Homotopy Classes","cluster":"0","author":"Zhangjie Cao*, Minae Kwon*, Dorsa Sadigh","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://arxiv.org/abs/2102.05207"},{"x":"11.375393","y":"6.342898","title":"Bayesian Inverse Reinforcement Learning","cluster":"0","author":"Deepak Ramachandran, Eyal Amir","source":"alignment newsletter","tags":"[]","date":"2007.0","url":"https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf"},{"x":"9.843346","y":"7.5814657","title":"AXRP 2: Learning Human Biases","cluster":"4","author":"Daniel Filan and Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://axrp.net/episode/2020/12/11/episode-2-learning-human-biases-rohin-shah.html"},{"x":"10.99555","y":"5.9907055","title":"Imitation Learning in the Low-Data Regime","cluster":"0","author":"Robert Dadashi, Léonard Hussenot, Matthieu Geist, Olivier Pietquin","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ai.googleblog.com/2020/09/imitation-learning-in-low-data-regime.html"},{"x":"11.438749","y":"5.557889","title":"Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery","cluster":"0","author":"Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/1907.08225"},{"x":"7.6562567","y":"6.7927628","title":"Learning to Summarize with Human Feedback","cluster":"2","author":"Nisan Stiennon*, Long Ouyang*, Jeff Wu*, Daniel M. Ziegler*, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://openai.com/blog/learning-to-summarize-with-human-feedback/"},{"x":"10.836577","y":"7.2135878","title":"Multi-Principal Assistance Games","cluster":"0","author":"Arnaud Fickinger, Simon Zhuang, Dylan Hadfield-Menell, Stuart Russell","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://arxiv.org/abs/2007.09540"},{"x":"11.2585","y":"5.547187","title":"Showing versus doing: Teaching by demonstration","cluster":"0","author":"Mark K. Ho, Michael Littman, James MacGlashan, Fiery Cushman, Joseph L. Austerweil","source":"alignment newsletter","tags":"[]","date":"2016.0","url":"http://papers.nips.cc/paper/6412-showing-versus-doing-teaching-by-demonstration"},{"x":"11.544893","y":"5.5579896","title":"Active Reward Learning","cluster":"0","author":"Christian Daniel, Malte Viering, Jan Metz, Oliver Kroemer, Jan Peters\n","source":"alignment newsletter","tags":"[]","date":"2014.0","url":"http://www.roboticsproceedings.org/rss10/p31.pdf"},{"x":"11.045199","y":"6.3742476","title":"Imitation Learning via Off-Policy Distribution Matching","cluster":"0","author":"Ilya Kostrikov, Ofir Nachum, Jonathan Tompson","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://openreview.net/forum?id=Hyg-JC4FDr"},{"x":"11.115017","y":"5.8084016","title":"State-only Imitation with Transition Dynamics Mismatch","cluster":"0","author":"Tanmay Gangwani, Jian Peng","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://arxiv.org/abs/2002.11879"},{"x":"11.201029","y":"5.7374077","title":"Goal-conditioned Imitation Learning","cluster":"0","author":"Yiming Ding*, Carlos Florensa*, Mariano Phielipp, Pieter Abbeel","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1906.05838"},{"x":"10.863059","y":"9.176808","title":"The two-layer model of human values, and problems with synthesizing preferences","cluster":"1","author":"Kaj Sotala","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.lesswrong.com/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with"},{"x":"11.394824","y":"9.571836","title":"Using vector fields to visualise preferences and make them consistent","cluster":"1","author":"Michael Aird, Justin Shovelain","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.lesswrong.com/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them"},{"x":"11.336465","y":"5.2722964","title":"Learning to Imitate Human Demonstrations via CycleGAN","cluster":"0","author":"Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/"},{"x":"10.060755","y":"9.092139","title":"Humans Are Embedded Agents Too","cluster":"4","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too"},{"x":"10.257581","y":"8.966489","title":"AI Alignment Podcast: Synthesizing a human’s preferences into a utility function","cluster":"4","author":"Lucas Perry and Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/09/17/synthesizing-a-humans-preferences-into-a-utility-function-with-stuart-armstrong/"},{"x":"10.613246","y":"8.757006","title":"Research Agenda v0.9: Synthesising a human's preferences into a utility function","cluster":"4","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into"},{"x":"8.938463","y":"7.4555283","title":"Some Comments on Stuart Armstrong's \"Research Agenda v0.9\"","cluster":"4","author":"Charlie Steiner","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://alignmentforum.org/posts/GHNokcgERpLJwJnLW/some-comments-on-stuart-armstrong-s-research-agenda-v0-9?_ga=2.216737811.48011077.1562349688-943761554.1470242885"},{"x":"11.426105","y":"6.7707863","title":"Delegative Reinforcement Learning","cluster":"0","author":"Vanessa Kosoy","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://intelligence.org/2019/04/24/delegative-reinforcement-learning/"},{"x":"11.549105","y":"6.2601314","title":"Batch Active Preference-Based Learning of Reward Functions","cluster":"0","author":"Erdem Bıyık and  Dorsa Sadigh","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://iliad.stanford.edu/blog/2018/10/06/batch-active-preference-based-learning-of-reward-functions/"},{"x":"11.501426","y":"5.496649","title":"End-to-End Robotic Reinforcement Learning without Reward Engineering","cluster":"0","author":"Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://sites.google.com/view/reward-learning-rl/home"},{"x":"11.286465","y":"9.035027","title":"Conditional revealed preference","cluster":"1","author":"Jessica Taylor","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://unstableontology.com/2019/04/04/conditional-revealed-preference/"},{"x":"11.048552","y":"5.5483","title":"Reward Learning from Narrated Demonstrations","cluster":"0","author":"Hsiao-Yu Fish Tung, Adam W. Harley, Liang-Kang Huang, Katerina Fragkiadaki","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1804.10692"},{"x":"9.396173","y":"7.954805","title":"AI Alignment Podcast: Human Cognition and the Nature of Intelligence","cluster":"4","author":"Lucas Perry and Joshua Greene","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/02/21/human-cognition-and-the-nature-of-intelligence-with-joshua-greene/"},{"x":"9.716073","y":"9.117419","title":"What AI Safety Researchers Have Written About the Nature of Human Values","cluster":"4","author":"avturchin","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/GermiEmcS6xuZ2gBh/what-ai-safety-researchers-have-written-about-the-nature-of"},{"x":"10.747015","y":"6.2548504","title":"Risk-Sensitive Generative Adversarial Imitation Learning","cluster":"0","author":"Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, Marco Pavone","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1808.04468"},{"x":"11.368222","y":"6.3551207","title":"Inverse Decision Modeling: Learning Interpretable Representations of Behavior","cluster":"0","author":"Daniel Jarrett*, Alihan Hüyük*, Mihaela van der Schaar","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"http://proceedings.mlr.press/v139/jarrett21a/jarrett21a.pdf"},{"x":"11.177742","y":"5.615561","title":"B-Pref: Benchmarking Preference-Based Reinforcement Learning","cluster":"0","author":"Kimin Lee, Laura Smith, Anca Dragan, Pieter Abbeel","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://openreview.net/forum?id=ps95-mkHF_"},{"x":"11.422716","y":"6.475129","title":"Reward Identification in Inverse Reinforcement Learning","cluster":"0","author":"Kuno Kim, Kirankumar Shiragur, Shivam Garg, Stefano Ermon","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"http://proceedings.mlr.press/v139/kim21c/kim21c.pdf"},{"x":"11.14814","y":"5.8629694","title":"Exploring Hierarchy-Aware Inverse Reinforcement Learning","cluster":"0","author":"Chris Cundy, Daniel Filan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1807.05037"},{"x":"11.105033","y":"7.9099746","title":"IBM researchers train AI to follow code of ethics","cluster":"2","author":"Ben Dickson","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://venturebeat.com/2018/07/16/ibm-researchers-train-ai-to-follow-code-of-ethics/"},{"x":"10.904133","y":"5.965157","title":"Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation","cluster":"0","author":"Sam Devlin*, Raluca Georgescu*, Ida Momennejad*, Jaroslaw Rzepecki*, Evelyn Zuniga*, Gavin Costello, Guy Leroy, Ali Shaw, Katja Hofmann","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://arxiv.org/abs/2105.09637"},{"x":"11.205176","y":"5.919328","title":"Learning What To Do by Simulating the Past","cluster":"0","author":"David Lindner, Rohin Shah, Pieter Abbeel, Anca Dragan","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://bair.berkeley.edu/blog/2021/05/03/rlsp/"},{"x":"10.847648","y":"6.233526","title":"Recursive Classification: Replacing Rewards with Examples in RL","cluster":"0","author":"Benjamin Eysenbach, Sergey Levine, Ruslan Salakhutdinov","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://ai.googleblog.com/2021/03/recursive-classification-replacing.html"},{"x":"10.787446","y":"8.182262","title":"Four Motivations for Learning Normativity","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/oqghwKKifztYWLsea/four-normativity-motivations"},{"x":"10.515872","y":"5.8325644","title":"Learning Montezuma’s Revenge from a Single Demonstration","cluster":"0","author":"Tim Salimans and Richard Chen","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/"},{"x":"10.87674","y":"5.701991","title":"Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning","cluster":"0","author":"Valerie Chen, Abhinav Gupta, Kenneth Marino","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2011.00517"},{"x":"9.612282","y":"8.806567","title":"Learning Normativity: A Research Agenda","cluster":"4","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda"},{"x":"11.193102","y":"9.208634","title":"\"Go west, young man!\" - Preferences in (imperfect) maps","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps"},{"x":"11.199042","y":"5.654423","title":"Learning a Prior over Intent via Meta-Inverse Reinforcement Learning","cluster":"0","author":"Kelvin Xu, Ellis Ratner, Anca Dragan, Sergey Levine, Chelsea Finn","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1805.12573"},{"x":"11.182359","y":"5.412399","title":"Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement","cluster":"0","author":"Chao Yang*, Xiaojian Ma*, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, Chuang Gan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1910.04417"},{"x":"11.365917","y":"5.8303957","title":"Multi-task Maximum Entropy Inverse Reinforcement Learning","cluster":"0","author":"Adam Gleave, Oliver Habryka","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"11.515393","y":"9.792224","title":"On the Foundations of Expected Expected Utility","cluster":"1","author":"Craig Boutilier","source":"alignment newsletter","tags":"[]","date":"2003.0","url":""},{"x":"9.105166","y":"8.088482","title":"What's the dream for giving natural language commands to AI?","cluster":"4","author":"Charlie Steiner","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/Bxxh9GbJ6WuW5Hmkj/what-s-the-dream-for-giving-natural-language-commands-to-ai"},{"x":"10.466423","y":"7.698494","title":"Defeating Goodhart and the closest unblocked strategy problem","cluster":"4","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"},{"x":"10.897188","y":"5.399533","title":"From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following","cluster":"0","author":"Justin Fu, Anoop Korattikara, Sergey Levine, Sergio Guadarrama","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1902.07742"},{"x":"11.259174","y":"6.2432055","title":"Risk-Aware Active Inverse Reinforcement Learning","cluster":"0","author":"Daniel S. Brown, Yuchen Cui, Scott Niekum","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1901.02161"},{"x":"10.843501","y":"8.757533","title":"Acknowledging Human Preference Types to Support Value Learning","cluster":"1","author":"Nandi, Sabrina, and Erin","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/mSPsyEwaymS74unND/acknowledging-human-preference-types-to-support-value"},{"x":"10.897657","y":"10.126836","title":"[Web of connotations: Bleggs, Rubes, thermostats and beliefs](https://www.alignmentforum.org/posts/ix3KdfJxjo9GQFkCo/web-of-connotations-bleggs-rubes-thermostats-and-beliefs), [Bridging syntax and semantics, empirically](https://www.alignmentforum.org/posts/EEPdbtvW8ei9Yi2e8/bridging-syntax-and-semantics-empirically) and [Bridging syntax and semantics with Quine's Gavagai](https://www.alignmentforum.org/posts/XApNuXPckPxwp5ZcW/bridging-syntax-and-semantics-with-quine-s-gavagai)","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"11.643683","y":"6.1711946","title":"Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization","cluster":"0","author":"Sreejith Balakrishnan, Quoc Phong Nguyen, Bryan Kian Hsiang Low, Harold Soh","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://papers.nips.cc/paper/2020/file/2bba9f4124283edd644799e0cecd45ca-Paper.pdf"},{"x":"7.804267","y":"7.5352745","title":"Toy Problem: Detective Story Alignment","cluster":"2","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/4kYkYSKSALH4JaQ99/toy-problem-detective-story-alignment"},{"x":"10.8338175","y":"5.9376473","title":"Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning","cluster":"0","author":"Lin Guan*, Mudit Verma*, Subbarao Kambhampati","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://arxiv.org/abs/2006.14804"},{"x":"10.589509","y":"5.5749326","title":"Learning a Behavioral Repertoire from Demonstrations","cluster":"0","author":"Niels Justesen*, Miguel Gonzalez Duque*, Daniel Cabarcas Jaramillo, Jean-Baptiste Mouret, Sebastian Risi","source":"alignment newsletter","tags":"[]","date":"2019.0","url":""},{"x":"10.008657","y":"9.158166","title":"Deconfusing Human Values Research Agenda v1","cluster":"4","author":"G Gordon Worley III","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1"},{"x":"11.121824","y":"5.676325","title":"Cycle-of-Learning for Autonomous Systems from Human Interaction","cluster":"0","author":"Nicholas R. Waytowich, Vinicius G. Goecks, Vernon J. Lawhern","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1808.09572"},{"x":"10.747954","y":"6.3366","title":"Directed Policy Gradient for Safe Reinforcement Learning with Human Advice","cluster":"0","author":"Hélène Plisnier, Denis Steckelmacher, Tim Brys, Diederik M. Roijers, Ann Nowé","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1808.04096"},{"x":"8.352034","y":"5.777586","title":"Quantifying Independently Reproducible Machine Learning","cluster":"2","author":"Edward Raff","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://thegradient.pub/independently-reproducible-machine-learning/"},{"x":"7.9582653","y":"5.330523","title":"Fluid Annotation: An Exploratory Machine Learning–Powered Interface for Faster Image Annotation","cluster":"2","author":"Jasper Uijlings and Vittorio Ferrari","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ai.googleblog.com/2018/10/fluid-annotation-exploratory-machine.html"},{"x":"9.836082","y":"6.869371","title":"Meta-Learning MCMC Proposals","cluster":"2","author":"Tongzhou Wang, Yi Wu, David A. Moore, and Stuart J. Russell","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"8.223777","y":"5.6543183","title":"A Theory of Universal Learning","cluster":"2","author":"Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon van Handel, Amir Yehudayoff","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"10.523193","y":"7.681867","title":"Demons in Imperfect Search","cluster":"1","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search"},{"x":"10.036496","y":"7.6761613","title":"Tessellating Hills: a toy model for demons in imperfect search","cluster":"4","author":"DaemonicSigil","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect"},{"x":"9.573902","y":"6.621635","title":"Towards an empirical investigation of inner alignment","cluster":"0","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/2GycxikGnepJbxfHT/towards-an-empirical-investigation-of-inner-alignment"},{"x":"9.923396","y":"6.9984617","title":"A simple environment for showing mesa misalignment","cluster":"4","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/AFdRGfYDWQqmkdhFq/a-simple-environment-for-showing-mesa-misalignment"},{"x":"9.119219","y":"6.371502","title":"2-D Robustness","cluster":"2","author":"Vladimir Mikulik","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness"},{"x":"9.090544","y":"6.567152","title":"Risks from Learned Optimization in Advanced Machine Learning Systems","cluster":"2","author":"Evan Hubinger*, Chris Van Merwijk*, Vladimir Mikulik*, Joar Skalse*, Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1906.01820"},{"x":"9.918256","y":"6.5685678","title":"Empirical Observations of Objective Robustness Failures","cluster":"0","author":"Jack Koch*, Lauro Langosco*, Jacob Pfau, James Le, Lee Sharkey","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures"},{"x":"9.840234","y":"7.3905697","title":"Malign generalization without internal search","cluster":"4","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search"},{"x":"9.064331","y":"7.0982394","title":"Relaxed adversarial training for inner alignment","cluster":"4","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment"},{"x":"9.564312","y":"6.659654","title":"Concrete experiments in inner alignment","cluster":"0","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inner-alignment"},{"x":"9.408229","y":"6.6876845","title":"Discussion: Objective Robustness and Inner Alignment Terminology","cluster":"2","author":"Jack Koch and Lauro Langosco","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment"},{"x":"9.29789","y":"7.029961","title":"Inner alignment requires making assumptions about human values","cluster":"4","author":"Matthew Barnett ","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human"},{"x":"9.363685","y":"6.7062874","title":"Is the term mesa optimizer too narrow?","cluster":"2","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow"},{"x":"9.585707","y":"7.1402354","title":"Are minimal circuits deceptive?","cluster":"4","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive"},{"x":"11.299438","y":"8.349728","title":"Impact measurement and value-neutrality verification","cluster":"1","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/jGB7Pd5q8ivBor8Ee/impact-measurement-and-value-neutrality-verification-1"},{"x":"9.436627","y":"6.57886","title":"AXRP #4 - Risks from Learned Optimization","cluster":"0","author":"Daniel Filan and Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://axrp.net/episode/2021/02/17/episode-4-risks-from-learned-optimization-evan-hubinger.html"},{"x":"9.777746","y":"6.586016","title":"Formal Solution to the Inner Alignment Problem","cluster":"0","author":"Michael K. Cohen, Marcus Hutter, Neel Nanda","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/CnruhwFGQBThvgJiX/formal-solution-to-the-inner-alignment-problem"},{"x":"8.3335085","y":"5.7450495","title":"Fixing The Good Regulator Theorem","cluster":"2","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem"},{"x":"8.698048","y":"6.182868","title":"Defining capability and alignment in gradient descent","cluster":"2","author":"Edouard Harris","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/Xg2YycEfCnLYrCcjy/defining-capability-and-alignment-in-gradient-descent"},{"x":"8.812894","y":"7.0825834","title":"Will transparency help catch deception? Perhaps not","cluster":"4","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/J9D6Bi3eFDDhCaovi/will-transparency-help-catch-deception-perhaps-not"},{"x":"9.479534","y":"6.6970315","title":"More variations on pseudo-alignment","cluster":"0","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/iydwbZhATANhjoGP7/more-variations-on-pseudo-alignment"},{"x":"9.193665","y":"6.054478","title":"Meta-Learning with Implicit Gradients","cluster":"2","author":"Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1909.04630"},{"x":"11.227385","y":"7.9139285","title":"Understanding meta-trained algorithms through a Bayesian lens","cluster":"1","author":"Vladimir Mikulik*, Grégoire Delétang*, Tom McGrath*, Tim Genewein*, Miljan Martic, Shane Legg, Pedro A. Ortega","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://medium.com/@deepmindsafetyresearch/understanding-meta-trained-algorithms-through-a-bayesian-lens-5042a1acc1c2"},{"x":"8.41242","y":"5.97117","title":"Unreproducible Research is Reproducible","cluster":"2","author":"Xavier Bouthillier, Cesar Laurent, Pascal Vincent","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://proceedings.mlr.press/v97/bouthillier19a.html"},{"x":"9.609413","y":"6.0063357","title":"The Bitter Lesson","cluster":"2","author":"Rich Sutton","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://www.incompleteideas.net/IncIdeas/BitterLesson.html"},{"x":"8.150863","y":"8.803681","title":"2021 AI Index Report","cluster":"3","author":"Daniel Zhang, Saurabh Mishra, Erik Brynjolfsson, John Etchemendy, Deep Ganguli, Barbara Grosz, Terah Lyons, James Manyika, Juan Carlos Niebles, Michael Sellitto, Yoav Shoham, Jack Clark, Raymond Perrault","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://aiindex.stanford.edu/report/"},{"x":"9.2248335","y":"7.78418","title":"Explainable AI, Sparse Representations, and Signals","cluster":"4","author":"nan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.notion.so/Explainable-AI-Sparse-Representations-and-Signals-fedf1522aff4415d8f156e1f94bb80c5"},{"x":"8.147063","y":"8.533618","title":"State of AI Report 2021","cluster":"3","author":"Nathan Benaich and Ian Hogarth","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.stateof.ai/2021-report-launch.html"},{"x":"7.973643","y":"8.406933","title":"State of AI Report 2020","cluster":"3","author":"Nathan Benaich and Ian Hogarth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.stateof.ai/"},{"x":"7.9177785","y":"5.675152","title":"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense","cluster":"2","author":"Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum, Song-Chun Zhu","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2004.09044"},{"x":"10.131233","y":"8.163831","title":"Predicting Slow Judgments","cluster":"2","author":"Andreas Stuhlmueller, Owain Evans, Tom McGrath, Zac Kenton, Chris Cundy, Ryan Carey, Andrew Schreiber, Neal Jean, Girish Sastry","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ought.org/projects/judgments"},{"x":"8.746826","y":"7.9553795","title":"The Precipice: Existential Risk and the Future of Humanity","cluster":"4","author":"Toby Ord","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911"},{"x":"8.14314","y":"9.619985","title":"2019 AI Alignment Literature Review and Charity Comparison","cluster":"3","author":"Larks","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/SmDziGM9hBjW9DKmf/2019-ai-alignment-literature-review-and-charity-comparison"},{"x":"8.771943","y":"7.520875","title":"I'm Buck Shlegeris, I do research and outreach at MIRI, AMA","cluster":"4","author":"Buck Shlegeris","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://forum.effectivealtruism.org/posts/tDk57GhrdK54TWzPY/i-m-buck-shlegeris-i-do-research-and-outreach-at-miri-ama"},{"x":"9.311033","y":"8.161751","title":"Human Compatible: Artificial Intelligence and the Problem of Control","cluster":"4","author":"Stuart Russell","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-ebook/dp/B07N5J5FTS"},{"x":"9.485677","y":"8.104259","title":"AI Alignment Podcast: Human Compatible: Artificial Intelligence and the Problem of Control","cluster":"4","author":"Lucas Perry and Stuart Russell","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/10/08/ai-alignment-podcast-human-compatible-artificial-intelligence-and-the-problem-of-control-with-stuart-russell/"},{"x":"7.661408","y":"7.6715527","title":"Alignment Newsletter One Year Retrospective","cluster":"2","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/3onCb5ph3ywLQZMX2/alignment-newsletter-one-year-retrospective"},{"x":"9.899864","y":"7.255123","title":"[Quantilizers: A Safer Alternative to Maximizers for Limited Optimization](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) and [When to use quantilization](https://www.alignmentforum.org/posts/Rs6vZCrnQFWQ4p37P/when-to-use-quantilization)","cluster":"4","author":"Jessica Taylor and Ryan Carey","source":"alignment newsletter","tags":"[]","date":"2015.0","url":""},{"x":"8.384861","y":"6.518286","title":"How does Gradient Descent Interact with Goodhart?","cluster":"2","author":"Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/pcomQ4Fwi7FnfBZBR/how-does-gradient-descent-interact-with-goodhart"},{"x":"7.998636","y":"8.679572","title":"80K podcast with Katja Grace","cluster":"3","author":"Katja Grace and Rob Wiblin","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://80000hours.org/podcast/episodes/katja-grace-forecasting-technology/"},{"x":"8.747107","y":"7.886017","title":"The \"most important century\" series","cluster":"4","author":"Holden Karnofsky","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.cold-takes.com/most-important-century/"},{"x":"8.215632","y":"6.873445","title":"Troubling Trends in ML Scholarship","cluster":"2","author":"Zachary C. Lipton and Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1807.03341"},{"x":"7.8385878","y":"7.71053","title":"Editorial policy for the Alignment Newsletter","cluster":"2","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":""},{"x":"7.7580137","y":"7.7350755","title":"Alignment Newsletter Three Year Retrospective","cluster":"2","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/L7yHdqRiHKd3FhQ7B/alignment-newsletter-three-year-retrospective"},{"x":"10.697525","y":"9.238059","title":"Literature Review on Goal-Directedness","cluster":"1","author":"Adam Shimi, Michele Campolo, Joe Collman","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/cfXwr6NC9AqZ9kr8g/literature-review-on-goal-directedness"},{"x":"8.925704","y":"8.573559","title":"AGI safety from first principles","cluster":"4","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ"},{"x":"8.918991","y":"6.9990697","title":"The Alignment Problem","cluster":"4","author":"Brian Christian","source":"alignment newsletter","tags":"[]","date":"nan","url":"https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/153669519X"},{"x":"8.800989","y":"8.672968","title":"Engineering a Safer World","cluster":"4","author":"Nancy G. Leveson","source":"alignment newsletter","tags":"[]","date":"2011.0","url":"https://static1.squarespace.com/static/53b78765e4b0949940758017/t/57d87eb6d2b8571af3501b26/1473898764674/Engineering_a_Safer_World+Nancy+Leveson.pdf"},{"x":"9.498291","y":"8.7828665","title":"The Basic AI Drives","cluster":"4","author":"Stephen M. Omohundro","source":"alignment newsletter","tags":"[]","date":"2008.0","url":"https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf"},{"x":"7.9938765","y":"9.510039","title":"Possible takeaways from the coronavirus pandemic for slow AI takeoff","cluster":"3","author":"Victoria Krakovna","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/wTKjRFeSjKLDSWyww/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai"},{"x":"10.207168","y":"7.1487613","title":"An overview of 11 proposals for building safe advanced AI","cluster":"4","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai"},{"x":"8.644836","y":"9.817025","title":"My personal cruxes for working on AI safety","cluster":"3","author":"Buck Shlegeris","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety"},{"x":"8.8622465","y":"9.730541","title":"A list of good heuristics that the case for AI x-risk fails","cluster":"3","author":"David Krueger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails"},{"x":"8.761558","y":"8.385175","title":"Chris Olah’s views on AGI safety","cluster":"4","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"},{"x":"11.041025","y":"8.349952","title":"When does rationality-as-search have nontrivial implications?","cluster":"1","author":"nostalgebraist","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/BGcXEijZ6HLnASNit/when-does-rationality-as-search-have-nontrivial-implications"},{"x":"12.256141","y":"8.238635","title":"Standard ML Oracles vs Counterfactual ones","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/hJaJw6LK39zpyCKW6/standard-ml-oracles-vs-counterfactual-ones"},{"x":"8.818329","y":"9.846908","title":"Beyond fire alarms: freeing the groupstruck","cluster":"3","author":"Katja Grace","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://aiimpacts.org/beyond-fire-alarms-freeing-the-groupstruck/"},{"x":"8.758225","y":"9.504877","title":"\"Existential risk from AI\" survey results","cluster":"3","author":"Rob Bensinger","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results"},{"x":"9.104432","y":"8.008738","title":"Testing The Natural Abstraction Hypothesis: Project Intro","cluster":"4","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro"},{"x":"9.467472","y":"8.863719","title":"My research methodology","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/EF5M6CmKRd6qZk27Z/my-research-methodology"},{"x":"10.656866","y":"8.22763","title":"Alignment By Default","cluster":"2","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"10.811422","y":"8.65496","title":"User-Agent Value Alignment ","cluster":"1","author":"Daniel Shapiro, Ross Shachter","source":"alignment newsletter","tags":"[]","date":"2002.0","url":"https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-07/SS02-07-002.pdf"},{"x":"9.095811","y":"9.032671","title":"Survey of prescient actions","cluster":"4","author":"Rick Korzekwa","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://aiimpacts.org/survey-of-prescient-actions/"},{"x":"11.222481","y":"8.011017","title":"Bayesian Evolving-to-Extinction","cluster":"2","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction"},{"x":"8.034582","y":"9.677534","title":"Cortés, Pizarro, and Afonso as Precedents for Takeover","cluster":"3","author":"Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://aiimpacts.org/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover/"},{"x":"11.559778","y":"8.903136","title":"The Incentives that Shape Behaviour","cluster":"1","author":"Ryan Carey*, Eric Langlois*, Tom Everitt, Shane Legg","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://medium.com/@RyanCarey/new-paper-the-incentives-that-shape-behaviour-d6d8bb77d2e4"},{"x":"11.434518","y":"9.215866","title":"Does Bayes Beat Goodhart?","cluster":"1","author":"Abram Demski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/YJq6R9Wgk5Atjx54D/does-bayes-beat-goodhart"},{"x":"10.606969","y":"6.1298733","title":"What are some non-purely-sampling ways to do deep RL?","cluster":"0","author":"Evan Hubinger","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/Ca3sCRGfWvXvYC5YC/what-are-some-non-purely-sampling-ways-to-do-deep-rl"},{"x":"8.866324","y":"9.77465","title":"Strategic implications of AIs' ability to coordinate at low cost, for example by merging","cluster":"3","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low"},{"x":"7.817769","y":"9.087103","title":"Misconceptions about continuous takeoff","cluster":"3","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff"},{"x":"10.956873","y":"5.9474263","title":"What You See Isn't Always What You Want","cluster":"0","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/AeHtdxHheMjHredaq/what-you-see-isn-t-always-what-you-want"},{"x":"9.166121","y":"9.182391","title":"The strategy-stealing assumption","cluster":"3","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption"},{"x":"9.464305","y":"7.8865223","title":"Open question: are minimal circuits daemon-free?","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free"},{"x":"11.04256","y":"9.433562","title":"Coherent behaviour in the real world is an incoherent concept","cluster":"1","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent"},{"x":"11.157943","y":"9.049038","title":"Partial preferences needed; partial preferences sufficient","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/sEqu6jMgnHG2fvaoQ/partial-preferences-needed-partial-preferences-sufficient"},{"x":"8.779027","y":"9.738698","title":"Three Biases That Made Me Believe in AI Risk","cluster":"3","author":"beth","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://forum.effectivealtruism.org/posts/Yseu9oG3gnb6ERc7n/three-biases-that-made-me-believe-in-ai-risk"},{"x":"8.977841","y":"9.637642","title":"Existential Risk, Creativity & Well-Adapted Science","cluster":"3","author":"Adrian Currie","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.cser.ac.uk/resources/xrisk-creativity/"},{"x":"9.059618","y":"8.3951","title":"Do what we mean vs. do what we say","cluster":"4","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/8Q5h6hyBXTEgC6EZf/do-what-i-mean-vs-do-what-i-say"},{"x":"11.327091","y":"8.765016","title":"VOI is Only Nonnegative When Information is Uncorrelated With Future Action","cluster":"1","author":"Diffractor","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/KER27SxZssfmsusxy/voi-is-only-nonnegative-when-information-is-uncorrelated"},{"x":"8.963088","y":"9.255115","title":"The alignment problem in different capability regimes","cluster":"4","author":"Buck Shlegeris","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/HHunb8FPnhWaDAQci/the-alignment-problem-in-different-capability-regimes"},{"x":"8.131579","y":"8.173604","title":"The theory-practice gap","cluster":"3","author":"Buck Shlegeris","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap"},{"x":"9.271746","y":"8.618711","title":"Human modeling in AGI","cluster":"4","author":"Scott Garrabrant and Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi"},{"x":"10.943006","y":"9.852797","title":"Compact vs. Wide Models","cluster":"1","author":"Vaniver","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/JkCPkMxuftohieb8B/compact-vs-wide-models"},{"x":"11.636942","y":"8.56683","title":"Progress on Causal Influence Diagrams","cluster":"1","author":"Tom Everitt, Ryan Carey, Lewis Hammond, James Fox, Eric Langlois, Shane Legg","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://deepmindsafetyresearch.medium.com/progress-on-causal-influence-diagrams-a7a32180b0d1"},{"x":"8.558787","y":"8.049991","title":"Frequent arguments about alignment","cluster":"4","author":"John Schulman","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment"},{"x":"8.816425","y":"9.785707","title":"Survey on AI existential risk scenarios","cluster":"3","author":"Sam Clarke, Alexis Carlier, Jonas Schuett","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios"},{"x":"9.102822","y":"6.7747216","title":"Mundane solutions to exotic problems","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://ai-alignment.com/mundane-solutions-to-exotic-problems-395bad49fbe7"},{"x":"9.128101","y":"8.557777","title":"My AGI Threat Model: Misaligned Model-Based RL Agent","cluster":"4","author":"Steve Byrnes","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent"},{"x":"9.290703","y":"7.8506017","title":"Distinguishing claims about training vs deployment","cluster":"4","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment"},{"x":"8.773549","y":"9.046559","title":"Putting the humanity into inhuman systems: How human factors and ergonomics can be used to manage the risks associated with artificial general intelligence","cluster":"4","author":"Paul M. Salmon, Tony Carden, Peter A. Hancock","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://onlinelibrary.wiley.com/doi/10.1002/hfm.20883"},{"x":"8.742746","y":"9.518956","title":"Mapping the Conceptual Territory in AI Existential Safety and Alignment","cluster":"3","author":"Jack Koch","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://jbkjr.com/posts/2020/12/mapping_conceptual_territory_AI_safety_alignment/"},{"x":"8.464351","y":"8.828669","title":"When AI Systems Fail: Introducing the AI Incident Database","cluster":"3","author":"Sean McGregor","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.partnershiponai.org/aiincidentdatabase/"},{"x":"8.347436","y":"8.345178","title":"The \"Backchaining to Local Search\" Technique in AI Alignment","cluster":"4","author":"Adam Shimi","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment"},{"x":"8.371992","y":"8.502152","title":"The Problem with Metrics is a Fundamental Problem for AI","cluster":"3","author":"Rachel Thomas, David Uminsky","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"9.902466","y":"7.8348083","title":"Search versus design","cluster":"4","author":"Alex Flint","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"8.478666","y":"5.596758","title":"To Trust Or Not To Trust A Classifier","cluster":"2","author":"Heinrich Jiang, Been Kim, Melody Y. Guan, Maya Gupta","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1805.11783"},{"x":"7.959457","y":"7.898279","title":"A space of proposals for building safe advanced AI","cluster":"4","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/S9GxuAEeQomnLkeNt/a-space-of-proposals-for-building-safe-advanced-ai"},{"x":"7.881691","y":"5.3200474","title":"From ImageNet to Image Classification","cluster":"2","author":"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, Aleksander Madry","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://gradientscience.org/benchmarks/"},{"x":"7.83435","y":"9.491326","title":"A Guide to Writing the NeurIPS Impact Statement","cluster":"3","author":"Carolyn Ashurst, Markus Anderljung, Carina Prunkl, Jan Leike, Yarin Gal, Toby Shevlane, Allan Dafoe","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://medium.com/@operations_18894/a-guide-to-writing-the-neurips-impact-statement-4293b723f832"},{"x":"8.32201","y":"8.427075","title":"Disambiguating \"alignment\" and related notions","cluster":"4","author":"capybaralet","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/FTpPC4umEiREZMMRu/disambiguating-alignment-and-related-notions"},{"x":"8.937224","y":"8.012742","title":"On Strong Artificial Intelligence","cluster":"4","author":"Zhou Zhihua, translated by Jeffrey Ding","source":"alignment newsletter","tags":"[]","date":"nan","url":"https://docs.google.com/document/d/1RP_bWfC1waWQaLwunQBN_R0yRNlDjVOOE4rhmqm8JSA/edit"},{"x":"8.950635","y":"9.569274","title":"Six AI Risk/Strategy Ideas","cluster":"3","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/dt4z82hpvvPFTDTfZ/six-ai-risk-strategy-ideas"},{"x":"9.38881","y":"8.395763","title":"Self-Fulfilling Prophecies Aren't Always About Self-Awareness","cluster":"4","author":"John Maxwell","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/yArZKCEheZt8GkK6p/self-fulfilling-prophecies-aren-t-always-about-self"},{"x":"12.656897","y":"9.013835","title":"[Analysing: Dangerous messages from future UFAI via Oracles](https://www.alignmentforum.org/posts/6WbLRLdmTL4JxxvCq/analysing-dangerous-messages-from-future-ufai-via-oracles) and [Breaking Oracles: hyperrationality and acausal trade](https://www.alignmentforum.org/posts/42z4k8Co5BuHMBvER/breaking-oracles-hyperrationality-and-acausal-trade)","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":""},{"x":"8.336164","y":"7.817385","title":"Distance Functions are Hard","cluster":"4","author":"Grue_Slinky","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://alignmentforum.org/posts/YuJNoCEgeWJfBtdtQ/distance-functions-are-hard-1"},{"x":"12.833762","y":"9.1573715","title":"Self-confirming prophecies, and simplified Oracle designs","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/wJ3AqNPM7W4nfY5Bk/self-confirming-prophecies-and-simplified-oracle-designs"},{"x":"11.469724","y":"6.532196","title":"Reinforcement learning with imperceptible rewards","cluster":"0","author":"Vanessa Kosoy","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards"},{"x":"11.089659","y":"9.300117","title":"Assuming we've solved X, could we do Y...","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/95i5B78uhqyB3d6Xc/assuming-we-ve-solved-x-could-we-do-y"},{"x":"9.669192","y":"9.5207615","title":"AI Alignment Podcast: Moral Uncertainty and the Path to AI Alignment","cluster":"4","author":"Lucas Perry and William MacAskill","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://futureoflife.org/2018/09/17/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/"},{"x":"8.659349","y":"8.5696745","title":"Key Concepts in AI Safety","cluster":"4","author":"Tim G. J. Rudner, Helen Toner","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://cset.georgetown.edu/research/key-concepts-in-ai-safety-an-overview/"},{"x":"10.590379","y":"8.673213","title":"Locality of goals","cluster":"1","author":"Adam Shimi","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/HkWB5KCJQ2aLsMzjt/locality-of-goals"},{"x":"8.357514","y":"9.526088","title":"Preparing for \"The Talk\" with AI projects","cluster":"3","author":"Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/QSBgGv8byWMjmaGE5/preparing-for-the-talk-with-ai-projects"},{"x":"10.464588","y":"9.663524","title":"Formal Metaethics and Metasemantics for AI Alignment","cluster":"1","author":"June Ku","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://www.metaethical.ai/v20-1/"},{"x":"8.931201","y":"8.978718","title":"A Psychopathological Approach to Safety Engineering in AI and AGI","cluster":"4","author":"Vahid Behzadan, Arslan Munir, Roman V. Yampolskiy","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1805.08915"},{"x":"9.0987625","y":"9.040813","title":"[The Unavoidable Problem of Self-Improvement in AI](https://futureoflife.org/2019/03/19/the-unavoidable-problem-of-self-improvement-in-ai-an-interview-with-ramana-kumar-part-1/) and [The Problem of Self-Referential Reasoning in Self-Improving AI](https://futureoflife.org/2019/03/21/the-problem-of-self-referential-reasoning-in-self-improving-ai-an-interview-with-ramana-kumar-part-2/)","cluster":"4","author":"Jolene Creighton and Ramana Kumar","source":"alignment newsletter","tags":"[]","date":"2019.0","url":""},{"x":"11.760201","y":"8.902695","title":"Understanding Agent Incentives with Causal Influence Diagrams","cluster":"1","author":"Tom Everitt","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://medium.com/@deepmindsafetyresearch/understanding-agent-incentives-with-causal-influence-diagrams-7262c2512486"},{"x":"9.484557","y":"8.504085","title":"Safely and usefully spectating on AIs optimizing over toy worlds","cluster":"4","author":"Alex Mennen","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/ikN9qQEkrFuPtYd6Y/safely-and-usefully-spectating-on-ais-optimizing-over-toy"},{"x":"8.722913","y":"9.049394","title":"Exploring AI Safety in Degrees: Generality, Capability and Control","cluster":"3","author":"John Burden, José Hernández-Orallo","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.cser.ac.uk/resources/exploring-ai-safety-degrees-generality-capability-and-control/"},{"x":"10.761826","y":"9.144004","title":"Three ways that \"Sufficiently optimized agents appear coherent\" can be false","cluster":"1","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/4K52SS7fm9mp5rMdX/three-ways-that-sufficiently-optimized-agents-appear"},{"x":"9.826349","y":"9.473321","title":"AI Alignment Podcast: On Becoming a Moral Realist","cluster":"3","author":"Lucas Perry and Peter Singer","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://futureoflife.org/2018/10/18/on-becoming-a-moral-realist-peter-singer/"},{"x":"10.14623","y":"5.806792","title":"Generally capable agents emerge from open-ended play","cluster":"0","author":"Open-Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard, Wojciech Marian Czarnecki","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play"},{"x":"10.207424","y":"6.766858","title":"Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design","cluster":"0","author":"Michael Dennis*, Natasha Jaques*, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2012.02096"},{"x":"8.024837","y":"8.671492","title":"Introducing the AI Alignment Forum (FAQ)","cluster":"3","author":"habryka","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq"},{"x":"7.770194","y":"8.443836","title":"AI Safety Papers","cluster":"3","author":"Ozzie Gooen","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://ai-safety-papers.quantifieduncertainty.org/"},{"x":"8.15037","y":"8.972613","title":"Announcement: AI alignment prize round 3 winners and next round","cluster":"3","author":"Zvi Mowshowitz and Vladimir Slepnev","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/juBRTuE3TLti5yB35/announcement-ai-alignment-prize-round-3-winners-and-next"},{"x":"8.044743","y":"9.591612","title":"Q&A with Jason Matheny, Founding Director of CSET","cluster":"3","author":"Jason Matheny","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.georgetown.edu/news/q-and-a-with-cset-founding-director-jason-matheny"},{"x":"7.758609","y":"9.252658","title":"Early-career funding for individuals interested in improving the long-term future","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future?fbclid=IwAR3bA_4piJVHwSREGaH6g0O3CReNw3SlLNpd7jMAQTygSeMrkwyRfoPRbcA"},{"x":"8.1326065","y":"9.3375845","title":"Internships and fellowships for 2019","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"7.8703604","y":"8.227784","title":"DeepMind job: Science Writer","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://deepmind.com/careers/1294000/"},{"x":"8.46874","y":"9.475157","title":"NIST AI Risk Management Framework","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.nist.gov/itl/ai-risk-management-framework"},{"x":"8.414493","y":"9.425196","title":"Introducing the AI Objectives Institute","cluster":"3","author":"Peter Eckersley","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://ai.objectives.institute/blog/ai-and-the-transformation-of-capitalism"},{"x":"10.422946","y":"8.802768","title":"Ought's Progress Update July 2018","cluster":"1","author":"Andreas Stuhlmüller","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://ought.org/blog/2018-07-16-progress-update"},{"x":"10.800013","y":"6.555571","title":"Political Economy of Reinforcement Learning (PERLS) Workshop","cluster":"0","author":"Stuart Russell, Thomas Gilbert, Tom Zick, Aaron Snoswell, Michael Dennis","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://perls-workshop.github.io/"},{"x":"8.038802","y":"9.671539","title":"FLI Job Postings","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://futureoflife.org/job-postings/"},{"x":"7.987022","y":"9.334264","title":"Research Scholars Programme","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.fhi.ox.ac.uk/rsp/"},{"x":"9.148054","y":"9.444946","title":"New Seminar Series and Call For Proposals On Cooperative AI","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2022.0","url":""},{"x":"8.954425","y":"8.208468","title":"Announcing the PIBBSS Summer Research Fellowship","cluster":"4","author":"Nora Ammann","source":"alignment newsletter","tags":"[]","date":"2021.0","url":""},{"x":"8.048605","y":"9.507145","title":"Survey: classifying AI systems used in response to the COVID-19 pandemic","cluster":"3","author":"Samuel Curtis, Adriana Bora, Nicolas Miailhe, Rui Wang","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.oecd.ai/wonk/pandemic"},{"x":"11.7212925","y":"10.478005","title":"Formal Methods for the Informal Engineer","cluster":"1","author":"Gopal Sarma, Jimmy Koppel, Ramana Kumar, Eric Drexler, Patrick Schultz, Gregory Malecha","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://fmie2021.github.io/"},{"x":"8.451675","y":"8.91589","title":"RAISE [releases prereq modules](https://www.lesswrong.com/posts/vi48CMtZL8ZkRpuad/soon-a-weekly-ai-safety-prerequisites-module-on-lesswrong) and [is looking for high level feedback](https://www.lesswrong.com/posts/z5SJBmTpqG6rexTgH/looking-for-ai-safety-experts-to-provide-high-level-guidance)","cluster":"3","author":"nan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"8.680392","y":"9.474822","title":"[Three AI Safety Related Ideas](https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas) and [Two Neglected Problems in Human-AI Safety](https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety)","cluster":"3","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"10.291697","y":"9.615726","title":"Some Thoughts on Metaphilosophy","cluster":"1","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy"},{"x":"11.139746","y":"8.67361","title":"Reframing Impact - Part 2","cluster":"1","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW"},{"x":"11.011448","y":"7.698975","title":"Reframing Impact - Part 3","cluster":"4","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW"},{"x":"11.173359","y":"7.9177246","title":"Subagents and impact measures, full and fully illustrated","cluster":"4","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated"},{"x":"10.978284","y":"6.787438","title":"Introducing SafeLife: Safety Benchmarks for Reinforcement Learning","cluster":"0","author":"Carroll Wainwright, Peter Eckersley","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.partnershiponai.org/safelife/"},{"x":"11.111335","y":"6.1753645","title":"Safety Gym","cluster":"0","author":"Alex Ray*, Joshua Achiam*, Dario Amodei","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/safety-gym/"},{"x":"11.396618","y":"7.0056233","title":"Designing agent incentives to avoid reward tampering","cluster":"0","author":"Tom Everitt, Ramana Kumar, and Marcus Hutter","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd"},{"x":"11.321855","y":"8.659164","title":"Reframing Impact - Part 1","cluster":"1","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW"},{"x":"10.095868","y":"8.032821","title":"Asymptotically Benign AGI","cluster":"4","author":"Michael Cohen","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/pZhDWxDmwzuSwLjou/asymptotically-benign-agi"},{"x":"11.044227","y":"8.259322","title":"Towards a New Impact Measure","cluster":"1","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure"},{"x":"11.202652","y":"8.510594","title":"Impact Measure Desiderata","cluster":"1","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/c2oM7qytRByv6ZFtz/impact-measure-desiderata"},{"x":"10.845161","y":"6.914208","title":"Avoiding Tampering Incentives in Deep RL via Decoupled Approval","cluster":"0","author":"Ramana Kumar*, Jonathan Uesato*, Victoria Krakovna, Tom Everitt, Richard Ngo, Shane Legg","source":"alignment newsletter","tags":"[]","date":"nan","url":"https://arxiv.org/abs/2011.08827"},{"x":"10.037417","y":"7.7999434","title":"Pessimism About Unknown Unknowns Inspires Conservatism","cluster":"4","author":"Michael Cohen, Marcus Hutter","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism"},{"x":"8.408161","y":"7.2486815","title":"Robust Change Captioning","cluster":"2","author":"Dong Huk Park, Trevor Darrell, Anna Rohrbach","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1901.02527"},{"x":"10.941319","y":"6.7443833","title":"Avoiding Side Effects in Complex Environments","cluster":"0","author":"Alex Turner, Neale Ratzlaff, Prasad Tadepalli","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/5kurn5W62C5CpSWq6/avoiding-side-effects-in-complex-environments"},{"x":"11.16597","y":"8.422841","title":"Tradeoffs between desirable properties for baseline choices in impact measures","cluster":"3","author":"Victoria Krakovna","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/nLhfRpDutEdgr6PKe/tradeoff-between-desirable-properties-for-baseline-choices"},{"x":"10.90223","y":"7.3171563","title":"Curiosity Killed the Cat and the Asymptotically Optimal Agent","cluster":"0","author":"Michael Cohen, Marcus Hutter","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/fSC98Cy3zR9GsEPnT/curiosity-killed-the-cat-and-the-asymptotically-optimal"},{"x":"11.231967","y":"7.9444847","title":"Attainable utility has a subagent problem","cluster":"4","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/sYjCeZTwA84pHkhBJ/attainable-utility-has-a-subagent-problem"},{"x":"10.3365135","y":"7.4804077","title":"Vehicle Automation Report","cluster":"4","author":"NTSB","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://assets.documentcloud.org/documents/6540547/629713.pdf"},{"x":"11.053304","y":"7.7912664","title":"Reversible changes: consider a bucket of water","cluster":"0","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/zrunBA8B5bmm2XZ59/reversible-changes-consider-a-bucket-of-water"},{"x":"11.312591","y":"6.63707","title":"Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning","cluster":"0","author":"Jaime F. Fisac*, Neil F. Lugovoy*, Vicenc Rubies-Royo, Shromona Ghosh, Claire J. Tomlin","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://files.davidqiu.com/research/papers/2019_fisac_Bridging%20Hamilton-Jacobi%20Safety%20Analysis%20and%20Reinforcement%20Learning%20%5BRL%5D%5BConstraints%5D.pdf"},{"x":"12.768576","y":"8.995092","title":"[Self-confirming predictions can be arbitrarily bad](https://www.alignmentforum.org/posts/KoEY9CjrKe93ErYhd/self-confirming-predictions-can-be-arbitrarily-bad) and [Oracles, sequence predictors, and self-confirming predictions](https://www.alignmentforum.org/posts/i2dNFgbjnqZBfeitT/oracles-sequence-predictors-and-self-confirming-predictions)","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":""},{"x":"11.019408","y":"8.001436","title":"AXRP Episode 7 - Side Effects","cluster":"4","author":"Daniel Filan and Victoria Krakovna","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/C9vj5ZX3KsgFfwXAN/axrp-episode-7-side-effects-with-victoria-krakovna"},{"x":"9.477691","y":"9.229907","title":"Overcoming Clinginess in Impact Measures","cluster":"3","author":"TurnTrout","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/DvmhXysefEyEvXuXS/overcoming-clinginess-in-impact-measures"},{"x":"11.388651","y":"7.4287195","title":"Avoiding Side Effects By Considering Future Tasks","cluster":"0","author":"Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, Shane Legg","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2010.07877"},{"x":"9.939265","y":"8.461044","title":"Worrying about the Vase: Whitelisting","cluster":"4","author":"TurnTrout","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/H7KB44oKoSjSCkpzL/worrying-about-the-vase-whitelisting"},{"x":"11.424058","y":"8.280246","title":"Dynamic inconsistency of the inaction and initial state baseline","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/w8QBmgQwb83vDMXoz/dynamic-inconsistency-of-the-inaction-and-initial-state"},{"x":"11.649685","y":"7.212386","title":"When Goodharting is optimal: linear vs diminishing returns, unlikely vs likely, and other factors","cluster":"0","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/megKzKKsoecdYqwb7/when-goodharting-is-optimal-linear-vs-diminishing-returns"},{"x":"12.280669","y":"8.879164","title":"Formal Language Constraints for Markov Decision Processes","cluster":"1","author":"Eleanor Quint, Dong Xu, Haluk Dogan, Zeynep Hakguder, Stephen Scott, Matthew Dwyer","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1910.01074"},{"x":"10.677558","y":"8.5593","title":"Wireheading as a potential problem with the new impact measure","cluster":"4","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/6EMdmeosYPdn74wuG/wireheading-as-a-potential-problem-with-the-new-impact"},{"x":"9.767899","y":"9.604958","title":"Pascal’s Muggle Pays","cluster":"3","author":"Zvi","source":"alignment newsletter","tags":"[]","date":"2017.0","url":"https://www.lesswrong.com/posts/CaPgNwxEFHh3Ahvf7/pascal-s-muggle-pays"},{"x":"7.8338118","y":"6.508819","title":"Does GPT-2 Know Your Phone Number?","cluster":"2","author":"Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://bair.berkeley.edu/blog/2020/12/20/lmmem/"},{"x":"8.489049","y":"5.3744597","title":"Privacy and machine learning: two unexpected allies?","cluster":"2","author":"Nicolas Papernot and Ian Goodfellow","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html"},{"x":"8.907141","y":"8.691213","title":"Alignment as Translation","cluster":"4","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation"},{"x":"9.306547","y":"9.716607","title":"What can the principal-agent literature tell us about AI risk?","cluster":"3","author":"Alexis Carlier and Tom Davidson","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai"},{"x":"8.8656225","y":"8.517497","title":"Conversation with Paul Christiano","cluster":"4","author":"Paul Christiano, Asya Bergal, Ronny Fernandez, and Robert Long","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://aiimpacts.org/conversation-with-paul-christiano/"},{"x":"9.387454","y":"8.34205","title":"Conversation with Rohin Shah","cluster":"4","author":"Rohin Shah, Asya Bergal, Robert Long, and Sara Haxhia","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://aiimpacts.org/conversation-with-rohin-shah/"},{"x":"9.426275","y":"8.835259","title":"Conversation with Robin Hanson","cluster":"4","author":"Robin Hanson, Asya Bergal, and Robert Long","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://aiimpacts.org/conversation-with-robin-hanson/"},{"x":"8.75538","y":"8.689373","title":"Conversation with Adam Gleave","cluster":"4","author":"Adam Gleave, Asya Bergal, and Robert Long","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://aiimpacts.org/conversation-with-adam-gleave/"},{"x":"11.677597","y":"8.280029","title":"Seeking Power is Provably Instrumentally Convergent in MDPs","cluster":"1","author":"Alex Turner, Logan Smith","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/6DuJxY8X45Sco4bS2/seeking-power-is-provably-instrumentally-convergent-in-mdps"},{"x":"8.5723505","y":"9.67374","title":"A shift in arguments for AI risk","cluster":"3","author":"Tom Sittler","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/hubbRt4DPegiA5gRR/a-shift-in-arguments-for-ai-risk"},{"x":"8.431955","y":"9.479217","title":"What failure looks like","cluster":"3","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like"},{"x":"9.116208","y":"9.290066","title":"Disentangling arguments for the importance of AI safety","cluster":"4","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety"},{"x":"9.132817","y":"8.014058","title":"Alignment difficulty","cluster":"4","author":"Richard Ngo and Eliezer Yudkowsky","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty"},{"x":"8.81422","y":"9.77001","title":"Draft report on existential risk from power-seeking AI","cluster":"3","author":"Joe Carlsmith","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai"},{"x":"8.59229","y":"9.862691","title":"Comments on Carlsmith's “Is power-seeking AI an existential risk?”","cluster":"3","author":"Nate Soares","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential"},{"x":"9.11552","y":"8.702686","title":"What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)","cluster":"4","author":"Andrew Critch, Tom Gilbert","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"},{"x":"9.522556","y":"8.087524","title":"Another (outer) alignment failure story","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story"},{"x":"9.11406","y":"9.33866","title":"Consequences of Misaligned AI","cluster":"3","author":"Simon Zhuang, Dylan Hadfield-Menell","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://proceedings.neurips.cc/paper/2020/file/b607ba543ad05417b8507ee86c54fcb7-Paper.pdf"},{"x":"9.247467","y":"6.843044","title":"[Better priors as a safety problem](https://www.alignmentforum.org/posts/roA83jDvq7F2epnHK/better-priors-as-a-safety-problem) and [Learning the prior](https://www.alignmentforum.org/posts/SL9mKhgdmDKXmxwE4/learning-the-prior)","cluster":"2","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2020.0","url":""},{"x":"9.607694","y":"8.139897","title":"Inaccessible information","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://alignmentforum.org/posts/ZyWyAJbedvEgRT2uF/inaccessible-information"},{"x":"10.641479","y":"7.5356913","title":"Multiparty Dynamics and Failure Modes for Machine Learning and Artificial Intelligence","cluster":"4","author":"David Manheim","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.mdpi.com/2504-2289/3/2/21/htm"},{"x":"10.8710165","y":"6.3026743","title":"The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models","cluster":"0","author":"Anonymous","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://openreview.net/forum?id=JYtwGwIL7ye"},{"x":"8.520559","y":"7.2283525","title":"Why AI alignment could be hard with modern deep learning","cluster":"2","author":"Ajeya Cotra","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/"},{"x":"10.587434","y":"7.340441","title":"Low-stakes alignment","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://ai-alignment.com/low-stakes-alignment-f3c36606937f"},{"x":"8.326541","y":"9.746808","title":"Clarifying “What failure looks like” (part 1)","cluster":"3","author":"Sam Clarke","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like-part-1"},{"x":"9.109748","y":"8.886332","title":"Non-Adversarial Goodhart and AI Risks","cluster":"3","author":"David Manheim","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/iK2F9QDZvwWinsBYB/non-adversarial-goodhart-and-ai-risks"},{"x":"10.701584","y":"7.996083","title":"Classifying specification problems as variants of Goodhart's Law","cluster":"1","author":"Victoria Krakovna, Ramana Kumar","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/yXPT4nr4as7JvxLQa/classifying-specification-problems-as-variants-of-goodhart-s"},{"x":"9.902038","y":"8.526538","title":"Defining AI wireheading","cluster":"4","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading"},{"x":"11.908633","y":"8.306001","title":"The \"Commitment Races\" problem","cluster":"1","author":"Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem"},{"x":"10.51726","y":"9.096285","title":"Agency Failure AI Apocalypse?","cluster":"1","author":"Robin Hanson","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html"},{"x":"11.716949","y":"8.001353","title":"Environmental Structure Can Cause Instrumental Convergence","cluster":"1","author":"Alex Turner","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/b6jJddSvWMdZHJHh3/environmental-structure-can-cause-instrumental-convergence"},{"x":"7.880018","y":"7.161338","title":"\"Unsupervised\" translation as an (intent) alignment problem","cluster":"2","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/saRRRdMnMPXXtQBNi/unsupervised-translation-as-a-safety-problem"},{"x":"9.991593","y":"7.420041","title":"Model splintering: moving from one imperfect model to another","cluster":"4","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1"},{"x":"9.27546","y":"6.9011974","title":"An Architectural Risk Analysis of Machine Learning Systems: Towards More Secure Machine Learning","cluster":"0","author":"Gary McGraw, Harold Figueroa, Victor Shepardson, Richie Bonett","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.garymcgraw.com/wp-content/uploads/2020/02/BIML-ARA.pdf"},{"x":"8.637997","y":"7.387566","title":"Steven Pinker and Stuart Russell on the Foundations, Benefits, and Possible Existential Threat of AI","cluster":"4","author":"Lucas Perry, Steven Pinker and Stuart Russell","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://futureoflife.org/2020/06/15/steven-pinker-and-stuart-russell-on-the-foundations-benefits-and-possible-existential-risk-of-ai/"},{"x":"10.207904","y":"7.682417","title":"Specification gaming: the flip side of AI ingenuity","cluster":"4","author":"Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, Shane Legg","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity"},{"x":"10.954108","y":"9.276396","title":"Comment on Coherence arguments do not imply goal directed behavior","cluster":"1","author":"Ronny Fernandez","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/posts/EnN7cm3KaRrEAuWfa/comment-on-coherence-arguments-do-not-imply-goal-directed"},{"x":"8.59071","y":"8.019487","title":"An Increasingly Manipulative Newsfeed","cluster":"2","author":"Michaël Trazzi and Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/EpdXLNXyL4EYLFwF8/an-increasingly-manipulative-newsfeed"},{"x":"9.030161","y":"9.6744995","title":"AI Risk for Epistemic Minimalists","cluster":"3","author":"Alex Flint","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/8fpzBHt7e6n7Qjoo9/ai-risk-for-epistemic-minimalists"},{"x":"9.01514","y":"9.395931","title":"Challenges of Aligning Artificial Intelligence with Human Values","cluster":"4","author":"Margit Sutrop","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.ies.ee/bahps/acta-baltica/abhps-8-2/04_Sutrop-2020-2-04.pdf"},{"x":"11.066024","y":"7.9664416","title":"Aligning AI to Human Values means Picking the Right Metrics","cluster":"2","author":"Jonathan Stray","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://medium.com/@PartnershipAI/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047"},{"x":"11.078946","y":"8.887562","title":"How Much Do Recommender Systems Drive Polarization?","cluster":"3","author":"Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://jsteinhardt.stat.berkeley.edu/blog/recsys-deepdive"},{"x":"11.230952","y":"8.29126","title":"Beyond Engagement: Aligning Algorithmic Recommendations With Prosocial Goals","cluster":"1","author":"Jonathan Stray","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.partnershiponai.org/beyond-engagement-aligning-algorithmic-recommendations-with-prosocial-goals/"},{"x":"11.22212","y":"8.090517","title":"What are you optimizing for? Aligning Recommender Systems with Human Values","cluster":"1","author":"Jonathan Stray, Steven Adler, Dylan Hadfield-Menell","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://participatoryml.github.io/papers/2020/42.pdf"},{"x":"11.313513","y":"5.408235","title":"Learning Latent Plans from Play","cluster":"0","author":"Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://learning-from-play.github.io/"},{"x":"10.006865","y":"5.841483","title":"AlphaStar: Mastering the Real-Time Strategy Game StarCraft II","cluster":"0","author":"The AlphaStar team","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/"},{"x":"9.939924","y":"6.0539727","title":"Spinning Up in Deep RL","cluster":"0","author":"Joshua Achiam","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/spinning-up-in-deep-rl/"},{"x":"11.503835","y":"5.945268","title":"Model-Based Reinforcement Learning via Meta-Policy Optimization","cluster":"0","author":"Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, Pieter Abbeel","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1809.05214"},{"x":"10.169826","y":"5.7607546","title":"Preserving Outputs Precisely while Adaptively Rescaling Targets","cluster":"0","author":"Matteo Hessel and Hado van Hasselt","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://deepmind.com/blog/preserving-outputs-precisely-while-adaptively-rescaling-targets/"},{"x":"10.777572","y":"5.687797","title":"Recurrent World Models Facilitate Policy Evolution","cluster":"0","author":"David Ha, Jürgen Schmidhuber","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1809.01999"},{"x":"10.109569","y":"5.9016557","title":"Large-Scale Study of Curiosity-Driven Learning","cluster":"0","author":"Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell and Alexei A. Efros","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://pathak22.github.io/large-scale-curiosity/"},{"x":"9.842512","y":"6.014274","title":"OpenAI Five Benchmark: Results","cluster":"2","author":"OpenAI's Dota Team","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/openai-five-benchmark-results/"},{"x":"11.178039","y":"6.2925806","title":"Variational Option Discovery Algorithms","cluster":"0","author":"Joshua Achiam, Harrison Edwards, Dario Amodei, Pieter Abbeel","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1807.10299"},{"x":"11.453377","y":"5.375841","title":"Learning Dexterity","cluster":"0","author":"Many people at OpenAI","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/learning-dexterity/"},{"x":"10.071023","y":"5.9646425","title":"Capture the Flag: the emergence of complex cooperative agents","cluster":"0","author":"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Harris, and Thore Graepel","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://deepmind.com/blog/capture-the-flag/"},{"x":"9.973917","y":"5.902045","title":"OpenAI Five","cluster":"0","author":"Many people at OpenAI","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/openai-five/"},{"x":"10.07907","y":"5.809106","title":"Agent57: Outperforming the human Atari benchmark","cluster":"2","author":"Adrià Puigdomènech Badia*, Bilal Piot*, Steven Kapturowski*, Pablo Sprechmann*, Alex Vitvitskyi, Daniel Guo, Charles Blundell","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark"},{"x":"10.183547","y":"5.792246","title":"On Catastrophic Interference in Atari 2600 Games","cluster":"0","author":"['William Fedus', 'Dibya Ghosh', 'John D. Martin', 'Marc G. Bellemare', 'Yoshua Bengio', 'Hugo Larochelle']","source":"alignment newsletter","tags":"[]","date":"2020-02-28 00:55:03+00:00","url":"http://arxiv.org/abs/2002.12499v2"},{"x":"11.33734","y":"6.5475507","title":"Reward-Conditioned Policies","cluster":"0","author":"['Aviral Kumar', 'Xue Bin Peng', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2019-12-31 18:07:43+00:00","url":"http://arxiv.org/abs/1912.13465v1"},{"x":"10.3845","y":"5.826924","title":"Procgen Benchmark","cluster":"0","author":"Karl Cobbe, Christopher Hesse, Jacob Hilton, John Schulman","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/procgen-benchmark/"},{"x":"10.738499","y":"5.908919","title":"Learning to Predict Without Looking Ahead: World Models Without Forward Prediction","cluster":"0","author":"C. Daniel Freeman, Luke Metz, David Ha","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://learningtopredict.github.io/"},{"x":"9.934669","y":"5.9793644","title":"Superhuman AI for multiplayer poker","cluster":"4","author":"Noam Brown, Tuomas Sandholm","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://science.sciencemag.org/content/early/2019/07/10/science.aay2400.full"},{"x":"10.126292","y":"5.9512687","title":"AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning","cluster":"0","author":"AlphaStar Team","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning"},{"x":"11.479265","y":"5.3848257","title":"Deep Dynamics Models for Dexterous Manipulation","cluster":"0","author":"Anusha Nagabandi, Kurt Konoglie, Sergey Levine, Vikash Kumar","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/"},{"x":"11.407672","y":"5.470286","title":"Let's Discuss OpenAI's Rubik's Cube Result","cluster":"0","author":"Alex Irpan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alexirpan.com/2019/10/29/openai-rubiks.html"},{"x":"11.4183445","y":"5.46981","title":"Solving Rubik’s Cube with a Robot Hand","cluster":"0","author":"OpenAI","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/solving-rubiks-cube/"},{"x":"10.055929","y":"7.3070307","title":"Emergent Tool Use from Multi-Agent Interaction","cluster":"4","author":"Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, Igor Mordatch","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/emergent-tool-use/"},{"x":"10.543709","y":"5.9656453","title":"A Survey of Reinforcement Learning Informed by Natural Language","cluster":"0","author":"Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, Tim Rocktäschel","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1906.03926"},{"x":"10.908948","y":"6.2805395","title":"The MineRL 2019 Competition on Sample Efficient Reinforcement Learning using Human Priors","cluster":"0","author":"['William H. Guss', 'Cayden Codel', 'Katja Hofmann', 'Brandon Houghton', 'Noboru Kuno', 'Stephanie Milani', 'Sharada Mohanty', 'Diego Perez Liebana', 'Ruslan Salakhutdinov', 'Nicholay Topin', 'Manuela Veloso', 'Phillip Wang']","source":"alignment newsletter","tags":"[]","date":"2019-04-22 22:18:37+00:00","url":"http://arxiv.org/abs/1904.10079v3"},{"x":"10.103374","y":"6.0723147","title":"How to Train Your OpenAI Five","cluster":"0","author":"OpenAI","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://openai.com/blog/how-to-train-your-openai-five/"},{"x":"11.135626","y":"6.3887544","title":"Off-Policy Deep Reinforcement Learning without Exploration","cluster":"0","author":"Scott Fujimoto, David Meger, Doina Precup","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1812.02900"},{"x":"8.991215","y":"7.0207644","title":"Open Sourcing Active Question Reformulation with Reinforcement Learning","cluster":"0","author":"Michelle Chen Huebscher and Rodrigo Nogueira","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://ai.googleblog.com/2018/10/open-sourcing-active-question.html"},{"x":"11.27039","y":"6.2720556","title":"Near-Optimal Representation Learning for Hierarchical Reinforcement Learning ","cluster":"0","author":"Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1810.01257"},{"x":"9.843492","y":"6.3147225","title":"The Animal-AI Testbed and Competition","cluster":"4","author":"Matthew Crosby, Benjamin Beyret, Murray Shanahan, José Hernández-Orallo, Lucy Cheke, Marta Halina","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://proceedings.mlr.press/v123/crosby20a/crosby20a.pdf"},{"x":"10.718721","y":"6.2454734","title":"Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey","cluster":"0","author":"['Sanmit Narvekar', 'Bei Peng', 'Matteo Leonetti', 'Jivko Sinapov', 'Matthew E. Taylor', 'Peter Stone']","source":"alignment newsletter","tags":"[]","date":"2020-03-10 20:41:24+00:00","url":"http://arxiv.org/abs/2003.04960v2"},{"x":"10.285706","y":"6.252918","title":"Suphx: Mastering Mahjong with Deep Reinforcement Learning","cluster":"0","author":"['Junjie Li', 'Sotetsu Koyamada', 'Qiwei Ye', 'Guoqing Liu', 'Chao Wang', 'Ruihan Yang', 'Li Zhao', 'Tao Qin', 'Tie-Yan Liu', 'Hsiao-Wuen Hon']","source":"alignment newsletter","tags":"[]","date":"2020-03-30 16:18:16+00:00","url":"http://arxiv.org/abs/2003.13590v2"},{"x":"10.083612","y":"5.941863","title":"Mastering Complex Control in MOBA Games with Deep Reinforcement Learning","cluster":"0","author":"Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, Qiaobo Chen, Yinyuting Yin, Hao Zhang, Tengfei Shi, Liang Wang, Qiang Fu, Wei Yang, Lanxiao Huang","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1912.09729"},{"x":"10.576533","y":"6.9674473","title":"\"Other-Play\" for Zero-Shot Coordination","cluster":"4","author":"['Hengyuan Hu', 'Adam Lerer', 'Alex Peysakhovich', 'Jakob Foerster']","source":"alignment newsletter","tags":"[]","date":"2020-03-06 00:39:37+00:00","url":"http://arxiv.org/abs/2003.02979v3"},{"x":"10.297627","y":"6.9467235","title":"Building AI that can master complex cooperative games with hidden information","cluster":"4","author":"Adam Lerer, Hengyuan Hu, Jakob Foerster, Noam Brown","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai.facebook.com/blog/building-ai-that-can-master-complex-cooperative-games-with-hidden-information/"},{"x":"11.3845625","y":"5.6177135","title":"The Ingredients of Real World Robotic Reinforcement Learning","cluster":"0","author":"Henry Zhu*, Justin Yu*, Abhishek Gupta*, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://bair.berkeley.edu/blog/2020/04/27/ingredients/"},{"x":"9.611773","y":"6.046611","title":"Massively Scaling Reinforcement Learning with SEED RL","cluster":"0","author":"Lasse Espeholt*, Raphaël Marinier*, Piotr Stanczyk*, Ke Wang, Marcin Michalski","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html"},{"x":"11.40492","y":"5.439061","title":"Robots Learning to Move like Animals","cluster":"0","author":"Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://bair.berkeley.edu/blog/2020/04/03/laikago/"},{"x":"10.408156","y":"5.7814527","title":"Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation","cluster":"0","author":"Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, Sebastian Risi","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1806.10729"},{"x":"11.067548","y":"6.1570616","title":"[Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions](http://arxiv.org/abs/1912.02875) and [Training Agents using Upside-Down Reinforcement Learning](http://arxiv.org/abs/1912.02877)","cluster":"0","author":"Juergen Schmidhuber","source":"alignment newsletter","tags":"[]","date":"2019.0","url":""},{"x":"11.141133","y":"5.727187","title":"Planning with Goal-Conditioned Policies","cluster":"0","author":"Soroush Nasiriany*, Vitchyr H. Pong*, Steven Lin, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1911.08453"},{"x":"11.280813","y":"6.895417","title":"Dream to Control: Learning Behaviors by Latent Imagination","cluster":"0","author":"['Danijar Hafner', 'Timothy Lillicrap', 'Jimmy Ba', 'Mohammad Norouzi']","source":"alignment newsletter","tags":"[]","date":"2019-12-03 18:57:16+00:00","url":"http://arxiv.org/abs/1912.01603v3"},{"x":"10.962178","y":"6.0872607","title":"Adaptive Online Planning for Continual Lifelong Learning","cluster":"0","author":"Kevin Lu, Igor Mordatch, Pieter Abbeel","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1912.01188"},{"x":"11.355136","y":"6.2848425","title":"Model-Based Reinforcement Learning: Theory and Practice","cluster":"0","author":"Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://bair.berkeley.edu/blog/2019/12/12/mbpo/"},{"x":"10.5052805","y":"6.1318545","title":"Stabilizing Transformers for Reinforcement Learning","cluster":"0","author":"['Emilio Parisotto', 'H. Francis Song', 'Jack W. Rae', 'Razvan Pascanu', 'Caglar Gulcehre', 'Siddhant M. Jayakumar', 'Max Jaderberg', 'Raphael Lopez Kaufman', 'Aidan Clark', 'Seb Noury', 'Matthew M. Botvinick', 'Nicolas Heess', 'Raia Hadsell']","source":"alignment newsletter","tags":"[]","date":"2019-10-13 20:02:15+00:00","url":"http://arxiv.org/abs/1910.06764v1"},{"x":"11.053361","y":"6.5079217","title":"Behaviour Suite for Reinforcement Learning","cluster":"0","author":"['Ian Osband', 'Yotam Doron', 'Matteo Hessel', 'John Aslanides', 'Eren Sezener', 'Andre Saraiva', 'Katrina McKinney', 'Tor Lattimore', 'Csaba Szepesvari', 'Satinder Singh', 'Benjamin Van Roy', 'Richard Sutton', 'David Silver', 'Hado Van Hasselt']","source":"alignment newsletter","tags":"[]","date":"2019-08-09 08:34:08+00:00","url":"http://arxiv.org/abs/1908.03568v3"},{"x":"11.239413","y":"6.237247","title":"The Principle of Unchanged Optimality in Reinforcement Learning Generalization","cluster":"0","author":"['Alex Irpan', 'Xingyou Song']","source":"alignment newsletter","tags":"[]","date":"2019-06-02 03:52:28+00:00","url":"http://arxiv.org/abs/1906.00336v1"},{"x":"11.077187","y":"6.1947007","title":"Learning to Learn with Probabilistic Task Embeddings","cluster":"0","author":"Kate Rakelly*, Aurick Zhou*, Deirdre Quillen, Chelsea Finn, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://bair.berkeley.edu/blog/2019/06/10/pearl/"},{"x":"10.385807","y":"5.6542387","title":"Unsupervised State Representation Learning in Atari","cluster":"0","author":"['Ankesh Anand', 'Evan Racah', 'Sherjil Ozair', 'Yoshua Bengio', 'Marc-Alexandre Côté', 'R Devon Hjelm']","source":"alignment newsletter","tags":"[]","date":"2019-06-19 17:16:46+00:00","url":"http://arxiv.org/abs/1906.08226v6"},{"x":"10.208952","y":"5.820188","title":"Toybox: A Suite of Environments for Experimental Evaluation of Deep Reinforcement Learning","cluster":"0","author":"['Emma Tosch', 'Kaleigh Clary', 'John Foley', 'David Jensen']","source":"alignment newsletter","tags":"[]","date":"2019-05-07 22:21:50+00:00","url":"http://arxiv.org/abs/1905.02825v1"},{"x":"9.852698","y":"6.0610943","title":"Diagnosing Bottlenecks in Deep Q-learning Algorithms","cluster":"2","author":"['Justin Fu', 'Aviral Kumar', 'Matthew Soh', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2019-02-26 22:17:47+00:00","url":"http://arxiv.org/abs/1902.10250v1"},{"x":"10.452306","y":"5.744127","title":"Simulated Policy Learning in Video Models","cluster":"0","author":"Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi, George Tucker and Henryk Michalewski","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai.googleblog.com/2019/03/simulated-policy-learning-in-video.html"},{"x":"11.135591","y":"5.802573","title":"TDM: From Model-Free to Model-Based Deep Reinforcement Learning","cluster":"0","author":"Vitchyr Pong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://bair.berkeley.edu/blog/2018/04/26/tdm/"},{"x":"9.3368225","y":"8.024167","title":"Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research","cluster":"4","author":"['Joel Z. Leibo', 'Edward Hughes', 'Marc Lanctot', 'Thore Graepel']","source":"alignment newsletter","tags":"[]","date":"2019-03-02 18:13:25+00:00","url":"http://arxiv.org/abs/1903.00742v2"},{"x":"11.359447","y":"5.6670904","title":"Long-Range Robotic Navigation via Automated Reinforcement Learning","cluster":"0","author":"Aleksandra Faust and Anthony Francis","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html"},{"x":"10.225434","y":"6.1009703","title":"Neural MMO","cluster":"0","author":"OpenAI","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://blog.openai.com/neural-mmo/"},{"x":"11.462235","y":"6.56211","title":"A Comparative Analysis of Expected and Distributional Reinforcement Learning","cluster":"0","author":"Clare Lyle, Pablo Samuel Castro, Marc G. Bellemare","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1901.11084"},{"x":"10.395419","y":"6.960357","title":"The Hanabi Challenge: A New Frontier for AI Research","cluster":"3","author":"['Nolan Bard', 'Jakob N. Foerster', 'Sarath Chandar', 'Neil Burch', 'Marc Lanctot', 'H. Francis Song', 'Emilio Parisotto', 'Vincent Dumoulin', 'Subhodeep Moitra', 'Edward Hughes', 'Iain Dunning', 'Shibl Mourad', 'Hugo Larochelle', 'Marc G. Bellemare', 'Michael Bowling']","source":"alignment newsletter","tags":"[]","date":"2019-02-01 18:59:07+00:00","url":"http://arxiv.org/abs/1902.00506v2"},{"x":"10.136925","y":"7.215456","title":"Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions","cluster":"4","author":"Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1901.01753"},{"x":"10.620267","y":"5.965067","title":"Natural Environment Benchmarks for Reinforcement Learning","cluster":"0","author":"Amy Zhang, Yuxin Wu, Joelle Pineau","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1811.06032"},{"x":"11.690793","y":"8.204242","title":"Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search","cluster":"1","author":"['Lars Buesing', 'Theophane Weber', 'Yori Zwols', 'Sebastien Racaniere', 'Arthur Guez', 'Jean-Baptiste Lespiau', 'Nicolas Heess']","source":"alignment newsletter","tags":"[]","date":"2018-11-15 10:08:58+00:00","url":"http://arxiv.org/abs/1811.06272v1"},{"x":"9.664179","y":"6.2314925","title":"Evolved Policy Gradients","cluster":"0","author":"Rein Houthooft et al","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/evolved-policy-gradients/"},{"x":"10.539592","y":"5.4467134","title":"Learning Acrobatics by Watching YouTube","cluster":"2","author":"Xue Bin (Jason) Peng and Angjoo Kanazawa ","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://bair.berkeley.edu/blog/2018/10/09/sfv/"},{"x":"9.90335","y":"6.075324","title":"The International 2018: Results","cluster":"3","author":"OpenAI","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/the-international-2018-results/"},{"x":"7.8779764","y":"5.519829","title":"Learning Actionable Representations from Visual Observations","cluster":"2","author":"['Debidatta Dwibedi', 'Jonathan Tompson', 'Corey Lynch', 'Pierre Sermanet']","source":"alignment newsletter","tags":"[]","date":"2018-08-02 17:24:54+00:00","url":"http://arxiv.org/abs/1808.00928v3"},{"x":"11.270519","y":"5.8711543","title":"Learning Plannable Representations with Causal InfoGAN","cluster":"0","author":"['Thanard Kurutach', 'Aviv Tamar', 'Ge Yang', 'Stuart Russell', 'Pieter Abbeel']","source":"alignment newsletter","tags":"[]","date":"2018-07-24 20:46:05+00:00","url":"http://arxiv.org/abs/1807.09341v1"},{"x":"10.067964","y":"6.176543","title":"Learning Heuristics for Quantified Boolean Formulas through Deep Reinforcement Learning","cluster":"0","author":"['Gil Lederman', 'Markus N. Rabe', 'Edward A. Lee', 'Sanjit A. Seshia']","source":"alignment newsletter","tags":"[]","date":"2018-07-20 23:59:36+00:00","url":"http://arxiv.org/abs/1807.08058v3"},{"x":"11.374868","y":"5.443658","title":"Visual Reinforcement Learning with Imagined Goals","cluster":"0","author":"['Ashvin Nair', 'Vitchyr Pong', 'Murtaza Dalal', 'Shikhar Bahl', 'Steven Lin', 'Sergey Levine']","source":"alignment newsletter","tags":"[]","date":"2018-07-12 17:51:16+00:00","url":"http://arxiv.org/abs/1807.04742v2"},{"x":"7.959773","y":"6.1750193","title":"DeepSpeed: Extreme-scale model training for everyone","cluster":"2","author":"DeepSpeed Team, Rangan Majumder, Junhua Wang","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/"},{"x":"11.161172","y":"6.2010565","title":"Fast reinforcement learning through the composition of behaviours","cluster":"0","author":"André Barreto, Shaobo Hou, Diana Borsa, David Silver, Doina Precup","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://deepmind.com/blog/article/fast-reinforcement-learning-through-the-composition-of-behaviours"},{"x":"11.394291","y":"6.6649623","title":"Does On-Policy Data Collection Fix Errors in Off-Policy Reinforcement Learning?","cluster":"0","author":"Aviral Kumar, Abhishek Gupta, Sergey Levine","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://bair.berkeley.edu/blog/2020/03/16/discor/"},{"x":"10.873905","y":"6.085563","title":"An Optimistic Perspective on Offline Reinforcement Learning","cluster":"0","author":"Rishabh Agarwal, Dale Schuurmans, Mohammad Norouzi","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/1907.04543"},{"x":"11.007819","y":"5.9170127","title":"Improving Sample Efficiency in Model-Free Reinforcement Learning from Images","cluster":"0","author":"['Denis Yarats', 'Amy Zhang', 'Ilya Kostrikov', 'Brandon Amos', 'Joelle Pineau', 'Rob Fergus']","source":"alignment newsletter","tags":"[]","date":"2019-10-02 15:50:03+00:00","url":"http://arxiv.org/abs/1910.01741v3"},{"x":"10.075106","y":"5.789507","title":"Retro Contest","cluster":"0","author":"Christopher Hesse, John Schulman, Vicki Pfau, Alex Nichol, Oleg Klimov, and Larissa Schiavo","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://blog.openai.com/retro-contest/"},{"x":"10.308363","y":"5.692463","title":"Mastering Atari with Discrete World Models","cluster":"0","author":"Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba","source":"alignment newsletter","tags":"[]","date":"2021.0","url":""},{"x":"11.052296","y":"6.346083","title":"What Can Learned Intrinsic Rewards Capture?","cluster":"0","author":"['Zeyu Zheng', 'Junhyuk Oh', 'Matteo Hessel', 'Zhongwen Xu', 'Manuel Kroiss', 'Hado van Hasselt', 'David Silver', 'Satinder Singh']","source":"alignment newsletter","tags":"[]","date":"2019-12-11 18:00:05+00:00","url":"http://arxiv.org/abs/1912.05500v3"},{"x":"11.113036","y":"5.904831","title":"Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Dense and Sparse Reward Environments","cluster":"0","author":"['Vinicius G. Goecks', 'Gregory M. Gremillion', 'Vernon J. Lawhern', 'John Valasek', 'Nicholas R. Waytowich']","source":"alignment newsletter","tags":"[]","date":"2019-10-09 22:32:23+00:00","url":"http://arxiv.org/abs/1910.04281v2"},{"x":"11.031774","y":"6.1534514","title":"Assessing Generalization in Deep Reinforcement Learning","cluster":"0","author":"['Charles Packer', 'Katelyn Gao', 'Jernej Kos', 'Philipp Krähenbühl', 'Vladlen Koltun', 'Dawn Song']","source":"alignment newsletter","tags":"[]","date":"2018-10-29 17:51:46+00:00","url":"http://arxiv.org/abs/1810.12282v2"},{"x":"11.370944","y":"6.235095","title":"CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning","cluster":"0","author":"['Cédric Colas', 'Pierre Fournier', 'Olivier Sigaud', 'Mohamed Chetouani', 'Pierre-Yves Oudeyer']","source":"alignment newsletter","tags":"[]","date":"2018-10-15 11:40:28+00:00","url":"http://arxiv.org/abs/1810.06284v4"},{"x":"9.9308605","y":"6.450496","title":"TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game","cluster":"4","author":"['Peng Sun', 'Xinghai Sun', 'Lei Han', 'Jiechao Xiong', 'Qing Wang', 'Bo Li', 'Yang Zheng', 'Ji Liu', 'Yongsheng Liu', 'Han Liu', 'Tong Zhang']","source":"alignment newsletter","tags":"[]","date":"2018-09-19 13:45:47+00:00","url":"http://arxiv.org/abs/1809.07193v3"},{"x":"10.537641","y":"5.867053","title":"Challenges of Context and Time in Reinforcement Learning: Introducing Space Fortress as a Benchmark","cluster":"0","author":"['Akshat Agarwal', 'Ryan Hope', 'Katia Sycara']","source":"alignment newsletter","tags":"[]","date":"2018-09-06 20:17:44+00:00","url":"http://arxiv.org/abs/1809.02206v1"},{"x":"7.7279425","y":"6.246714","title":"The use of embeddings in OpenAI Five","cluster":"2","author":"Tambet Matiisen","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"11.1359825","y":"7.2494936","title":"[Beyond algorithmic equivalence: algorithmic noise](https://www.lesserwrong.com/posts/meG3Pai2YeRYcwPwS/beyond-algorithmic-equivalence-rewards) and [Beyond algorithmic equivalence: self-modelling](https://www.lesserwrong.com/posts/kmLP3bTnBhc22DnqY/beyond-algorithmic-equivalence-self-modelling)","cluster":"0","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"10.900616","y":"8.1618395","title":"Resolving human values, completely and adequately","cluster":"0","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately"},{"x":"11.550011","y":"7.055284","title":"Why we want unbiased learning processes","cluster":"0","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesserwrong.com/posts/KT4Nau2XhuNejkXQR/why-we-want-unbiased-learning-processes"},{"x":"11.254879","y":"9.237898","title":"A theory of human values","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/qezBTig6p6p5xtL6G/a-theory-of-human-values"},{"x":"10.985646","y":"9.612885","title":"Hierarchical system preferences and subagent preferences","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/iutXWSDd56ieAiyTi/hierarchical-system-preferences-and-subagent-preferences"},{"x":"11.469469","y":"6.4788756","title":"[Reward function learning: the value function](https://www.lesswrong.com/posts/55hJDq5y7Dv3S4h49/reward-function-learning-the-value-function) and [Reward function learning: the learning process](https://www.lesswrong.com/posts/upLot6eG8cbXdKiFS/reward-function-learning-the-learning-process)","cluster":"0","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"10.923766","y":"8.540526","title":"Figuring out what Alice wants: non-human Alice","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/YfQGZderiaGv3kBJ8/figuring-out-what-alice-wants-non-human-alice"},{"x":"11.540608","y":"9.336773","title":"[One-step hypothetical preferences](https://www.alignmentforum.org/posts/i6hWWcKyxBPj7ELT6/one-step-hypothetical-preferences) and [A small example of one-step hypotheticals](https://www.lesswrong.com/posts/zo5K8QeDZiLicSCe6/a-small-example-of-one-step-hypotheticals)","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2019.0","url":""},{"x":"11.014176","y":"8.722689","title":"Figuring out what Alice wants, parts [I](https://www.lesswrong.com/posts/so78tQvTCaBgw4bfL/figuring-out-what-alice-wants-part-i) and [II](https://www.lesswrong.com/posts/rcXaY3FgoobMkH2jc/figuring-out-what-alice-wants-part-ii)","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"10.719443","y":"5.9462953","title":"Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures","cluster":"0","author":"Jonathan Uesato*, Ananya Kumar*, Csaba Szepesvari*, Tom Erez, Avraham Ruderman, Keith Anderson, Krishmamurthy (Dj) Dvijotham, Nicolas Heess, Pushmeet Kohli","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://arxiv.org/abs/1812.01647"},{"x":"8.6987095","y":"8.567684","title":"Designing robust & reliable AI systems and how to succeed in AI","cluster":"4","author":"Rob Wiblin and Pushmeet Kohli","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research/"},{"x":"8.773441","y":"7.146179","title":"A naive alignment strategy and optimism about generalization","cluster":"2","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/QvtHSsZLFCAHmzes7/a-naive-alignment-strategy-and-optimism-about-generalization"},{"x":"8.017545","y":"5.213942","title":"Pretrained Transformers Improve Out-of-Distribution Robustness","cluster":"2","author":"Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/2004.06100"},{"x":"8.438134","y":"5.2384286","title":"Learning from Untrusted Data","cluster":"2","author":"Moses Charikar, Jacob Steinhardt, Gregory Valiant","source":"alignment newsletter","tags":"[]","date":"2017.0","url":"https://arxiv.org/abs/1611.02315"},{"x":"8.416587","y":"7.252527","title":"Experimentally evaluating whether honesty generalizes","cluster":"2","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/BxersHYN2qcFoonwg/experimentally-evaluating-whether-honesty-generalizes"},{"x":"8.262337","y":"8.764629","title":"Building Trust through Testing","cluster":"3","author":"Michèle A. Flournoy, Avril Haines, Gabrielle Chefitz","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://cset.georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf"},{"x":"8.833481","y":"8.404951","title":"Towards Robust and Verified AI: Specification Testing, Robust Training, and Formal Verification","cluster":"4","author":"Pushmeet Kohli, Krishnamurthy (Dj) Dvijotham, Jonathan Uesato, Sven Gowal, and the Robust & Verified Deep Learning group","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://medium.com/@deepmindsafetyresearch/towards-robust-and-verified-ai-specification-testing-robust-training-and-formal-verification-69bd1bc48bda"},{"x":"8.732509","y":"5.1068854","title":"AI Alignment Podcast: The Byzantine Generals’ Problem, Poisoning, and Distributed Machine Learning","cluster":"2","author":"Lucas Perry and El Mahdi El Mahmdi","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/01/30/ai-alignment-podcast-the-byzantine-problem-poisoning-and-distributed-machine-learning-with-el-mahdi-el-mahmdi-beneficial-agi-2019/"},{"x":"8.315975","y":"5.3205876","title":"Iterative Learning with Open-set Noisy Labels","cluster":"2","author":"Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, Shu-Tao Xia","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1804.00092"},{"x":"8.730954","y":"7.199636","title":"Teaching ML to answer questions honestly instead of predicting human answers","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of"},{"x":"7.983289","y":"5.151023","title":"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty","cluster":"2","author":"Dan Hendrycks*, Norman Mu*, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://arxiv.org/abs/1912.02781"},{"x":"7.5370917","y":"7.006794","title":"Redwood Research’s current project","cluster":"2","author":"Buck Shlegeris","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project"},{"x":"8.52168","y":"5.0617223","title":"Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks","cluster":"2","author":"Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1804.00792"},{"x":"11.556473","y":"6.4713206","title":"Maximum Entropy Inverse Reinforcement Learning","cluster":"0","author":"Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, and Anind K. Dey","source":"alignment newsletter","tags":"[]","date":"2008.0","url":"http://www.cs.cmu.edu/~bziebart/publications/maxentirl-bziebart.pdf"},{"x":"11.715405","y":"9.297159","title":"Modeling Interaction via the Principle of Maximum Causal Entropy","cluster":"1","author":"Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey","source":"alignment newsletter","tags":"[]","date":"2010.0","url":"http://www.cs.cmu.edu/~bziebart/publications/maximum-causal-entropy.pdf"},{"x":"11.329034","y":"6.2457447","title":"Learning from humans: what is inverse reinforcement learning?","cluster":"0","author":"Jordan Alexander","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/"},{"x":"9.493343","y":"7.468259","title":"Current Work in AI Alignment","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment"},{"x":"10.172382","y":"7.1274896","title":"AI Alignment Podcast: On DeepMind, AI Safety, and Recursive Reward Modeling","cluster":"4","author":"Lucas Perry and Jan Leike","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://futureoflife.org/2019/12/16/ai-alignment-podcast-on-deepmind-ai-safety-and-recursive-reward-modeling-with-jan-leike/"},{"x":"9.100419","y":"8.562907","title":"AI Alignment Research Overview","cluster":"4","author":"Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit"},{"x":"9.01601","y":"7.8579535","title":"Reframing Superintelligence: Comprehensive AI Services as General Intelligence","cluster":"4","author":"Eric Drexler","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.fhi.ox.ac.uk/reframing/"},{"x":"9.951627","y":"7.6663246","title":"AI Alignment Podcast: Inverse Reinforcement Learning and the State of AI Alignment","cluster":"4","author":"Lucas Perry and Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://futureoflife.org/2018/12/17/inverse-reinforcement-learning-and-the-state-of-ai-alignment-with-rohin-shah/"},{"x":"8.386293","y":"9.033643","title":"80K podcast with Paul Christiano","cluster":"3","author":"Paul Christiano and Rob Wiblin","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/"},{"x":"8.622898","y":"8.8298","title":"Building safe artificial intelligence: specification, robustness, and assurance","cluster":"4","author":"Pedro A. Ortega, Vishal Maini DeepMind safety team","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1"},{"x":"11.030358","y":"9.30575","title":"Realism about rationality","cluster":"1","author":"Richard Ngo","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality"},{"x":"8.463305","y":"6.351113","title":"RFP: Measuring and forecasting risks","cluster":"2","author":"Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://docs.google.com/document/d/1cPwcUSl0Y8TyZxCumGPBhdVUN0Yyyw9AR1QshlRI3gc/edit"},{"x":"9.987435","y":"7.160789","title":"RFP: Techniques for enhancing human feedback","cluster":"4","author":"Ajeya Cotra","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://docs.google.com/document/d/1uPOQikvqhxANvejgFfnzH-vNX3tMap4uFL3KCYqPeSg/edit?usp=sharing"},{"x":"8.39172","y":"6.107615","title":"RFP: Interpretability","cluster":"2","author":"Chris Olah","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://docs.google.com/document/d/1PB58Fx3fmahx8vutW7TY6sG3-Ho0qfd1RwA1gP2yzWg/edit?usp=sharing"},{"x":"8.625341","y":"7.830763","title":"RFP: Truthful and honest AI","cluster":"4","author":"Owain Evans","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://docs.google.com/document/d/186GGXoi_g0ML_YRKnppfLNxZIdvHTpLakOyg6GQENi4/edit?usp=sharing"},{"x":"8.702951","y":"7.518786","title":"The case for aligning narrowly superhuman models","cluster":"2","author":"Ajeya Cotra","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models"},{"x":"8.650185","y":"9.33257","title":"Some AI research areas and their relevance to existential safety","cluster":"3","author":"Andrew Critch","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1"},{"x":"8.755397","y":"9.544664","title":"Andrew Critch on AI Research Considerations for Human Existential Safety","cluster":"3","author":"Lucas Perry and Andrew Critch","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://futureoflife.org/2020/09/15/andrew-critch-on-ai-research-considerations-for-human-existential-safety/?utm_source=feedly&utm_medium=rss&utm_campaign=andrew-critch-on-ai-research-considerations-for-human-existential-safety"},{"x":"11.456464","y":"6.4763303","title":"The Alignment Problem for Bayesian History-Based Reinforcement Learners","cluster":"0","author":"Tom Everitt, Marcus Hutter","source":"alignment newsletter","tags":"[]","date":"2018.0","url":""},{"x":"8.634676","y":"9.470861","title":"AI Research Considerations for Human Existential Safety","cluster":"3","author":"Andrew Critch, David Krueger","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"http://acritch.com/papers/arches.pdf"},{"x":"8.71736","y":"8.030318","title":"An Analytic Perspective on AI Alignment","cluster":"4","author":"Daniel Filan","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment"},{"x":"8.847325","y":"7.907936","title":"A dilemma for prosaic AI alignment","cluster":"4","author":"Daniel Kokotajlo","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment"},{"x":"8.549304","y":"8.538333","title":"Useful Does Not Mean Secure","cluster":"4","author":"Ben Pace","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/mdau2DBSMi5bWXPGA/useful-does-not-mean-secure"},{"x":"8.402522","y":"9.193243","title":"AI Alignment Podcast: An Overview of Technical AI Alignment: [Part 1](https://futureoflife.org/2019/04/11/an-overview-of-technical-ai-alignment-with-rohin-shah-part-1/) and [Part 2](https://futureoflife.org/2019/04/25/an-overview-of-technical-ai-alignment-with-rohin-shah-part-2/)","cluster":"3","author":"Lucas Perry and Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2019.0","url":""},{"x":"10.685597","y":"9.1071825","title":"Selection Theorems: A Program For Understanding Agents","cluster":"1","author":"John Wentworth","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents"},{"x":"8.567534","y":"6.3880267","title":"Measurement, Optimization, and Take-off Speed","cluster":"2","author":"Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://jsteinhardt.stat.berkeley.edu/blog/measurement-and-optimization"},{"x":"8.749086","y":"8.675341","title":"Transparency and AGI safety","cluster":"4","author":"Jennifer Lin","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/QirLfXhDPYWCP8PK5/transparency-and-agi-safety"},{"x":"9.669385","y":"9.2090025","title":"AI Alignment, Philosophical Pluralism, and the Relevance of Non-Western Philosophy","cluster":"4","author":"Tan Zhi Xuan","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of"},{"x":"10.815104","y":"7.6387057","title":"The Learning-Theoretic AI Alignment Research Agenda","cluster":"4","author":"Vadim Kosoy","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://agentfoundations.org/item?id=1816"},{"x":"8.602861","y":"8.978686","title":"Conceptual issues in AI safety: the paradigmatic gap","cluster":"3","author":"Jon Gauthier","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://www.foldl.me/2018/conceptual-issues-ai-safety-paradigmatic-gap/"},{"x":"9.568208","y":"8.493046","title":"Plausible cases for HRAD work, and locating the crux in the \"realism about rationality\" debate","cluster":"4","author":"Issa Rice","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/BGxTpdBGbwCWrGiCL/plausible-cases-for-hrad-work-and-locating-the-crux-in-the"},{"x":"8.672808","y":"9.776686","title":"Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda","cluster":"3","author":"Jesse Clifton","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.lesswrong.com/s/p947tK8CoBbdpPtyK"},{"x":"8.356529","y":"9.116166","title":"AI Safety \"Success Stories\"","cluster":"3","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/bnY3L48TtDrKTzGRb/ai-safety-success-stories"},{"x":"9.31098","y":"7.6023836","title":"Four Ways An Impact Measure Could Help Alignment","cluster":"4","author":"Matthew Barnett","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://alignmentforum.org/posts/wJK944YqvFwjdbqCP/four-ways-an-impact-measure-could-help-alignment"},{"x":"10.067466","y":"9.5229845","title":"On the purposes of decision theory research","cluster":"3","author":"Wei Dai","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/JSjagTDGdz2y6nNE3/on-the-purposes-of-decision-theory-research"},{"x":"8.832266","y":"5.2227974","title":"Unsolved research problems vs. real-world threat models","cluster":"3","author":"Catherine Olsson","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://medium.com/@catherio/unsolved-research-problems-vs-real-world-threat-models-e270e256bc9e"},{"x":"11.391078","y":"6.7167907","title":"Inverse Reinforcement Learning and Inferring Human Preference with Dylan Hadfield-Menell","cluster":"0","author":"Lucas Perry and Dylan Hadfield-Menell","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://futureoflife.org/2018/04/04/podcast-ai-systems-learning-human-preferences/"},{"x":"8.781494","y":"8.692154","title":"Why I expect successful alignment","cluster":"4","author":"Tobias Baumann","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://s-risks.org/why-i-expect-successful-alignment/"},{"x":"9.024854","y":"7.9881225","title":"The Rocket Alignment Problem","cluster":"4","author":"Eliezer Yudkowsky","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem"},{"x":"10.385865","y":"9.536641","title":"Comment on decision theory","cluster":"1","author":"Rob Bensinger","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/uKbxi2EJ3KBNRDGpL/comment-on-decision-theory"},{"x":"8.280888","y":"7.286552","title":"Some criteria for sandwiching projects","cluster":"2","author":"Daniel Ziegler","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://www.alignmentforum.org/posts/Gfbf7RsE2fvxGXKC5/some-criteria-for-sandwiching-projects"},{"x":"8.755608","y":"8.619861","title":"High Impact Careers in Formal Verification: Artificial Intelligence","cluster":"4","author":"Quinn Dougherty","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://forum.effectivealtruism.org/posts/4rMxiyPTPdzaFMyGm/high-impact-careers-in-formal-verification-artificial"},{"x":"8.730598","y":"6.7628155","title":"Mechanistic Transparency for Machine Learning","cluster":"2","author":"Daniel Filan","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/3kwR2dufdJyJamHQq/mechanistic-transparency-for-machine-learning"},{"x":"8.269542","y":"7.45164","title":"Alignment of Language Agents","cluster":"4","author":"Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving","source":"alignment newsletter","tags":"[]","date":"2021.0","url":"https://medium.com/@deepmindsafetyresearch/alignment-of-language-agents-9fbc7dd52c6c"},{"x":"11.094372","y":"9.266087","title":"Another take on agent foundations: formalizing zero-shot reasoning","cluster":"1","author":"zhukeepa","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/pu3ddLSZjjmiiqQfh/another-take-on-agent-foundations-formalizing-zero-shot"},{"x":"8.63023","y":"7.4348454","title":"Optimization Amplifies","cluster":"2","author":"Scott Garrabrant","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies"},{"x":"8.755164","y":"8.849473","title":"AI Paradigms and AI Safety: Mapping Artefacts and Techniques to Safety Issues","cluster":"4","author":"Jose Hernandez-Orallo, Fernando Martinez-Plumed, Shahar Avin, Jess Whittlestone, Seán Ó hÉigeartaigh","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.cser.ac.uk/resources/ai-paradigms-and-ai-safety-mapping-artefacts-and-techniques-safety-issues/"},{"x":"8.879223","y":"8.172648","title":"AI Services as a Research Paradigm","cluster":"4","author":"Vojta Kovarik","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://www.alignmentforum.org/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm"},{"x":"9.6094475","y":"8.692669","title":"The Value Definition Problem","cluster":"4","author":"Sammy Martin","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/W95gbuognJu5WxkTW/the-value-definition-problem"},{"x":"10.918361","y":"8.501775","title":"Multi-agent minds and AI alignment","cluster":"4","author":"Jan Kulveit","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.lesswrong.com/posts/3fkBWpE4f9nYbdf7E/multi-agent-minds-and-ai-alignment"},{"x":"8.261616","y":"5.6277905","title":"On Calibration of Modern Neural Networks","cluster":"2","author":"Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1706.04599"},{"x":"8.436585","y":"5.958435","title":"Evaluating the Unsupervised Learning of Disentangled Representations","cluster":"2","author":"Olivier Bachem","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html"},{"x":"7.789892","y":"5.4344053","title":"Understanding View Selection for Contrastive Learning","cluster":"2","author":"Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola","source":"alignment newsletter","tags":"[]","date":"2020.0","url":"https://ai.googleblog.com/2020/08/understanding-view-selection-for.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+blogspot%2FgJZg+%28Google+AI+Blog%29"},{"x":"9.538786","y":"9.009175","title":"Following human norms","cluster":"4","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/eBd6WvzhuqduCkYv3/following-human-norms"},{"x":"11.400845","y":"6.354795","title":"Reward uncertainty","cluster":"0","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/ZiLLxaLB5CCofrzPp"},{"x":"10.244582","y":"8.347635","title":"Ambitious vs. narrow value learning","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/SvuLhtREMy8wRBzpC/ambitious-vs-narrow-value-learning"},{"x":"9.22257","y":"8.757642","title":"AI safety without goal-directed behavior","cluster":"4","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/tHxXdAn8Yuiy9y2pZ/ai-safety-without-goal-directed-behavior"},{"x":"9.568168","y":"8.134316","title":"Will humans build goal-directed agents?","cluster":"4","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"https://www.alignmentforum.org/posts/9zpT9dikrrebdq3Jf/will-humans-build-goal-directed-agents"},{"x":"10.834243","y":"9.556303","title":"Coherence arguments do not imply goal-directed behavior","cluster":"1","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior"},{"x":"9.799034","y":"8.964407","title":"Intuitions about goal-directed behavior","cluster":"4","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior"},{"x":"11.50508","y":"7.1482635","title":"Latent Variables and Model Mis-Specification","cluster":"1","author":"Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/gnvrixhDfG7S2TpNL/latent-variables-and-model-mis-specification"},{"x":"11.494498","y":"6.6111107","title":"Model Mis-specification and Inverse Reinforcement Learning","cluster":"0","author":"Owain Evans and Jacob Steinhardt","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning"},{"x":"11.399749","y":"7.710827","title":"Humans can be assigned any values whatsoever…","cluster":"1","author":"Stuart Armstrong","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever"},{"x":"10.773011","y":"8.145857","title":"The easy goal inference problem is still hard","cluster":"4","author":"Paul Christiano","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard"},{"x":"10.219619","y":"8.13061","title":"What is ambitious value learning?","cluster":"4","author":"Rohin Shah","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning"},{"x":"8.848743","y":"5.670462","title":"A Dual Approach to Scalable Verification of Deep Networks","cluster":"2","author":"Krishnamurthy (Dj)Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, Pushmeet Kohli","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"https://arxiv.org/abs/1803.06567"},{"x":"10.647476","y":"6.246687","title":"An Inductive Synthesis Framework for Verifiable Reinforcement Learning","cluster":"0","author":"He Zhu, Zikang Xiong, Stephen Magill, Suresh Jagannathan","source":"alignment newsletter","tags":"[]","date":"2019.0","url":"http://arxiv.org/abs/1907.07273"},{"x":"10.9527855","y":"6.419069","title":"Verifiable Reinforcement Learning via Policy Extraction","cluster":"0","author":"Osbert Bastani, Yewen Pu, Armando Solar-Lezama","source":"alignment newsletter","tags":"[]","date":"2018.0","url":"http://arxiv.org/abs/1805.08328"}]